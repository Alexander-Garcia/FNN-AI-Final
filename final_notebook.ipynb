{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase - 1: Data analysis & preparation\n"
      ],
      "metadata": {
        "id": "jVIptk5wUu6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uahccdjy0l_C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the data\n"
      ],
      "metadata": {
        "id": "WDLf-weDUlIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('heart_statlog_cleveland_hungary_final.csv')"
      ],
      "metadata": {
        "id": "64yMYFFgG-V1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW1v5Wng0-De",
        "outputId": "1ca27beb-690e-48fd-9d7b-62ffef443e1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol',\n",
              "       'fasting blood sugar', 'resting ecg', 'max heart rate',\n",
              "       'exercise angina', 'oldpeak', 'ST slope', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wff2m4PYKLHT",
        "outputId": "e02655d4-2ef6-473f-f9bf-91dd17582529"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1190, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vV_CSMVXHPDG",
        "outputId": "94ff897e-580a-477c-88ea-e91ddb387800"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  chest pain type  resting bp s  cholesterol  fasting blood sugar  \\\n",
              "0   40    1                2           140          289                    0   \n",
              "1   49    0                3           160          180                    0   \n",
              "2   37    1                2           130          283                    0   \n",
              "3   48    0                4           138          214                    0   \n",
              "4   54    1                3           150          195                    0   \n",
              "\n",
              "   resting ecg  max heart rate  exercise angina  oldpeak  ST slope  target  \n",
              "0            0             172                0      0.0         1       0  \n",
              "1            0             156                0      1.0         2       1  \n",
              "2            1              98                0      0.0         1       0  \n",
              "3            0             108                1      1.5         2       1  \n",
              "4            0             122                0      0.0         1       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e9f7f2d-765b-44df-b3be-032b5a9709db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>chest pain type</th>\n",
              "      <th>resting bp s</th>\n",
              "      <th>cholesterol</th>\n",
              "      <th>fasting blood sugar</th>\n",
              "      <th>resting ecg</th>\n",
              "      <th>max heart rate</th>\n",
              "      <th>exercise angina</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>ST slope</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140</td>\n",
              "      <td>289</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>160</td>\n",
              "      <td>180</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>283</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>138</td>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e9f7f2d-765b-44df-b3be-032b5a9709db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e9f7f2d-765b-44df-b3be-032b5a9709db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e9f7f2d-765b-44df-b3be-032b5a9709db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-702aa296-1d3a-4998-9fa2-bc77e12e77cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-702aa296-1d3a-4998-9fa2-bc77e12e77cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-702aa296-1d3a-4998-9fa2-bc77e12e77cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1190,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 28,\n        \"max\": 77,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          44,\n          68,\n          66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chest pain type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resting bp s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 0,\n        \"max\": 200,\n        \"num_unique_values\": 67,\n        \"samples\": [\n          165,\n          118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cholesterol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 101,\n        \"min\": 0,\n        \"max\": 603,\n        \"num_unique_values\": 222,\n        \"samples\": [\n          305,\n          321\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fasting blood sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resting ecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max heart rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 60,\n        \"max\": 202,\n        \"num_unique_values\": 119,\n        \"samples\": [\n          132,\n          157\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exercise angina\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0863372185219862,\n        \"min\": -2.6,\n        \"max\": 6.2,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          1.3,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ST slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbGsExsRHT-T",
        "outputId": "64458f07-4fb8-47ed-d0b1-e041c271b26b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1190 entries, 0 to 1189\n",
            "Data columns (total 12 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   age                  1190 non-null   int64  \n",
            " 1   sex                  1190 non-null   int64  \n",
            " 2   chest pain type      1190 non-null   int64  \n",
            " 3   resting bp s         1190 non-null   int64  \n",
            " 4   cholesterol          1190 non-null   int64  \n",
            " 5   fasting blood sugar  1190 non-null   int64  \n",
            " 6   resting ecg          1190 non-null   int64  \n",
            " 7   max heart rate       1190 non-null   int64  \n",
            " 8   exercise angina      1190 non-null   int64  \n",
            " 9   oldpeak              1190 non-null   float64\n",
            " 10  ST slope             1190 non-null   int64  \n",
            " 11  target               1190 non-null   int64  \n",
            "dtypes: float64(1), int64(11)\n",
            "memory usage: 111.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x5-uDOv9HXka",
        "outputId": "b5dd81ae-681b-42ee-c5b8-225e5926de13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                    0\n",
              "sex                    0\n",
              "chest pain type        0\n",
              "resting bp s           0\n",
              "cholesterol            0\n",
              "fasting blood sugar    0\n",
              "resting ecg            0\n",
              "max heart rate         0\n",
              "exercise angina        0\n",
              "oldpeak                0\n",
              "ST slope               0\n",
              "target                 0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chest pain type</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resting bp s</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cholesterol</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fasting blood sugar</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resting ecg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max heart rate</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exercise angina</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ST slope</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-9PpI4v5HlE1",
        "outputId": "1bfc4283-16e7-40e7-ebcf-bd1d12343f5d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age          sex  chest pain type  resting bp s  cholesterol  \\\n",
              "count  1190.000000  1190.000000      1190.000000   1190.000000  1190.000000   \n",
              "mean     53.720168     0.763866         3.232773    132.153782   210.363866   \n",
              "std       9.358203     0.424884         0.935480     18.368823   101.420489   \n",
              "min      28.000000     0.000000         1.000000      0.000000     0.000000   \n",
              "25%      47.000000     1.000000         3.000000    120.000000   188.000000   \n",
              "50%      54.000000     1.000000         4.000000    130.000000   229.000000   \n",
              "75%      60.000000     1.000000         4.000000    140.000000   269.750000   \n",
              "max      77.000000     1.000000         4.000000    200.000000   603.000000   \n",
              "\n",
              "       fasting blood sugar  resting ecg  max heart rate  exercise angina  \\\n",
              "count          1190.000000  1190.000000     1190.000000      1190.000000   \n",
              "mean              0.213445     0.698319      139.732773         0.387395   \n",
              "std               0.409912     0.870359       25.517636         0.487360   \n",
              "min               0.000000     0.000000       60.000000         0.000000   \n",
              "25%               0.000000     0.000000      121.000000         0.000000   \n",
              "50%               0.000000     0.000000      140.500000         0.000000   \n",
              "75%               0.000000     2.000000      160.000000         1.000000   \n",
              "max               1.000000     2.000000      202.000000         1.000000   \n",
              "\n",
              "           oldpeak     ST slope       target  \n",
              "count  1190.000000  1190.000000  1190.000000  \n",
              "mean      0.922773     1.624370     0.528571  \n",
              "std       1.086337     0.610459     0.499393  \n",
              "min      -2.600000     0.000000     0.000000  \n",
              "25%       0.000000     1.000000     0.000000  \n",
              "50%       0.600000     2.000000     1.000000  \n",
              "75%       1.600000     2.000000     1.000000  \n",
              "max       6.200000     3.000000     1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d989902-8b6d-41c1-97f9-9c259a0df80d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>chest pain type</th>\n",
              "      <th>resting bp s</th>\n",
              "      <th>cholesterol</th>\n",
              "      <th>fasting blood sugar</th>\n",
              "      <th>resting ecg</th>\n",
              "      <th>max heart rate</th>\n",
              "      <th>exercise angina</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>ST slope</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>53.720168</td>\n",
              "      <td>0.763866</td>\n",
              "      <td>3.232773</td>\n",
              "      <td>132.153782</td>\n",
              "      <td>210.363866</td>\n",
              "      <td>0.213445</td>\n",
              "      <td>0.698319</td>\n",
              "      <td>139.732773</td>\n",
              "      <td>0.387395</td>\n",
              "      <td>0.922773</td>\n",
              "      <td>1.624370</td>\n",
              "      <td>0.528571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.358203</td>\n",
              "      <td>0.424884</td>\n",
              "      <td>0.935480</td>\n",
              "      <td>18.368823</td>\n",
              "      <td>101.420489</td>\n",
              "      <td>0.409912</td>\n",
              "      <td>0.870359</td>\n",
              "      <td>25.517636</td>\n",
              "      <td>0.487360</td>\n",
              "      <td>1.086337</td>\n",
              "      <td>0.610459</td>\n",
              "      <td>0.499393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>54.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>229.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>140.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>269.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>603.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d989902-8b6d-41c1-97f9-9c259a0df80d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d989902-8b6d-41c1-97f9-9c259a0df80d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d989902-8b6d-41c1-97f9-9c259a0df80d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bbd8ad4-ba79-456d-b80b-b41bd0b393ff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bbd8ad4-ba79-456d-b80b-b41bd0b393ff')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bbd8ad4-ba79-456d-b80b-b41bd0b393ff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 404.62708498303755,\n        \"min\": 9.358202797635386,\n        \"max\": 1190.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          53.72016806722689,\n          54.0,\n          1190.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.4666192873422,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7638655462184873,\n          1.0,\n          0.4248843096754764\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chest pain type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 419.71179324661676,\n        \"min\": 0.9354803611453992,\n        \"max\": 1190.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1190.0,\n          3.2327731092436975,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resting bp s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 388.9431366122736,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          132.15378151260504,\n          130.0,\n          1190.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cholesterol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 381.8591975767479,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          210.36386554621848,\n          229.0,\n          1190.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fasting blood sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.6466850369232,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2134453781512605,\n          1.0,\n          0.4099117568473306\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resting ecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.44809211602285,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6983193277310924,\n          2.0,\n          0.8703588379852838\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max heart rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 381.93094813522424,\n        \"min\": 25.517635548982874,\n        \"max\": 1190.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          139.7327731092437,\n          140.5,\n          1190.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exercise angina\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.58354274381423,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3873949579831933,\n          1.0,\n          0.48735992951791174\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.34115711284187,\n        \"min\": -2.6,\n        \"max\": 1190.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9227731092436974,\n          0.6,\n          1190.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ST slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.21262571225157,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1190.0,\n          1.6243697478991597,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.5252975807134,\n        \"min\": 0.0,\n        \"max\": 1190.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5285714285714286,\n          1.0,\n          0.4993928790311868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlations and data distribution\n"
      ],
      "metadata": {
        "id": "3OHCC65yVYAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt_index = 1\n",
        "for c_name, content in df.items():\n",
        "  ax = plt.subplot(6,2, plt_index)\n",
        "  plt.subplots_adjust(hspace = 1)\n",
        "  sns.histplot(content, ax=ax)\n",
        "  plt.title(c_name)\n",
        "  plt_index += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "heXSPmHEP_Fz",
        "outputId": "46ae9746-53a2-41a4-a6bd-2a5b6f8bd0ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(data=df.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt=\".2f\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "eq3gRcbdHxeo",
        "outputId": "02afdc1a-9a85-47ca-82a1-67e8947c2ea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwIAAAMWCAYAAABC+zE1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzddXQUVxvA4V827kpC3EmQ4O7uFCvFihVooVCsBUpxK6UFihUpLVrcKe5Q3D24BUggbsQ33x8LGxY2EJbQlI/3OWcP7OTOnTdzZyZzbUYvMzMzEyGEEEIIIcRHRZHXAQghhBBCCCH+fVIREEIIIYQQ4iMkFQEhhBBCCCE+QlIREEIIIYQQ4iMkFQEhhBBCCCE+QlIREEIIIYQQ4iMkFQEhhBBCCCE+QlIREEIIIYQQ4iMkFQEhhBBCCCE+QlIREEIIIYQQ4iMkFQEhhBBCCCFy0cGDB2nSpAkuLi7o6emxYcOGN66zf/9+SpYsibGxMX5+fixcuPC9xykVASGEEEIIIXJRYmIixYoV47fffstR+jt37tCoUSNq1KjBuXPn6NevH926dWPHjh3vNU69zMzMzPe6BSGEEEIIIT5Senp6rF+/nmbNmmWbZvDgwWzZsoVLly6pl7Vp04aYmBi2b9/+3mKTHgEhhBBCCCHy0NGjR6ldu7bGsnr16nH06NH3ul2D95q7EEIIIYQQ/wdSUlJISUnRWGZsbIyxsfE75x0WFoaTk5PGMicnJ+Li4khKSsLU1PSdt6GNVASEEEIIIcQHYYthQJ5t++TQtowePVpj2ciRIxk1alTeBJQLpCIg/jV5efLmVKO0ayzcn9dRvF7n6rD/UlJeh/FG1YuYEnzrYV6H8UYFfV2Z9vd/f6pU3yZ6HA2Oy+swXqtCQStOXovJ6zDeqEyADXXan87rMN5o19JSHLsam9dhvFH5QGsu3wzN6zBeq7CfMxdvPs7rMN4oyM+Jq7ce5HUYbxTo65bXIeSJIUOGMGDAAI1ludEbAJA/f34eP9Y8Rh8/foyVldV76w0AqQgIIYQQQogPhJ6hXp5tO7eGAWlToUIFtm7dqrFs165dVKhQ4b1s7zmZLCyEEEIIIUQuSkhI4Ny5c5w7dw5QPR703Llz3L9/H1D1LnTs2FGdvkePHty+fZtBgwZx9epVZs2axapVq+jfv/97jVN6BIQQQgghxAdBYZB3PQJv49SpU9SoUUP9/fmQok6dOrFw4UJCQ0PVlQIAb29vtmzZQv/+/Zk2bRpubm788ccf1KtX773GKRUBIYQQQgghclH16tV53au6tL01uHr16pw9e/Y9RvUqGRokhBBCCCHER0h6BIQQQgghxAdBz1DasHOT7E0hhBBCCCE+QtIjIIQQQgghPggfymThD4X0CAghhBBCCPERkoqAEEIIIYQQHyEZGiSEEEIIIT4Ieflm4f9H0iMghBBCCCHER0h6BIQQQgghxAdBJgvnLukREEIIIYQQ4iMkPQJCCCGEEOKDIHMEcpf0CAghhBBCCPERkoqAEEIIIYQQHyEZGiTynF3l0vh82xXrkkUwcXHkVMuvebxpz+vXqVqWQpO+x6KQP8khodycMJsHi9drpPHs2Q6fAV0xzp+PuAtXudxvLLEnL75TrKf3LeX4rj9JiA3H0S2Qum2G4+JdNNv0wae3cXDjNGIjH2Ln6EX1Ft/hF1QNgIyMNA5umMqtSweJiQjB2NQCr4IVqd78WyxtnN4pzn3bVrBr4yJiYyJx8ypAm66D8fYP0pr20f2bbFoxm/u3rxAZHkqrLt9Ru/HnGmn+XjmbzavmaixzcvFizIwN7xTn1r83sH7tSmKio/Dy9qV7z28oEFBQa9qd2zezb88u7t+7A4CvXwE+79T1lfQh9++xeMHvXL54gYyMDNw9PBk8dBT5HHXfp5mZmZzcMYMrx1eTkhSHs3dJqrYYiU0+r9eud/HwUs7t/5On8RHYOwdSpfkwnDyyjpcNszrw6PZJjXUKlW9N9U9Hv3WMu7euYtv6v4iNicTDy5/Puw/Ep0BhrWkf3r/FumVzuXvrKpHhobT9oj/1PmmnkUaZkcH6Fb9z9MB2YmMisbF1oHLNxnzyWVf09HTvmt+1ZTVb1i8lNjoSD29/On75Lb7ZxPng/m3WLp3LnVvXiHgSyudd+1G/aVuNNElPE1mzdC6njh0gLjYaL58CfN59AL7+hXSO8blOLZ1pUCMfFub6XL6ewPT593n4OCXb9I1rOdCkdj6c8hkDcO9BEn+tD+Xk+TgAnByM+Gua9vNw7LRbHDwR81bx7d6ymm0b/iI2OhJ3L38+//K71+zLW6xf9jt3b10l4kko7br2p94nr+7LdcvmcvrYfuJio/H0LkD77t/i8477ctvm9WxYu+LZee5Htx598M/mPN+1fTP79+7g/t2s87x9p+4a6WdMmcC+PTs01itesgwjxv7yjnGuY9OzOD29fenaoy/+Adp/913b/+bA3h2E3L0NgI9fAO06dc82/dyZk9i1bROdu/emcbPPdI5xy98b2LB2FdHPrplf9vyGAgGBWtPu3L6FfXt2cu/eXUC1Lzt06vpK+pD791i0YJ7GNfP7oSPf6Zr5Pslk4dwlPQIiz+mbmxF34RqX+uTs5sfUy40ym+YSuf84h0o35c6MRQTNHYdDncrqNM6tGlDwlyHcGPcbh8o2J/7CVcpt+ROjfHY6x3nl5Fb2rJlA5Ua9+GLoepzcAlk5vSuJcZFa0z+4dYaNf3xLsUqf8sWwDfgXr8Xa2b0If3gdgLTUZMJCrlCpUU+6DF1Hix4ziQy7w5rfeuocI8DJwztYs3AyjT77iqG/LMfNswDTx35NXGyU1vSpqck4OLnS/PO+WNk4ZJuvi7svP/+xW/0ZNH7BO8V56MA+5s+bTZt2HZkyYy5ePr6MHj6YmJhorekvXThPlWo1GTthChMnz8TBIR+jhg0iMiJcnSY09CE/DOyLq5sH4yZOYeqseXzW9nMMjYzeKdaz+/7gwqElVGs5ipZ9VmFgZMrmed1IT8v+pvDGua0c3vQTpev0olW/dTi4BLB5XjeexmseL4XKtaLziH/Un4qNB751fMcP7WTF/Kk0a9ON0VOW4O7lz6TR3xAXo73MU1KSyZfflVYde2Nta681zZZ1i9m3fS2ffzmQH2es4rNO37Bt/RJ2b1n51vE9d+yfXSz9cxrN23Rl3K+L8PDyY+LIvsS+Ic7WHb/ONs4/Zv7IpXMn6Nl/FBOmL6VI8XL8NLw3UZFPdI4ToHVjJ5rVc2Tagnt8M+IqySlKJnzvj+FrxidHRKXx54qH9BoaTK9hwZy7HM/oAb54upoAEB6Zymdfn9f4LFrziKdJGZx4VlnIqeP/7GL5/Kk0bd2N0VMW4+7tz6RRfbIt89SUFPI5udKqQ69s9+X8meO5dO44X/YfxfjpyyhSohw/j+j1Tvvy0MG9LJg3i8/adWbS9Hl4efsyZvjA7M/zi+eoXLUWYyb8yoTJv2Gfz5HRw7/TOM8BSpQqy59L1qo/AwaN0DlGgMMH97Bo3m+0ateZn6f/gZe3H+OGf0dsNnFevniWylVrMWrCNH6cPBuHfI6M1RInwPEjB7lx9Qp29tlfX3PinwP7mD9vDq3bdWTKjDl4+/gy6jXXzIvPrpnjJkzm58kzsrlmPmLIwL64ubkzfuJkpuXSNVN8OKQi8H9u+/btVK5cGRsbG+zt7WncuDG3bt1S//zIkSMUL14cExMTSpcuzYYNG9DT0+PcuXPqNJcuXaJBgwZYWFjg5OREhw4diIiIyLUYw3cc5PrIqTzeuDtH6T2/bEPSnQcED5pIwtXb3Ju1lLC1O/Du21mdxrtfF0L+XMWDRetICL7Fxa9HkvE0GffOLXWO88TuBRSr/BlFK7XEwcWP+u1HY2BkwoUja7WmP7VnMT6Fq1C+XjccnH2p1rQf+T0KcXr/XwCYmFrStt8CCpZuiH1+H1x9ilO37XDC7l8mNuqRznHu/nsJlWu3oFLNZri4+9L+q2EYGZtwZM8Grem9/IrwaacBlKlcH0NDw2zzVejrY23roP5YWNnqHCPAxvWrqVu/IbXqNsDdw4uevftjbGzMnp3btKYfMGgoDRs3xcfXDzd3D3r1/Y5MZSYXzp9Vp1m6aD4lS5elc9ev8PH1x9nZlbLlK2Fjo3usmZmZXPhnMaVq98C7SC0cXAKo1WYiiXFPuHMp+2P2/IGFFCrXioJlW2KX349qLUdjYGjC1ZOax4uBkSlmVvnUHyMTi7eOccfGZVSr24wqtT7B1d2HTj2HYGRswsE9m7Sm9/EvTJvOfSlfpS4GBtr/4N+8doESZatRvHRl8jm5UKZiLQoXL8ftG5ffOr7ntm1cTo26TalWuwmuHj50+fp7jI1NOLD7b63pff0L0a5LHypUrYuh4atxpqYkc/LIPtp07k1gkRLkd3GnZbvuODm7sWfbOp3jBGhe34mlG8I4ejqWOyFJTJx9B3sbQyqVssl2nWNnYzlxPo6Hj1N4GJbCgtWPSEpWUtDPHABlJkTHpmt8KpW24cDxaJJTlG8V3/ZnZV712b7s3PN7VZlnsy99/AvRpksfyr9mX546uo/Wnb8hsHBJnJzdad72Sxyd3dm7Tfs1Lif+Xr+aOvUbUauO6jz/qvcAjE1M2Ltzq9b0/QcOo0HjZnj7+uPm7snXfQY+O8/PaKQzNDTE1s5e/bGwtNQ5RlWcq6hdvzE16zTE3cOLL3t/+yzOLVrT9xs4gvqNm+Pt64+ruyc9+gwiU6nk4vnTGukiI8L5c840+g4cjr7+uw3C2Lh+DXXrN6R23fp4eHjRs3c/jI2N2b1zu9b03w76QeOa2bvvtyiVmZx/4Zr516I/KVW63AvXTBfKla/4TtfM901PXy/PPv+PpCLwfy4xMZEBAwZw6tQp9uzZg0KhoHnz5iiVSuLi4mjSpAlBQUGcOXOGsWPHMnjwYI31Y2JiqFmzJiVKlODUqVNs376dx48f89lnundtviub8sWJ2HtUY1n4rkPYli8OgJ6hIdYlCxOx50hWgsxMIvYewaZ8CZ22mZGeStj9y3gXrKhepqdQ4BVYkYe3z2pd5+Htc3gFVtBY5l2oMg9vn8t2OylJCaCnh4mplU5xpqelcf9WMAWLllMvUygUBBYtx+3rF3TK87knofcZ1K0OQ3s24s+pQ4gKD9U5r7S0NG7dvE7R4qU04ixWvBTXrl7JUR6pKSlkZKRjYaG6AVAqlZw6eQwXV3dGDRtEp7YtGNjva44dOaRznABxUQ94Gh+Ou39W2RubWuLkUZSwe+e0rpORnkr4w8u4FdA8Xtz8K7yyzvUzfzN/RHlW/NKEo1snk5aa9FbxpaelcffWVQoVLateplAoKFysLLeu6T4Uzi+gKFcunCTs4T0A7t+5zo3g8wSVrPiGNbOP887NqxQu/nKcZbh5Vbc4MzIyUCozMDQy1lhuZGTMtSvndcoTIH8+I+xtDTl7OauV/mmSkqu3Einkb56jPBR6UL28LSbGCq7cTNSaxt/LDD8vM7bvf7uGledlXrhYmaztPd+XOpa5el++VEkwMjLmRrBu+1J1nl975TwvqsN5bvnSjf6li+fo3K4Zvb/swNzfphAfF6tTjM/jvH3zOkWLl9aIM6h4Ka5dzVnFV309ssy6diuVSmZMHkfTlm1w9/TWOb7nMd66eZ1ixUtqxFiseMkc78uU5/tS45p5HBdXN0YOG0zHti35rl+vd75mig+LzBH4P9eypWYL+Pz588mXLx9Xrlzh0KFD6OnpMW/ePExMTChUqBAPHz6ke/fu6vQzZ86kRIkS/Pjjjxp5uLu7c/36dQoUKPCv/S7PGTs5kPJY8w9nyuMIDK0tUZgYY2hrjcLAgJQnkS+licQ8wEenbT5NiCZTmYGZpWaXurmVPZFht7WukxAXgbmVwyvpE2K1/9FPT0th/7pJFCrTCGPTt28VBkiIj0apzMDSRjNOK2t7wh7e1SlPAG//IDr3HoOTixex0RFsXj2HX4Z9wcipazAxzdmN0Yvi42JRKpXY2Gq2Olnb2PIg5H6O8li04Hds7ewpVkJ1kxEbE0NyUhLrVi+nfccudOzyJWdPn2Di+JGM/WkKRYKKvXWcAE/jVd3opi+VvamFA0/jtZdlcuKz48XipXUsHYh+ckf93b9kYyxtXTC3ciQy9DpHt0wi5sldGnSekeP44uNjUCozsLbRHPZmZW1H6IO7Oc7nZY1adiIpKYEhvVuhUChQKpW0bN+TitUa6JRffJz2OK1t7Ah9Vtl4W6Zm5vgHBrFh5Xxc3bywtrHjyMGd3Lh2CSdnN53yBLCzUfWMRcemaSyPjk3D1ib7XjMAL3cTpo8KxMhQQVJyBqN/vcX9h8la09avbs+9h0lcuaG9opCd1+7LB7rvS7+AIDatmo+LmzfWNnYc/WcnN69dxCm/bvtSfZ6/FKeNjS0Pc3ieL14wF1s7B43KRIlSZSlXsSpO+Z0JC33I0kV/MHbkYCZM+g19fX0d48zA+qVWcBsbuxzH+deCOa/EuWHNMhT6+jT85NO3jullcdlcM21sbHkQEpKjPBYvmIedlmvm2tUraN+xC526dOfM6ZP8NH4U436arPM1U3xYpCLwf+7GjRuMGDGC48ePExERgVKp6n6+f/8+165do2jRopiYmKjTly1bVmP98+fPs2/fPiwsXr0xvXXrltaKQEpKCikpmmOnjY2NX0knsmRkpLH+975kZmZSv93bTxR934qUzJp/4eZVAO8CRRjSoyGnDu+kcu3m/3o8a1ct49CBfYybOAWjZ2NZMzNVx3bZ8hX5pHkrAHx8/bgafJkdWzfl+I/a9TN/s3/NSPX3Rl3n5HL0WQqXb63+v71zAGaW+dg0tzOxEfexdvB4b9vNiROHd3PswHa+GjAOV3cf7t+5zrL5U7Cxy0flmo3zNLYX9eg/innTx/FNl8YoFPp4+QZQoUpd7t66muM8ala0o1/XrP097JebOsfz4FEKPX4IxtxUnyrlbBjYw4tvx11/pTJgZKhHzYp2LN2ge89abvuy/2j+nDGWfl80QqHQx9M3gPJvuS9z07pVSzl8cC9jfpqK0Qu9PpWr1VL/39PLB08vX77u1o7LF89p3Ij/W9av+ovDB/cw6qfp6jhv3bjG1o1r+Hn6H+80uT63rFm1nH8O7GP8xMnqa6by2TWzXPmKNG2uqqw8v2Zu3/r3f7YioPg/HaKTV6Qi8H+uSZMmeHp6Mm/ePFxcXFAqlRQpUoTU1NQcrZ+QkECTJk2YOHHiKz9zdnbWus6ECRMYPVrzZnbkyJGU0Zr67aU8jsDYSbOl3djJgbTYeJTJKaRGRKNMT8fY0f6lNPakhOk2t8HMwhY9hf4rEz0T4yKxsNY+AczCyoHEuIg3ps/ISGPD7/2Ii3pE2/6LdO4NALCwtEWh0Cc+RjPOuNhIrF8zEfhtmZlb4eTsQXhYzlqiXmZpZY1CoSAmWnOSW2xMNLZ2r5/QvWHtStauXs6Y8ZPw8vbVyFNfXx93D0+N9G7ungRfzvlwCa9CNWg9IOvJPhnpqnMlKT4ScytH9fKkhAjsXbQ/+cTE/NnxkqBZDknxEZhZZV8Oz58oFBt5L8cVAUtLGxQK/Vcm3MbFRmU7KTQnVi2cRsOWnShfpS4A7l5+RIaHsnntQp0qApZW2uOMjYl6pWX7bTg5uzFswhySk5NIepqIrZ0DM34eSr78LjnO4+iZGK7eymqVN3z2VBJba0OiYtLVy22tDbl17+lr80rPyOTRsycL3bj7lAAfc5rXc2TafM2W5arlbDE2VrDrH+2Te1/ntfvyHcrcydmNH36cS8qzfWlj58BvP/+Ao5OrTvmpz/OX4oyJicbG9k3n+QrWrVnGqPGTNc5zbfI7u2BlZU1o6EOdKgKqOPVfmRgcExP1xjg3rl3O+jXLGDF+ikacwZfPExsbTY/OrdTLlMoMFv85iy0b1zB7waq3itEqm2tmTA6umevXrmLd6uWMHv+LRoxW2Vwz3d09uHL50lvFJz5cMkfg/1hkZCTXrl1j2LBh1KpVi4IFCxL9wkUkICCAixcvarTenzyp+SjDkiVLcvnyZby8vPDz89P4mJtrHxIyZMgQYmNjNT5DhgzJtd8r5tg57GuW11jmUKsi0cfOAZCZlkbsmcs41HxhfL6eHvY1KhBzTPt4/jfRNzAiv0dh7gZnzU3IVCq5d/Uorj7a5x24+hTn3tVjGsvuBh/B1ae4+vvzSkDUk3u07bcQM4t3m6BlYGiIh29Bgi+eUC9TKpVcvXACnwLZP+b0bSUnPSX88QOsbXWrXBgaGuLrV0BjAqBSqeTCuTMEBGb/qMJ1q1ewavlfjBw7Eb8CAa/k6VcggIcPNCsnjx6GvNVj8IxMLLB28FR/bJ38MLPMx4MbWWWfmpzA4/sXyO9ZXGse+gZG5HMtzMMbmsfLg5vHsl0HIOKRquXVzNIx2zQvMzA0xMs3kCsXss5dpVLJlQsn8Q3Q/qjKnEhJTUGhp/knQqFQkJmZqVN+BoaGePsFcvm8ZpyXL5zEL1D3OJ8zMTHF1s6BxIQ4Lp49RqmyVXO8blKykkePU9Sfew+TiYxOo0ThrHHpZqYKAn3N33oYj56eqvX/ZfWrOXD0TCyx8ela1nq97Mv8FH7vUObPGZuYYvNsX146d4wS5XK+L1+kOs8DuHDu5fP89GvP8/VrlrNmxRKGj/kZP3/tj8Z8UUTEE+Lj47DVsRJkaGiIj18BLp7LmuirVCq5eO4MAYHaH8cKqqE/a1csZtiYX16Js1rNekyeuYBJM/5Uf+zsHfikRRuGjZ2kU4yqa2bW3zDVvjybw2vmT/jn8Jr58OEDHP+jjw4F0FPo5dnn/5FUBP6P2draYm9vz++//87NmzfZu3cvAwYMUP+8Xbt2KJVKvvzyS4KDg9mxYweTJqkuUM+7Mnv16kVUVBRt27bl5MmT3Lp1ix07dtClSxcyMjK0btfY2BgrKyuNz+uGBumbm2FVLBCrYqoLqZm3G1bFAjFxV/U4BIwbQLEFWT0S935fgZm3O4ETBmIe4INnj3Y4t2rAnWkL1WnuTF2Ae9fPcO3QDItAH4r8NgoDc1NCFun+JJGytbtw7tAqLhxdT0ToLbYvG0VaahJFK7YA4O8Fg9i/frI6felaHbl9+R+O75pPZNgt/vl7BqH3LlGquuoZ/RkZaayf24fQe5f45ItJKJUZJMSGkxAbrm6F1kXtJh04tHsdR/dtIvTBbZb9Pp7UlCQq1mwKwILpw1j/13R1+vS0NELuXCXkzlXS09OJiXxCyJ2rPAnNasFcs2gK1y+fIuLJQ25dPcecn/ujUOhTpnJ9neNs2rwVu7ZvYe/uHYTcv8ec36aSnJJMrTqqPKdOmsCSBfPU6detXs6yJQvo3W8gjo75iY6KIjoqiqSkrMm1zVu25vA/+9m5fTOhjx6y5e/1nDx+lAaNm+ocp56eHkWrdOT0njncubyXyNBr7Fk+GHMrR7yL1Fan2zinMxcP/aX+XqxaZ64cX83Vk+uJenyLA+tGkZ6aRGAZ1fESG3GfU7tm8eTBJeKiHnDn8l72rBiMi09pHFwCXonjdeo1bceBXRs4tHczj0LusHjOT6QkJ1GlVhMAfp86ktVLZqrTp6elce/2Ne7dvkZGehrRUeHcu32Nx6FZNwTFS1fm7zULOHfqEOGPH3H62D52bFpGqXLVddmNADRo2pb9OzdycM8WHobcYcHsiaQkJ1OtlqqHYc6vo1i56LeX4rzOvdvXSU9PIyoqnHu3rxP2KCvOC2eOcf70UZ6EPeLi2eOMH/o1zq6eVK3dROc4AdZvf0y7Zs5UKGmNl7sJg3p4ExmTxuHTMeo0Pw/xp2mdfOrvX7R2ISjQAicHI7zcTfiitQvFClqy57Bmi7iLkzFBgRZs26f709fqN23HgZ0b1WW+aM5EVZnXVu3Lub+OZNXibPZlWhrRkap9+WKZXzxzlAtnjhL++CGXzh3np2E9cXb1Uh9HumjSvBW7d2xm3+7tPLh/j7m//UpKcjI166jmmkyb/CN/LfxdnX7d6mUsXzKfXv0GPTvPI4mOiiQpSdUTk5T0lEV/zuba1cs8eRzKhXOn+WnMMPI7u1KilO79zk2af8buHZvZv3sbD+7fZd5vk0lJTqJGnYYATJ88nqULs96lsn71UlYs+ZOv+w0mn5Y4La2s8fDy0fjo6xtgY2uHq5tuw/6aNv+UnVqumbXr1APg10k/sXjBH+r0a1cvZ+mShXzT77vXXjMP/bOfndu3PLtmbnh2zfxEpxjFh0eGBv0fUygUrFixgj59+lCkSBECAgKYPn061atXB8DKyoq///6bnj17Urx4cYKCghgxYgTt2rVTzxtwcXHh8OHDDB48mLp165KSkoKnpyf169dHocideqR1qSJU2LNE/b3QpB8ACFm8jgtdh2DsnA9T96xhSEl3H3Dyk68oNHkIXt90JPlBGBe/GkbErqwnHYSu3oZRPjsKjOyjeqHY+WBONO5G6hPtz/zPiUJlGvI0IYp/Nk0nMS4cR7eCfNbnD/WE4LioUPReaEF18y3JJ90mcXDjVA5smIKtoxcte/5GPlfVvIr46MfcOL8XgPnjNG9U2w1YjGdAOXRRplI9EmKj2bRiNnExEbh5B9Bn2Cysnk0gjooI1RizGhP9hHHftVF/37VpMbs2LaZA4VJ8O+ZPAKIjH/PHr0NIjI/BwsoWv4Il+H7CYiytdR/SUblaDWLjYli+ZAHR0dF4+/gycsxEdVd8ePgT9F44xrZt2UR6eho//zhKI5/W7TrS9vPOAJSvWIUevfuzdtUy/pgzExc3dwYPHU2hwu/WSlqiRjfSU5PYv2YEqUlxOHuXonH3eRgYZlVw4yLvk5SY1ePmX7whyQlRnNgxg6fx4Ti4FKRxt3mYWaqOF4WBIQ9uHOH8P4tIT03CwsYZn6C6lK799u+RKFe5LvGxMaxfPvfZi7oK8O3I6Vg/K/PI8DCNMo+OCmfkgKyXxm3f8BfbN/xFQOGSDBmvutn5/MuBrFs6hyVzJxIXG42NrQPV67Wg6Wfd3jq+58pXqUNcbAxrl/1ObHQknj4FGDRqqno4S0T4Y41zKDoqnKH9Oqi/b12/lK3rlxJYpCTDfpwNwNOnCaxaPIuoiCeYW1pRtkINWnXoiYHBu/15W7n5MSbGCvp19cTCTJ9L1xMYMvEGaWlZPSLOTsZYWWZtx8bKkEE9vLCzMSTxaQZ3QpIYMvEGZy7Fa+Rdv5o9EVFpnL74du8OeFG5KnWIi4tm3bN96eFdgO9GTlOXeVTEY41rdHRUOCP6Z5X5tg1/sW3DXwQWKcmQ8ap5ME+fJrB6ySyin+3L0hVq8unn77YvK1etSVxsDMv/WkBMdBTePn4MH/Oz+jyPCH+M4oVjc8fWjaSnp/HLjyM18vmsXSfatO+CQqHPvbu32bdnB08TE7C1s6d4iTK07fCF1sei5lSlqrWIi41hxV/zVS8+8/Fj6JhJ2ca581mck37UfH9Bq3adad3+C53jeJ0q1WoQFxfLsiULX7hm/vRCjE9QvNBqvX3L36SnpzHxR82hum3adaTt550AqFCxMj1792PNquXMmzMTVzd3vh866p2vmeLDoZepaz+v+L+0dOlSunTpQmxsLKamprma9xbDt2vlzAuN0q6xcH9eR/F6navD/ktv94jJvFC9iCnBtx7mdRhvVNDXlWl///cvg32b6HE0WPcbx39DhYJWnLwWk9dhvFGZABvqtD/95oR5bNfSUhy7qvtjMf8t5QOtuXzzvzPpWZvCfs5cvPk4r8N4oyA/J67eepDXYbxRoK/uT+V6V0dK59aMw7dX8dTJNyf6wEiPwEdu8eLF+Pj44Orqyvnz5xk8eDCfffZZrlcChBBCCCHEf4tUBD5yYWFhjBgxgrCwMJydnWnVqhXjx4/P67CEEEIIIV4hjw/NXVIR+MgNGjSIQYMG5XUYQgghhBDiXyYVASGEEEII8UH4f32MZ16Rx4cKIYQQQgjxEZKKgBBCCCGEEB8hGRokhBBCCCE+CDJZOHdJj4AQQgghhBAfIekREEIIIYQQHwQ96RHIVdIjIIQQQgghxEdIKgJCCCGEEEJ8hGRokBBCCCGE+CDoKaQNOzfJ3hRCCCGEEOIjJD0CQgghhBDigyBvFs5d0iMghBBCCCHER0gqAkIIIYQQQnyEZGiQEEIIIYT4IMibhXOX9AgIIYQQQgjxEZIeASGEEEII8UGQycK5S3oEhBBCCCGE+AhJj4AQQgghhPggyAvFcpdeZmZmZl4HIYQQQgghxJtcaFg9z7ZddOv+PNv2+yI9AuJfs3B/XkfwZp2rwxbDgLwO47UapV1jz8XkvA7jjWoFmTBvd15H8Wbda0PPX2LyOow3mj3Qhm1n0/I6jNdqUMKQ2dvzOoo361kf+k6Lz+sw3mhaX0tOXovJ6zDeqEyADZvPpOd1GK/VuKQBuy+k5HUYb1S7qDH3bl7L6zDeyNPvv/13UuScVASEEEIIIcQHQSYL5y4ZaCWEEEIIIcRHSHoEhBBCCCHEB0FeKJa7pEdACCGEEEKIj5BUBIQQQgghhPgIydAgIYQQQgjxQZDJwrlLegSEEEIIIYT4CEmPgBBCCCGE+CDIm4Vzl+xNIYQQQgghPkJSERBCCCGEEOIjJEODhBBCCCHEB0EmC+cu6REQQgghhBDiIyQ9AkIIIYQQ4oMgPQK5S3oEhBBCCCGE+AhJj4AQQgghhPggSI9A7pIeASGEEEIIIT5CUhEQQgghhBDiIyRDg4QQQgghxAdB3iycu6QiIP4TTu9byvFdf5IQG46jWyB12wzHxbtotumDT2/j4MZpxEY+xM7Ri+otvsMvqBoAGRlpHNwwlVuXDhITEYKxqQVeBStSvfm3WNo46RyjXeXS+HzbFeuSRTBxceRUy695vGnP69epWpZCk77HopA/ySGh3JwwmweL12uk8ezZDp8BXTHOn4+4C1e53G8ssScv6hwnwIFtK9i1aRFxMRG4eRbgs67f4+UfpDXto5CbbF4xi/u3g4kKf8SnnQdSs/Hn2ea9Y/2fbFw6nRqN2tOqy6B3ijMzM5PDW6Zz8fBqUpLicPEpSZ02o7B19HrtemcPLOXk7j9JjAsnn2sgtT4bjrNX1vFy/tBKgk9t5knIZVKTE+n9y0lMzKzeKdbGlUyoXNQIU2M9bj9KZ9nOJMJjlNmmr1fOmOL+huS31yctLZNbjzLYcCCJx9FZ61QuakSZgka4O+ljaqzHgOmxJKVk6hTfPzuWs/fvBcTHRuDiEUDLLj/g6ae9zENDbrJt9UxCbl8hOuIRzToOpnrDDhppbgWfYu/fCwi5c4W46HC++HYaRcvU0im2F2VmZnJs23QuHn1W5t4lqdnqzWV+/p+lnNr7J0/jwnFwDaRGy+Hk91SVeXJiDEe3zeD+tUPERYdiZm6Hb9HaVGjYF2NTS51jbVDeiApFDDE11uPOowxW70smPCb78vF10admKSPcHRVYWyj44+8kLt5O10hjZAhNKhlT1McAM1M9omKVHDyfxuGLaW8d364tq9myfimx0ZF4ePvT8ctv8S1QWGvaB/dvs3bpXO7cukbEk1A+79qP+k3baqRJeprImqVzOXXsAHGx0Xj5FODz7gPw9S/01rG96NDOZex/4dhs3vkHPPy0X9/DQm6yfc0MHjw7Npt2GEzVhh3fKc+cOrB9Bbs3LSQuJgJXzwJ89sWQ1143t6z8TX3dbNl5IDUbaZ5DB3es5J+dq4gKfwSAs5svDVp9ReESVXSOcdPmLaxeu56o6Gh8vL3p1eNLAgMKaE176PARlq9aw6PQUNLT03F1ceHTFs2oXbOGRprN27Zz4+Yt4uPjmT19Kr6+PjrHJz48Uq0See7Kya3sWTOByo168cXQ9Ti5BbJyelcS4yK1pn9w6wwb//iWYpU+5YthG/AvXou1s3sR/vA6AGmpyYSFXKFSo550GbqOFj1mEhl2hzW/9XynOPXNzYi7cI1LfUbnKL2plxtlNs0lcv9xDpVuyp0ZiwiaOw6HOpXVaZxbNaDgL0O4Me43DpVtTvyFq5Tb8idG+ex0jvPU4e2sXTSJRq2+YsjPK3D1CmDGuJ7Ex2rfn6kpyTg4udGsfR+sbBxem/fdm5c4tGsNrp7a//C8rRO75nF2/xLqtBlF+4GrMDQyZc3MrqSnpWS7ztXTW9m/bgIVGvaiw/frcXQLZM3MriTGZ/1+6alJeBeqQrl6PXIlzrpljalR0phlu57y89J4UlKhTytzDPSzX8ff3YADZ1P5+a94pq1OQF8B37SywMgwK42RoR6X76Sx/VjyO8V35sg2Niz5mfqf9uS7Catx9QxgzoSvsi3ztNQk7B3daNKuX7ZlnpKchItnAJ92GfpOsb3s1J55nD24hFqfjaJNf1WZr5/z+jK/dmYrB9dPoHy9XrQbuJ58LoGsn92Vp8/KPCH2CYmxT6jSdDAdvt9M3fYTuBv8D7uW6x57rVJGVC1uxKq9Kfy68impaZn0aGb22jI3MoSHERms2Z/979K8ijEFPQ1YsiOZCYsT2X8ujZbVjSni/ZqMtTj2zy6W/jmN5m26Mu7XRXh4+TFxZF9iY6K0pk9JSSZffldad/waa1t7rWn+mPkjl86doGf/UUyYvpQixcvx0/DeREU+eavYXnT26DY2LfmZui2/pv+Pq3HxDOD3n7I/NlNTk7B3dKdR2/5YZnNsvm2eOXH68HbWLfqFhq168P3Elbh5BjBzfI/sz6GUZOwd3Wjavm+255CtvRNN2/dj8MQVDPppOQWKlGXuxL48CrmpU4z7D/7D3Hl/8nm7Nsya/is+3l78MHwk0TExWtNbWlrStnUrpk36mbm/TadenVpM+nUap06fUadJTkmhSKFCdOvSSaeY8oJCXy/PPv+PpCLwkVizZg1BQUGYmppib29P7dq1SUxMBOCPP/6gYMGCmJiYEBgYyKxZs9TrffHFFxQtWpSUFNUfttTUVEqUKEHHjq+20OjqxO4FFKv8GUUrtcTBxY/67UdjYGTChSNrtaY/tWcxPoWrUL5eNxycfanWtB/5PQpxev9fAJiYWtK23wIKlm6IfX4fXH2KU7ftcMLuXyY26pHOcYbvOMj1kVN5vHF3jtJ7ftmGpDsPCB40kYSrt7k3aylha3fg3bezOo13vy6E/LmKB4vWkRB8i4tfjyTjaTLunVvqHOfev5dQqXYLKtRshrO7L22/HIaRsQlH9m7Qmt7LrwgtOg6gdOUGGBgaZZtvctJTFk4bQvseIzEzf7fWdVC1DJ/Zt5jy9XviV6w2+VwDadjpZxJin3DzfPb7+NSeBQRV/IygCi1xcPajTpvRGBqZcOlo1vFSqmZnytX9EmevYu8cJ0DNUsZsO5bMhZvpPAxXsnBrItYWCor7G2a7zsw1iRy7nEpopJKH4UoWb3uKvbUCD6esG769p1PYeSKFO6EZ7xTf/i2LqVDzU8pVb05+N19adRuBkZEJx/ev15rewzeIpp9/R8mKDdE30F7mhUpUoVHrPhQtW/udYntRZmYmZw8splzdnvgGqcq83uc/kxj7hFsXsy/zM/sXUKTiZxQu3xL7/H7U+kx1jbh8TFXmDi4FaNx1Bj5FamLj4IF7gQpUbNSPO5f2osxIzzbf16lWwpCdJ1K4dDudRxFK/tqZjLW5HkG+2XekB9/LYOvRVC7cyn6b3s76nAhO4+bDDKLiMzl6KY1H4Uo88r9dRWDbxuXUqNuUarWb4OrhQ5evv8fY2IQDu//Wmt7XvxDtuvShQtW6GGo5z1NTkjl5ZB9tOvcmsEgJ8ru407Jdd5yc3dizbd1bxfaig1sWUb7mp5St3pz8bn607DoSQyMTTuzXnqeHbxBN2n9HiYoNMcjm2HzbPHNiz+bFVKzVkgo1VNfNNl8Ox8jIlKPZXDc9/YrQouO3lK6U/XUzqHR1ipSsgqOzJ04uXnzSrg/GJmbcvX5BpxjXrt9Ig/p1qVenNp4eHvTt/TXGJsbs2Kn93ClWNIjKFSvg4eGOi7MzzZt+go+3F5euXFGnqV2zBp+3a0OJ4rlzrRQfHqkIfARCQ0Np27YtX3zxBcHBwezfv58WLVqQmZnJ0qVLGTFiBOPHjyc4OJgff/yR4cOHs2jRIgCmT59OYmIi33//PQBDhw4lJiaGmTNn5kpsGemphN2/jHfBiuplegoFXoEVeXj7rNZ1Ht4+h1dgBY1l3oUq8/D2uWy3k5KUAHp6mJi++w1sTtmUL07E3qMay8J3HcK2fHEA9AwNsS5ZmIg9R7ISZGYSsfcINuVL6LTN9LQ07t8OJqBoefUyhUJBYFB57lzT7Y/Pcyv/+JEiJasS+ELe7yI28gGJceF4BmSVvbGpJc5exXh0R3vZZ6Sn8jjkMp6BmseLR2BFHmVzvLwrB2vVMI+r97Ju7pJT4U5oBt4uOR9daWqsak16mqzb0J/spKen8eDOFQoEaZZ5gaDy3L1+Ple39a7iIh/wNC4c9wKaZZ7fsxihrynzJyGXNdbRUyjwKFCR0LvZl3lqcgJGJhYo9N9+BKy9lR7W5gqu38+qoCWnwr2wDLzf8ob9ZXdCMwjyMcDaXHU8+Lnpk89WwbV7Oa8MpqelcefmVQoXL6teplAoKFysDDev6jasMCMjA6UyA0MjY43lRkbGXLui23GUnp7KgztX8C+Sdb1WKBQUKFKeezf+Q3mmpRFyO1jj2qZQKAgsWo7buXQOKTMyOHV4G6kpSXgXePub7rS0NG7cvEmJ4sU1YixRvBjBV6++cf3MzEzOnjtPyIOHBBXRPnxMfJxkjsBHIPTZ+MAWLVrg6ekJQFCQatzjyJEjmTx5Mi1atADA29ubK1euMHfuXDp16oSFhQV//fUX1apVw9LSkqlTp7Jv3z6srHLnhvppQjSZygzMLDW7qs2t7IkMu611nYS4CMytHF5JnxAboTV9eloK+9dNolCZRhibWuRK3Dlh7ORAymPNmFIeR2BobYnCxBhDW2sUBgakPIl8KU0k5gG6jdFMiI9GqczAylpzf1ra2PP44R2d8gQ4dWgbIXeCGfzTMp3zeFliXDgAZlaasZpZ2pMYp70sk54dL+YvHy+W9kRlc7y8K6tnN2xxiZrzAeITleqfvYke0KqmKTcfqFqXc1NinKrMLV8uc+t3K/P3ITFeVeYvl5+ZpT2J8dmUeaL2a4SZpT1RT7SXeVJCFMd3zKJIxdY6xWn5rFzjn2pW2uKfZqp/pqs1B1JoU9OEMd0syMjIJDMTVuxJ5tajnFcE4uNiUCozsLbRHEJobWNH6MN7OsVlamaOf2AQG1bOx9XNC2sbO44c3MmNa5dwcnbTKc/EZ3G+fGxaWNvz5JFux+b7yPP5dVPbORT2jufQw3vXmTS0A+lpqRibmNF94FSc3X3fOp+4uDiUSiW2NjYay21tbAgJeZjteomJibTt2IW0tDQUCgXffN2DUiV0a2j6r5D3COQuqQh8BIoVK0atWrUICgqiXr161K1bl08//RQjIyNu3bpF165d6d69uzp9eno61tbW6u8VKlTgu+++Y+zYsQwePJjKlStr24xaSkqKeijRc8bGxoCx9hXeo4yMNNb/3pfMzEzqt8vZ2H6hKSoijNULfuab4XNfaS18G1dObGLX8pHq7y2+npsb4eW6MgUNaVfXTP191tqEd86zTR1TXBz0mbQs/p3z+pBcPbWJPSuzyrzpV++/zFOSE9jw+1fY5felfIPeOVqnVIABrWuaqL/P3ZT0vsKjajFDPJ31+X3TU6LjM/F10efTGibEJiZxPeTdhoi9qx79RzFv+ji+6dIYhUIfL98AKlSpy91bb25xFto5uXgz5JfVJD9N4OyxXSyZOYx+o+frVBnQhampKbNnTCU5KZmz588z94/5OOfPT7Gi2idBi4+PVAQ+Avr6+uzatYsjR46wc+dOZsyYwdChQ/n7b9VY0nnz5lGuXLlX1nlOqVRy+PBh9PX1uXnzzZOcJkyYwOjRmjfdI0eOxKv6qFfSmlnYoqfQV0/6ey4xLhILa+0TsCysHF5pMdaWPiMjjQ2/9yMu6hFt+y/6V3sDQNX6b+ykGZOxkwNpsfEok1NIjYhGmZ6OsaP9S2nsSQnT3jr6JhaWtigU+sS9NMEtPibyjROBs3P/9hXiY6P4aVAb9TKlMoObwac5sG0F05efRKH/5uESfkVraozZz0hPBeBpXCQW1o7q5U/jI3F0C9Sah+mz4yXx5eMlPvKVXiJdXbiZxt3QrBv255NDrcwVxCVm3ahZmit48OTNN26ta5lSxMeQKSsSiEnI3WFBAOZWqjJ/eVJjfKzuZZ5bfIrUJL/nq2WeGB+J+Utlns81mzI3136NeBofibml5u+XmpzAhtndMDI2p0nX39DXz34Ox4su3U7nXlii+rvBs0mBlmZ6xL3QK2BppsfDcN17dAz1oXFFY/7cnMSVu6pj51GEEtd8CmqWNOJ6SM4qIJZWNigU+q9MDI6NiXqll+BtODm7MWzCHJKTk0h6moitnQMzfh5KvvwuOuVn/izOl4/NhNjIbCcC50Wez6+b7+McMjA0xNHZAwAP30Lcu3WJfVuX0u6rEW+Vj5WVFQqF4pWJwdExMdjZ2mS7nkKhwNVFVX6+vj7cD3nAitVrPuiKgDw+NHfJ3vxI6OnpUalSJUaPHs3Zs2cxMjLi8OHDuLi4cPv2bfz8/DQ+3t7e6nV/+eUXrl69yoEDB9i+fTsLFix47baGDBlCbGysxmfIkCFa0+obGJHfozB3g7PG0mcqldy7ehRXH+3dl64+xbl39ZjGsrvBR3D1Ka7+/rwSEPXkHm37LcTMwvZNuyjXxRw7h31NzfH0DrUqEn3sHACZaWnEnrmMQ80X5jvo6WFfowIxx3Qb725gaIiHT0GuXTyuXqZUKrl28TjeAbo9Wi8wqBzDpqzhh0kr1R8P38KUqdKQHyatzFElAMDIxAJbR0/1x97ZD3OrfNy7llX2KUkJhN49j4u39rLXNzDCyb0w969pHi/3rx3FJZvj5W2lpEF4jFL9CY1UEpugJMAjq93ExEg16fPOo9dPRG1dy5Ti/oZMXZlAZGzuDgl6zsDAEDfvQty4pFnm1y8dx0uHsci5ycjEApt8nuqPXX4/zKzyEXL9hTJPTiDs3nmcX1Pmju6FNdbJVCoJuX4UZ6+sdVKSE1g3uysKA0M+6T4bA8Oc916lpEFEbKb6ExalJDZRSQH3rGPb2Ag88+tzJ0z3VnuFvqqSkflSfVCZCXpvMdrBwNAQb79ALp8/mZWHUsnlCyfxC3z3GzwTE1Ns7RxITIjj4tljlCpbVad8DAyMnh2bWddrpVLJjcvH8fTX7dh8L3kaGuKezXXTJ5fPoUylkvS01Ldez9DQEH8/P86dy5qzoFQqOXfuAgUDtVeitW4/U0la2ts/qlb8/5IegY/A8ePH2bNnD3Xr1sXR0ZHjx48THh5OwYIFGT16NH369MHa2pr69euTkpLCqVOniI6OZsCAAZw9e5YRI0awZs0aKlWqxJQpU+jbty/VqlXDx0f7OHZjY+NnQ4FypmztLmxeOJj8XkVw8SrKyT2LSEtNomhF1byFvxcMwtLGierNvwWgdK2OLJ3UgeO75uMXVI0rJ7cSeu8SDT4fAzwbDjS3D2H3r9Cq11yUygwSYlVjk03NrbN9Ssqb6JubYe7nof5u5u2GVbFAUqNiSQ4JJWDcAExcnTjfZTAA935fgefX7QmcMJCQhWtxqFEe51YNOPnJV+o87kxdQLH5E4k5fYnYkxfw6tMJA3NTQhbp/vSLmk06sHjmcDx9C+PpV4R9W/4iJSWJCjWaAbBw+lBs7B1p1r4voJooF/rgFgAZ6WnERD0h5M5VjE3McHT2wMTUHBcPf41tGBubYm5p88ryt6Gnp0fJGh05tn02to6eWNu7cXjzNCysHfErlvWkmlXTOuFXrA4lq6vebVC6Vhe2LR6Mk0cRnL2KcnrvItJSkihSvoV6ncTYcBLjIogJvw9AxKPrGBmbY2nnjKm5zVvHuvd0Cg0rGBMenUFErJImlU2JTVBy7kbWH9S+n5lz7kYaB86q/si3qW1KmYJGzFmfQEpapno+QVJKJmnP6g9W5npYmStwtFG1ybg6KEhOg6g45VtNKq7eqCPLZg/F3acwHn5FOLD1L1JTkihXrRkAf/02BGs7R5q07Q+oJhiHPS/zjDRiox7z4K6qzPPlVx3jKclPCQ+7r95G1JOHPLh7FXMLa2wdnN96H4KqzEtU68iJnbOxyacq8yNbp2Fu7YhvUFaZr53ZCd+idSheVVXmJat3YedSVZnn9yjKmQOqa0Shci2exZrA+llfkJ6aRP0Ov5CanEBqsmpIl6mFHQrF20/wPXA2jbpljQmPURIZl0nDCkbEJmZy8YUnAvVqYcqFm+n8c0F1HBgZQj7rrPY1e2s9XB0UPE3JJDo+k5RUuPEgnaaVjUlLTyEqXomfqz5lChqy4WD2jxzVpkHTtsydOgZvv4L4FijE9k0rSElOplqtxgDM+XUUtnb5aN2pF6A6zx+GqMa7p6enERUVzr3b1zE2MSW/izsAF84cIzMzE2dXTx6HhrB84QycXT2pWrvJW++/56o26sSK2T88OzaDOLhtCakpSZSt1hyAZbOGYG3rSCP1sZnK4xeuR7HRT3h4NxhjEzMc8nvmKE9d1GrckcW/DcPDtxBefkHsfXbdLP/surloxg/Y2DnRNLvrZqTmdRNg49JpFCpRCTsHZ5KTEjl1aBs3rpyi19A5OsXYsnlTfpkyFX9/PwILFGDdxk0kJydTr47q/R4/T/4Ve3s7unZWPQp0+arVFPD3wyW/M2lpaZw4dYrde/fTp1fWo7Tj4uMJfxJOZJSqdynkoWq+ga2tLXZ2/34DWk7IHIHcJRWBj4CVlRUHDx5k6tSpxMXF4enpyeTJk2nQoAEAZmZm/PLLLwwcOBBzc3OCgoLo168fycnJfP7553Tu3JkmTVR/CL788ku2bNlChw4dOHjwoMYQIl0VKtOQpwlR/LNpOolx4Ti6FeSzPn+oh3rERYWip5f1x9XNtySfdJvEwY1TObBhCraOXrTs+Rv5XFXPto+PfsyN83sBmD+uqca22g1YjGeA5jConLIuVYQKe5ZkxT3pBwBCFq/jQtchGDvnw9Q96wYp6e4DTn7yFYUmD8Hrm44kPwjj4lfDiNh1SJ0mdPU2jPLZUWBkH9ULxc4Hc6JxN1JfmkD8NkpXqk9CXDSbV8xSvVDMK4DeQ2dhZaMaghQdEYbiha7V2OgnTBiYNaly96ZF7N60CP9Cpek/5k+d48iJsnW6k5aaxM5lI0hJisPVtxQte/2h0ZobExFCUmK0+ntgqYY8jY/i8ObpPI0PJ59rQT7t9YfG0KBzh1ZwdGvWk61W/NoegPqfT6BIhawKQ07tPJGCkaEe7eqZYWasx62H6cxYk0j6C43D+Wz0sTDNWlCthOp3GNBW84VWi7Y+5dhlVWWhSjFjGlfKGpv+bTvLV9LkRMmKDUiMi2bb6pnPXoYUyFffz1EPlYiO0DyHYqOeMOn7T9Xf921eyL7NC/EtWJpvRi4E4P6tS/w29gt1mg1LfgagTNWmtP96fI5je1npWt1JT01iz8oRz14iV4rmPV4q80jNMg8o2ZCkhCiObp2ueqGYW0Ga9cgq8ychlwm7p2opXTi2jsb2uozYg7X920923XM6FSNDaF3L5NlL5DKYs+GpRpnbWyswN826KfFw1OebT7PmlzSvqirb41fSWLZL9a6IRduSaVLJmA71TTAz0SM6TsmWIylv/UKx8lXqEBcbw9plvxMbHYmnTwEGjZqqfkdARPhjjTKPjgpnaL+sF15tXb+UreuXElikJMN+nA3A06cJrFo8i6iIJ5hbWlG2Qg1adeiJgYHutwolKjQgMS6KHWuyjs3u389VH5sxEaHovdAdEhcdzpQhWcfm/s0L2L95Ab4Fy/D1iIU5ylMXpSrVJz4ums0rZxEfE4GrVwC9hs7WuG5qnEPRT/hp0Gfq73v+XsSev1XXzX6j5wMQHxvF4pnDiIsOx8TMAlfPAvQaOoeCxTSfepdT1atWITY2lsV/LSM6OhofHx/GjxmFra3qhv1JeLjGvkxOTmHGrDlERERibGSEu5sbg78bQPWqWS80O3bsBJOmTlN//3HiLwB83q4NHdu30ylO8WHRy8x8uZNSiPdj4f68juDNOleHLYYBeR3GazVKu8aei+/2Aqp/Q60gE+bl7JULeap7bej5S0xeh/FGswfasO3sf7tLv0EJQ2Zvz+so3qxnfeg77b8/cXtaX0tOXovJ6zDeqEyADZvP6Pa+hn9L45IG7L7wdr0ueaF2UWPu3byW12G8kadf3v2dvNut6ZsTvSdef2zMs22/L9IjIIQQQgghPggyNCh3yWRhIYQQQggh3oPffvsNLy8vTExMKFeuHCdOnHht+qlTpxIQEICpqSnu7u7079+f5OT3NwpAegSEEEIIIcQH4UN6fOjKlSsZMGAAc+bMoVy5ckydOpV69epx7do1HB0dX0m/bNkyvv/+e+bPn0/FihW5fv06nTt3Rk9PjylTpryXGD+cvSmEEEIIIcQHYsqUKXTv3p0uXbpQqFAh5syZg5mZGfPnz9ea/siRI1SqVIl27drh5eVF3bp1adu27Rt7Ed6FVASEEEIIIYR4g5SUFOLi4jQ+KSnaJ6GnpqZy+vRpatfOejSyQqGgdu3aHD16VOs6FStW5PTp0+ob/9u3b7N161YaNmyY+7/M85jeW85CCCGEEELkIj2FXp59JkyYgLW1tcZnwoQJWuOMiIggIyMDJycnjeVOTk6EhYVpXaddu3aMGTOGypUrY2hoiK+vL9WrV+eHH37I9f34nFQEhBBCCCGEeIMhQ4YQGxur8RkyZEiu5b9//35+/PFHZs2axZkzZ1i3bh1btmxh7NixubaNl8lkYSGEEEII8UHIy8nCxsbGGBsbvzkh4ODggL6+Po8fP9ZY/vjxY/Lnz691neHDh9OhQwe6desGQFBQEImJiXz55ZcMHTpU42WguUV6BIQQQgghhMhFRkZGlCpVij179qiXKZVK9uzZQ4UK2t8u/fTp01du9vX19QF4X+//lR4BIYQQQgghctmAAQPo1KkTpUuXpmzZskydOpXExES6dOkCQMeOHXF1dVXPM2jSpAlTpkyhRIkSlCtXjps3bzJ8+HCaNGmirhDkNqkICCGEEEKID4Peh/Nm4datWxMeHs6IESMICwujePHibN++XT2B+P79+xo9AMOGDUNPT49hw4bx8OFD8uXLR5MmTRg/fvx7i1EqAkIIIYQQQrwHvXv3pnfv3lp/tn//fo3vBgYGjBw5kpEjR/4LkT3b5r+2JSGEEEIIId6BnuLD6RH4EMhkYSGEEEIIIT5C0iMghBBCCCE+CHn5+ND/R7I3hRBCCCGE+AhJRUAIIYQQQoiPkAwNEkIIIYQQHwSZLJy7pEdACCGEEEKIj5D0CAghhBBCiA+CTBbOXbI3hRBCCCGE+AjpZWZmZuZ1EEIIIYQQQrxJ2MDP82zb+X/5K8+2/b7I0CDxr9l/KSmvQ3ij6kVM2XMxOa/DeK1aQSZsMQzI6zDeqFHaNRbsy+so3qxLDRizND2vw3ijEe0NWH8iI6/DeK3mZfVZdui/37bUrrIejbpdyusw3mjLH0WIPn8gr8N4I9ti1Th3Izyvw3it4v75uHHrXl6H8Ub+vp4kHt2Q12G8kXmFZnm2bZksnLtkaJAQQgghhBAfIekREEIIIYQQHwTpEchd0iMghBBCCCHER0gqAkIIIYQQQnyEZGiQEEIIIYT4MMh7BHKV7E0hhBBCCCE+QtIjIIQQQgghPgh6ejJZODdJj4AQQgghhBAfIekREEIIIYQQHwQ9mSOQq2RvCiGEEEII8RGSioAQQgghhBAfIRkaJIQQQgghPgjyZuHcJT0CQgghhBBCfISkR0AIIYQQQnwYZLJwrpK9KYQQQgghxEdIKgJCCCGEEEJ8hGRokBBCCCGE+CDIZOHc9UH2CNy9exc9PT3OnTuX16HkioULF2JjY5PXYQghhBBCiI+I9Ahko3r16hQvXpypU6e+9221bt2ahg0bvlMeCxcupF+/fsTExOROUP+yfdtWsGvjImJjInHzKkCbroPx9g/SmvbR/ZtsWjGb+7evEBkeSqsu31G78ecaaf5eOZvNq+ZqLHNy8WLMjA3vFOeBbSvYtWkRcTERuHkW4LOu3+OVXZwhN9m8Yhb3bwcTFf6ITzsPpOZLcb5ox/o/2bh0OjUatadVl0E6xWdXuTQ+33bFumQRTFwcOdXyax5v2vP6daqWpdCk77Eo5E9ySCg3J8zmweL1Gmk8e7bDZ0BXjPPnI+7CVS73G0vsyYs6xfii0/uXcnznnyTGhePoFkid1sNx8S6abfqrp7dxcNM0YiMfYufoRfXm3+EbVE3982tnd3L24ArC7l8mOTGGLkM34ORe8J3jBKheVEEJPz1MDCEkPJOtJ5VExWef3sMRKhZU4Gynh6WZHisPZHDtQaZGmkB3PUr56+Fsp4eZsR5zt6bzOFq3+I7uWsaBrfNJiI3A2T2ATzoOxd03+3154fh2dq2dQXTEQ+ydPGnQegCBxbP2ZUpyIttX/srl03t4mhCDXT5XKtb9nPK12ugW4DMn9i7lyPY/SYiNIL97IA3aDcPVJ/s4L5/czr4N04h5FmftT7/Dv2g1rWk3Lx7J6QMrqddmCOXrdHqnOAE+b+pIvSq2mJvpE3zzKb/99YhHT1KzTd+wuh0Nq9vhZG8IwL1HKSz/+wmnLyVoTT+6ryelgywZO/Mex8695mDKxprt+/jr751ExcTi5+nGt1+0pbCf9xvX23X4BMOn/UHV0sX4eVAv9fLMzEzmrdrExj3/kJCYRFCgL4O6tcfD2emtY3vRjs1r+XvdcmKio/D09qXLV/3xCyikNe2e7Zs4uHc7IfduA+DtF0Dbjl+p06enp7Nyye+cPXWMJ2GPMDM3p0ix0rTr3BM7e4d3inPz35tYt3Y10dFReHv78FXPXgQEBGpNu337Vvbu2c29e3cB8PPzp2OnLhrpk5KSWLjgT44dPUJ8fBxOTvlp8kkzGjZqrHOMK3cfYfG2g0TGxlPAw5lBnzeliI/7G9fbcewcQ+Ysp3qJQkzpq3lu3H70mOmrtnHm2m3SM5T4uDrxS+/Pcba31TnO90lP74Nsw/7Pkr35H2Bqaoqjo2Neh5FnTh7ewZqFk2n02VcM/WU5bp4FmD72a+Jio7SmT01NxsHJleaf98XKJvsLv4u7Lz//sVv9GTR+wTvFeerwdtYumkSjVl8x5OcVuHoFMGNcT+JjI7XHmZKMg5Mbzdr3eW2cAHdvXuLQrjW4ehZ4pxj1zc2Iu3CNS31G5yi9qZcbZTbNJXL/cQ6VbsqdGYsImjsOhzqV1WmcWzWg4C9DuDHuNw6VbU78hauU2/InRvns3inW4FNb2btmApUb96LLD+txdAtk5YyuJMZp358Pbp1h45/fUqzSp3QZugH/4rVYO6cX4Q+vq9OkpTzFza8kNZp/906xvaxiIT3KBuix5YSSP3dkkJYO7Wvoo/+aK6iRgR6PY2DrSWW2aQwNIORJJnvOZp8mJ84f28bmZROp3fxrvhm7BmePQP78+UsSsjk2710/y4pZAyldrQV9xq6lcKlaLJn6DWEhN9Rptiz9mesX/qF1z4kMmLiZSvU6smnxeK6c2atznJdObGXnyp+o9kkvvhq5Dif3AP76tVu2ZR5y8wxrf/+WElU+5auR6wkoUZsVM3vz5MH1V9IGn9nFg9vnsbTJnWvpp/UdaFLLnt/+esSAH2+RnKJkbH8vDA2yH5YQEZ3GwrVh9B17i77jbnHhagLDe3vg4WL8StpmdezJ1JJHTu06cpJpi1fT7dPGLJo4DH9Pd/qNn0ZUbNxr13v0JILpS9ZQvKD/Kz9bsnEHq7btZXD3z/njxyGYGhvTb/w0UlLTdI7zyME9LP5jJi3bduGnaX/i6e3HjyMGEBujvcZ7+eJZKlarzYgJMxg7aS72+ZwYP2IAURHhgOq6eufWdVq26cRP0+Yz4IfxhD68zy9jB+scI8DBA/v5Y95c2rb7nGkzZuHt48OI4T8Qk02cFy+cp1q16kyY8AuTJk8ln0M+RgwbQkREhDrNH/PmcOb0Kb4dOJjZc/+gabPmzJk9k+PHjuoU447j55myYjNfNqvFstF98Hd3ptekP4mK017RfO5ReBS/rtxCiQKvVhJDnkTSdfwcvJwd+f37r1g5rj/dP6mFsaGhTjGKD89/tiKgVCr5+eef8fPzw9jYGA8PD8aPH6+R5vbt29SoUQMzMzOKFSvG0aOaJ9ehQ4eoUqUKpqamuLu706dPHxITE9U/nzVrFv7+/piYmODk5MSnn34KQOfOnTlw4ADTpk1DT08PPT097t69qzVOLy8vxo4dS9u2bTE3N8fV1ZXffvtNI82UKVMICgrC3Nwcd3d3vv76axISsk7cl4cGjRo1iuLFi7NkyRK8vLywtramTZs2xMdrbzHav38/Xbp0ITY2Vh3vqFGjGDNmDEWKFHklffHixRk+fLj6d23WrBmjR48mX758WFlZ0aNHD1JTs1q9lEolEyZMwNvbG1NTU4oVK8aaNWu0xqKL3X8voXLtFlSq2QwXd1/afzUMI2MTjuzZoDW9l18RPu00gDKV62P4mouVQl8fa1sH9cfC6t1aN/b+vYRKtVtQoWYznN19afvlszj3Zh9ni44DKF25AQaGRtnmm5z0lIXThtC+x0jMzK3eKcbwHQe5PnIqjzfuzlF6zy/bkHTnAcGDJpJw9Tb3Zi0lbO0OvPt2Vqfx7teFkD9X8WDROhKCb3Hx65FkPE3GvXPLd4r1xO4FFKv0GUUrtsTBxY/67UZjaGjChSNrtaY/tXcxPoWrUK5uNxycfan6ST/yexTi9P6/1GmKlG9G5Ua98Qys8E6xvaxcoIJ/Lim5/iCTJzGw4agSSzNVi352bj7KZN955Su9AC+6eCeTg5cyuR32LreEcGjbQspWb0Xpqi1wcvWjWZeRGBmbcOrgOq3pD+9cQoGilanWqCuOrr7U/bQPLl6FOLp7qTrNvRtnKVmlGb4Fy2KXz5VyNT/D2SOAkFu69wQd27mQklVbUaJyS/K5+NG4w2gMjUw4e0h7mR/fvQS/IpWpVL8r+Vx8qdm8L86ehTixd6lGurjox2xbNo4W3X9BoZ87Hd1Na9uzcvMTjp2L5+6DFCbPf4CdjQEVSmR/jp44H8+piwk8epLKo8epLF7/hOQUJYE+ZhrpfNxNaF7HgWkLHuoc3/LNu2haqzKNa1TC282Fwd3bY2JkxOZ9h7NdJ0OpZOSMP+n+2Se4OGo2TmRmZrJy6266tGhE1TLF8fd0Y2TvLkREx3Dw5Fmd49yyYQW16jWhRp1GuHl4063XQIyMTdi3a7PW9H0GjqReoxZ4+fjj6u5Jj28Gk6lUcvH8KQDMzC0YNm4qFarUwsXNgwKBRejSYwC3b14j4kmYznFuWL+WevUbUKduPTw8POnVuy/Gxsbs2rlDa/qBg4bQqPEn+Pj64u7uwTd9+6NUZnL+fNa+Cg6+Qs1atSlatBhOTvmp36AR3j4+XL92VacYl+74h+bVytK0Shl8XJ0Y2qk5JkaGbDx4Mtt1MpRKhs5dQY9mdXDT0njz25rtVCoaQL/WDQn0dMXd0Z5qJQphZ2WhU4ziw/OfrQgMGTKEn376ieHDh3PlyhWWLVuGk5Nm9+TQoUP57rvvOHfuHAUKFKBt27akp6cDcOvWLerXr0/Lli25cOECK1eu5NChQ/Tu3RuAU6dO0adPH8aMGcO1a9fYvn07VatWBWDatGlUqFCB7t27ExoaSmhoKO7u2Xe9/fLLLxQrVoyzZ8/y/fff07dvX3bt2qX+uUKhYPr06Vy+fJlFixaxd+9eBg16/dCPW7dusWHDBjZv3szmzZs5cOAAP/30k9a0FStWZOrUqVhZWanj/e677/jiiy8IDg7m5Mmsi8TZs2e5cOECXbp0US/bs2cPwcHB7N+/n+XLl7Nu3TpGj85qUZ4wYQKLFy9mzpw5XL58mf79+/P5559z4MCB1/4OOZGelsb9W8EULFpOvUyhUBBYtBy3r194p7yfhN5nULc6DO3ZiD+nDiEqPPTd4rwdTEDR8ppxBpXnzrV3i3PlHz9SpGRVAl/I+99iU744EXs1K9Dhuw5hW744AHqGhliXLEzEniNZCTIzidh7BJvyJXTebkZ6KmH3L+NVsKJ6mZ5CgVfBijy8rf2m49Htc3i9dIPvXagyD2+f0zmOnLCxAEtTPY2b9ZQ0eBgBbg55P2ktPT2Vh3ev4FdY89j0K1yBezfPaV3n3s1z+BXW3JcFgipx78Z59XdP/xIEn9lHbNRjMjMzuXXlOOFhd/EPqqRTnBnpqTy6dxmfl8rcp1AFHtzSHmfIrXP4FKqoscy3cCWN9JlKJev/GETFel1xdH21lVsX+R0MsbMx5FxwVsPR0yQl124nEehrmqM8FHpQtYw1JkYKgm89VS83NtJjYHc3Zi97RHRcuk7xpaWnc+32fcoEZQ17UygUlAkqyMXrt7Ndb/6azdhZWfJJzcqv/OzRkwgiY+IoUzQrTwszMwr7eb82z9dJT0vj9s3rBBUvrRFnUPHS3Lh6OUd5pKSkkJ6RjoVl9hWwp08T0NPTw8zCUqc409LSuHnzBsWLZ13TFAoFxYuX4OrV4BzHmZGRjuULMRQsWIgTx48RERFBZmYmF86f49HDh5QoWertY0xPJ/juQ8oVyjrGFQoF5Qr7ceHW/WzX+33jbuysLGhWrewrP1MqlRy6cBXP/A58PekPan0zho5jZrLvdM7KJs8o9PLu83/oPzlHID4+nmnTpjFz5kw6dVKNZfP19aVyZc2L13fffUejRo0AGD16NIULF+bmzZsEBgYyYcIE2rdvT79+/QDw9/dn+vTpVKtWjdmzZ3P//n3Mzc1p3LgxlpaWeHp6UqKE6iJgbW2NkZERZmZm5M+f/43xVqpUie+//x6AAgUKcPjwYX799Vfq1KkDoI4BVD0I48aNo0ePHsyaNSvbPJVKJQsXLsTSUnVR6dChA3v27HmlVwTAyMgIa2tr9PT0NOK1sLCgXr16LFiwgDJlygCwYMECqlWrho+Pj8b68+fPx8zMjMKFCzNmzBgGDhzI2LFjSUtL48cff2T37t1UqKC6cfDx8eHQoUPMnTuXatW0j9XNqYT4aJTKDCxt7DWWW1nbE/bwrs75evsH0bn3GJxcvIiNjmDz6jn8MuwLRk5dg4mpuc5xWllrxmlpY8/jh3d0jvPUoW2E3Alm8E/LdM7jXRg7OZDyOEJjWcrjCAytLVGYGGNoa43CwICUJ5EvpYnEPMAHXT1NiCZTmYG5leb+NLe0JzJM+01HQlwE5lYOr6RPjIvQmj63WJio/k1Meime5EwscnZP+F49jY9BqczAwlpz31hY2RP+KJt9GROBxUvHsoW1AwmxWfvyk45DWTd/JBP61kChb4Cenh4tuo7BJ7D0y9nlMM5sytzKgYhQ7edQQmzEK+ktrBxIeKHMD22bh0KhT7naHXSKSxtba9Wfxpdv1GPi0rG1fv2QCU9XYyYP8cHIUEFSipJxs+4TEpqi/nn31s4E33qq05yArDgSyFAqsbPRvDm2tbHk7iPtDR7nrt5g095DLPl5uNafR8aohhTZWWveTNtZW6l/9rbi4mJRKjOwttFsiba2sePRg3s5ymPpwlnY2TloVCZelJqawrIFs6lYtTZmZm9/bVfFGYdSqcTGVrPX2MbGlgchITnKY+GCP7Czs6d4iZLqZT169mLG9Kl07tgOfX199PQUfNO3H0WCsp8Tk52Y+KeqMrfWbKm3s7Lkbmi41nXOXr/DxoMnWT6mn9afR8Ul8jQ5lQVb9vN1y3r0bdWQIxev8d3MJfw++EtKBep+jRcfjv9kRSA4OJiUlBRq1ar12nRFi2adTM7OzgA8efKEwMBAzp8/z4ULF1i6NKsLOTMzE6VSyZ07d6hTpw6enp74+PhQv3596tevT/PmzTEzM3tlO2/y/Ab5xe8vTjLevXs3EyZM4OrVq8TFxZGenk5ycjJPnz7NdnteXl7qSsDz3+/JkydvHVv37t354osvmDJlCgqFgmXLlvHrr79qpClWrJhGHBUqVCAhIYGQkBASEhJ4+vSpulLzXGpqqrri9LKUlBRSUlI0lhkbvzpG9n0qUjKr0ujmVQDvAkUY0qMhpw7vpHLt5v9qLNmJighj9YKf+Wb4XAyN/t39I7JXxEuPxmWzOkuX78/Iw2jyzpGdf3H/5nk69v8NWwcX7lw7xcZFY7GyyYd/kYpvzuBf8OjuJY7vXsJXI9aip6d7a131ctb07uCi/j5qes5uUrV5GJbKN2NuYW6qoFIpawZ84cbgn+8QEppCuWKWFA00p8+YWzrnr4vEpGRGz5jPkK86YGOlW6t5XtiweglHDu5h5IQZGGm5RqanpzP1pxFkAt165e68oLexetUKDh44wISJv2BklDUM9O9NG7l29SrDR47G0dGJS5cuMmfWTOxfqjC8D4lJKQz/fSXDu7TE1lJ7BSkzU9XLWb1kYT6vVwWAAE8Xzt+8x5p9x/6zFQE9ebNwrvpPVgRMTXPWzPbi+PDnfwSUStWku4SEBL766iv69OnzynoeHh4YGRlx5swZ9u/fz86dOxkxYgSjRo3i5MmTufooz7t379K4cWN69uzJ+PHjsbOz49ChQ3Tt2pXU1NRsKwIvj33X09NT/25vo0mTJhgbG7N+/XqMjIxIS0tTz4XIiedzGbZs2YKrq6vGz7K7uZ8wYYLG0CKAkSNHUv3TVydzWVjaolDoEx+j2eIcFxuJ9Rsm2L4NM3MrnJw9CA/LWevOy57HGffS5Mv4mMg3TgTOzv3bV4iPjeKnQVlPYVEqM7gZfJoD21YwfflJFPr6OuWdUymPIzB20ozf2MmBtNh4lMkppEZEo0xPx9jR/qU09qSE6d4Sb2Zhi55C/5VJoonxka+0+j9nYeXwSuv/69Lr6vqDTOZGZN38GzwrAnNTSEh+IR4TPcKi321sf24ws7RBodDXaM0HSIiLxCKbY9PCxuGVicQJsRHqXoW01GR2rJ5Kh34z1E8ScvYI4NG9q/yzdaFOFQEzy2zKPC7ild4MdZzWDq+kT4iLwOJZmd+/cZrE+Eh+HVRT/fNMZQY7V07k2K5F9Ps5ZxObj5+L59qdrJvz5xOCba0MiI7N6hWwsTLgdkjSK+u/KD0jk9BnTxa6eS+ZAl6mNK1tz8wljygaaI5zPiNWTdd8ktUPX3tw+cZThvySs95FGysL9BUKol5qqY+OicfexvqV9A8fhxMaHsnAiVnz15TPbgIrtenByqljsH/WuxAVG4+DrY06XVRsHP5eb34qjTZWVtYoFPrExmg++CE2JgobW/ts1lL5e90yNq5ZyrBxU/H09nvl56pKwHDCn4Qx4sfpOvcGqOK0QqFQEBOtOTE4JiYaW7vXPxRh3drVrFm9knHjJ+LtnXXjnJKSwuJFCxg6bCRlyqqGvnp7+3Dn1i3WrVvz1hUBG0szVZnHak4MjoqLx9761crdgyeRPIqIpt/UReplz8u8zBdDWPfTd+S3s8ZAX4GPi+YEe28XR85dv/tW8YkP13+yIuDv74+pqSl79uyhW7duOuVRsmRJrly5gp/fqxeQ5wwMDKhduza1a9dm5MiR2NjYsHfvXlq0aIGRkREZGTlrCTx27Ngr3wsWVF3oT58+jVKpZPLkySie1WJXrVql0+/0OtnFa2BgQKdOnViwYAFGRka0adPmlYrW+fPnSUpKUi8/duwYFhYWuLu7Y2dnh7GxMffv38/xMKAhQ4YwYMAAjWXGxsYcvfFqRcbA0BAP34IEXzxB8XKqP+ZKpZKrF05Qo8G7PabwRclJTwl//IDytrrdNBoYGuLhU5BrF49TvGxWnNcuHqeajnEGBpVj2BTNSdeLfxtJflcv6jbr8t4rAQAxx86Rr0FVjWUOtSoSfewcAJlpacSeuYxDzQpZjyHV08O+RgXuzfoLXekbGJHfozB3rx6lQPHaqm0pldy7epSS1bU/YtXFpzh3rx6jTK3O6mV3g4/g6lNc5zi0SU2H1JcewhGflIm3kx6Pn934GxmAqwOcupH3FQEDAyNcvQpx88oxCpdW7UulUsnNy8eoWKed1nU8/Ypz8/IxKtfvqF5249JRPP2LAZCRkU5GRvorrewKhYLMTN2ecKRvYISLZ2FuBx8lsGRWmd8OPkbZmu21ruPuW5w7wUc1HgV6+8oR3HyLA1C0wif4FNTskf3r124UrdCU4pVz3vOXlKIk6aXHgkbFpFGsoDm3Q1S1P1MTBQE+pmzdr/1pZtnR08uqWKzZFsHOfzRvNmeN8WfeylBOnM/5UCFDAwMCfDw4eekq1cqqemaVSiUnLwXTqn6NV9J7uuRn6aSRGsvmrtjA0+QU+ndujZODHQb6+tjbWHHyYjAFnt34Jz5N4vLNO7Soq9sQUANDQ3z8CnDx/GnKVKiqjvPS+dPUa9wi2/U2rlnK+lWL+WHMZHz9X3185/NKQOijB4ycMB1Lq1crP2/D0NAQPz9/zp8/R4WKldRxnj93jsZNPsl2vTWrV7Fq5TLGjJuAfwHNJ75lZKSTnq7lHNJXkKlDo56hgQEFvVw5ceUmNUoVVsd44spNWtd6tWLu5ZyPVeP6ayybtXYHickpDGz/CfntrDE0MKCQt9srQ4vuh0Xg7PDffHQoyAvFctt/siJgYmLC4MGDGTRoEEZGRlSqVInw8HAuX75M165dc5TH4MGDKV++PL1796Zbt26Ym5tz5coVdu3axcyZM9m8eTO3b9+matWq2NrasnXrVpRKJQEBAYBqaM7x48e5e/cuFhYW2NnZqW/kX3b48GF+/vlnmjVrxq5du1i9ejVbtmwBwM/Pj7S0NGbMmEGTJk04fPgwc+bMyZ0d9QIvLy8SEhLYs2ePeqjP896Gbt26qSsmhw+/+kSJ1NRUunbtyrBhw7h79y4jR46kd+/eKBQKLC0t+e677+jfvz9KpZLKlSsTGxvL4cOHsbKyUs/heJGxsXE2vQXaW9JqN+nAwhnD8fIthJd/EfZsXkpqShIVazYFYMH0YdjYOdL8c1XvTnpaGqEPVK136enpxEQ+IeTOVYxNzHB09gBgzaIpFC1dFbt8zsRGhfP3ytkoFPqUqVz/7XbsC2o26cDimcPx9C2Mp18R9m35i5SUJCrUaAbAwulDsbF3pFn7vq/EmZGeRkyUZpwmpua4eGhObjQ2NsXc0uaV5Tmlb26GuZ+H+ruZtxtWxQJJjYolOSSUgHEDMHF14nwXVe/Mvd9X4Pl1ewInDCRk4VocapTHuVUDTn7ylTqPO1MXUGz+RGJOXyL25AW8+nTCwNyUkEXan0iTU2Vrd2HzwsE4exbB2asop/YuIjU1iaIVVTcIfy8YhKWNE9WbfwtA6ZodWTa5A8d3zccvqBpXTm4l9N4l6rcfo84zKTGGuKhQEmJUw+iiHqtaWM2tHLCwzqdzrMevKqlSREFUvJKYxEyqF1UQ/xSuhmRVBDrUUnA1JJOT11XLDA3A7oWGOhsLcLKFpBSIezZ31MQIrM1Vk5EB7K30gEwSkiDxhd6HN6ncoDOrfx+Cm3cR3H2COLRjMakpSZSqqroZXjnne6xtHanfWlVBr1S3A3N/7MTBrQsILF6N88e28vDOJVp8oerJMzG1wDuwDFuXT8LAyARbexduXz3JmUObaNxO98c0lq/bmQ1/fo+LVxFcvYtybPci0lKSKF5JVebr/xiMpa0jtVuqyrxc7Q4s/LkjR3bMp0DR6lw6sYVHdy/TpKOqzM0sbDGz0LxhUegbYGHtgEP+dxvWsHF3JG0aOfLocSphEal0aOZEVEw6R89mtcKP/9aLo2fi2LxPVTno1MKJUxfjCY9Kw9REQfVyNgQFmDN86l1ANedA2wTh8Mg0Hke83SM62zauw9jfFlDQx5NCft6s3Lqb5JRUGlVX3ciOnjmffHY2fN2uBcZGhvh6aPboWpir/j68uLx1w9osXLcVd2dHXBwd+H3FRhxsbahaRvcHAzRq1oZZv47H1z8Q3wIF2bpxFSnJSVSvrZrfN3PyWOzs89Gucw8ANq75i1V//UmfgSNxdHImJlrVI2RiYoqJqRnp6en8OmEYd25dZ9CIiSiVSnUaCwsrDHR87GWz5i35dcov+Pv7U6BAIBs3riM5JZnadeoBMHnSz9jb29O5i+oeZM3qlfy1ZDEDB32Pk6MT0VGqY8DE1BRTU1PMzMwpElSU+fPnYWRsjKOjI5cuXmTvnt106/5VtnG8Tvt6VRg5bxWFvN0o7OPGsp2HSEpJ45MqqvkTw39fiaOtFd+0aoCxkSF+bppzHC3NVI19Ly7v2KAa389aRskAb0oX9OXIxescPBfM799/qVOM4sPzn6wIAAwfPhwDAwNGjBjBo0ePcHZ2pkePHjlev2jRohw4cIChQ4dSpUoVMjMz8fX1pXXr1gDY2Niwbt06Ro0aRXJyMv7+/ixfvpzChVU17e+++45OnTpRqFAhkpKSuHPnDl5eXlq39e2333Lq1ClGjx6NlZUVU6ZMoV491cWjWLFiTJkyhYkTJzJkyBCqVq3KhAkT6Nixo9a8dFWxYkV69OhB69atiYyMZOTIkYwaNQpQ9bBUrFiRqKgoypUr98q6tWrVwt/fn6pVq5KSkkLbtm3V6wKMHTuWfPnyMWHCBG7fvo2NjQ0lS5bkhx9+yJXYy1SqR0JsNJtWzFa9qMs7gD7DZmH1bAJxVESoRqtKTPQTxn2X1Qq/a9Nidm1aTIHCpfh2zJ8AREc+5o9fh5AYH4OFlS1+BUvw/YTFWFrr/uz70pXqkxAXzeYVs1RxegXQe2hWnNERYRqVxdjoJ0wY2Fr9ffemRezetAj/QqXp/yzO3GZdqggV9ixRfy80SVVGIYvXcaHrEIyd82Hq7qz+edLdB5z85CsKTR6C1zcdSX4QxsWvhhGx65A6TejqbRjls6PAyD6qF4qdD+ZE426kvjSB+G0VLN2Qp/FR/PP39GcvFCtI62/+UA/1iYsK1XhxjJtvST7pOomDm6ZycOMUbB29aNnjN/K5ZrXE3Ti/l62Lh6i/b/xD1SJWqVFvqjT5RudYj1zJxMggk8blFJgYwf0nmSzdl0HGCw17thZ6mBkDz54O72KnR6c6Wb069Uqp/n/ulpJNx1QrBrjp0bRCVppPK6v+f+CCkgMXc95qWKx8AxLjo9i1dgbxsRG4eATyxcC5WD4bchMTqbkvPQuUoE3Pn9m5Zjo7Vk/FwcmTDv1mkN89qwLartcktq/6lZWzB/E0IRZbBxfqtepLuVqtX9l+ThUpqyrz/RtmkBAXTn73grTvP089NCg26pHGue7uV5IW3Sexb/1U9q77FTtHL9r0nomj27u9byMn1myPwMRYwTcdXTA30+fKjacMn3qXtPSsyp9zPiOsLLP+jNpYGvBtVzfsrA1ITFJy90Eyw6fe5dyVRG2beCd1KpYhJi6eeas2ERkTh7+XG7/+0Ec9xCcsIuqt5010aFqP5JQUfpr7FwlPn1I00I+pP/TF2Ej3Z8pXrFqLuNgYVv31BzHRUXj5+DFkzGRsbFXX4sjwxxrXzV1bN5CensaUCcM08vm0bRdate9KVGQ4p46rrk+D+3TRSDPix+kULqrb2Puq1aoTGxfLX0sWEx0djY+PD2PGjMf22QTi8PAnKF5oid66ZTPp6WlM+HGsRj5t231O+89Vf98HD/6BRQvnM+mXn0iIj8fR0ZEOHTvToKFuLxSrV64Y0fGJzF6/k8jYeAI8XJj57RfqoUFhkTEo3rLMa5Yqwg+dmrNgyz5+WboJz/z5+KX351rfOSD+P+llPp8tInTi5eVFv379NJ4M9F+TmZmJv78/X3/99StDdjp37kxMTAwbNmx473Hsv/T6sbX/BdWLmLLn4ls0xeaBWkEmbDEMyOsw3qhR2jUW7MvrKN6sSw0Ys1S3xzj+m0a0N2D9if/2xOXmZfVZdui//yelXWU9GnW7lNdhvNGWP4oQff7dH9P8vtkWq8a5G9qfXPNfUdw/Hzdu6T4J/N/i7+tJ4tENeR3GG5lXaJZn246d1DfPtm393bQ82/b78p/tERC5Izw8nBUrVhAWFqbx7gAhhBBCCPFxk4rA/zlHR0ccHBz4/fff1V2cQgghhBAfIpksnLukIvCO7t69m9chvNabRn4tXLjw3wlECCGEEEL8p8hbGYQQQgghhPgISY+AEEIIIYT4MMibhXOV7E0hhBBCCCE+QtIjIIQQQgghPghv+34M8XrSIyCEEEIIIcRHSCoCQgghhBBCfIRkaJAQQgghhPgwyGThXCV7UwghhBBCiI+Q9AgIIYQQQogPgrxZOHdJj4AQQgghhBAfIekREEIIIYQQHwY9acPOTbI3hRBCCCGE+AhJRUAIIYQQQoiPkAwNEkIIIYQQHwaZLJyrpEdACCGEEEKIj5D0CAghhBBCiA+CnkwWzlWyN4UQQgghhPgISUVACCGEEEKIj5BeZmZmZl4HIYQQQgghxJskzhuWZ9s27z4uz7b9vsgcAfGvCb71MK9DeKOCvq7M253XUbxe99qwYF9eR/FmXWrAFsOAvA7jjRqlXWPzmfS8DuONGpc0YNOpjLwO47U+Ka3/wezLscv/+3EOb2vAvotJeR3GG9UIMuVIcHxeh/FaFQta8vfp/36ZNyllwM7zqXkdxhvVLWaU1yGIXCIVASGEEEII8UHQU8io9twke1MIIYQQQoiPkPQICCGEEEKID4OevFAsN0mPgBBCCCGEEB8hqQgIIYQQQgjxEZKhQUIIIYQQ4sMgk4VzlexNIYQQQgghPkLSIyCEEEIIIT4MMlk4V0mPgBBCCCGEEB8hqQgIIYQQQgjxEZKhQUIIIYQQ4oMgbxbOXbI3hRBCCCGE+AhJj4AQQgghhPgw6Ekbdm6SvSmEEEIIIcRHSCoCQgghhBBCvAe//fYbXl5emJiYUK5cOU6cOPHa9DExMfTq1QtnZ2eMjY0pUKAAW7dufW/xydAgIYQQQgjxYVB8OO8RWLlyJQMGDGDOnDmUK1eOqVOnUq9ePa5du4ajo+Mr6VNTU6lTpw6Ojo6sWbMGV1dX7t27h42NzXuLUXoE/gV3795FT0+Pc+fOvdft7N+/Hz09PWJiYt7rdoQQQgghxOtNmTKF7t2706VLFwoVKsScOXMwMzNj/vz5WtPPnz+fqKgoNmzYQKVKlfDy8qJatWoUK1bsvcUoPQK5rHPnzsTExLBhwwb1Mnd3d0JDQ3FwcMi7wP7jtv69gfVrVxITHYWXty/de35DgYCCWtPu3L6ZfXt2cf/eHQB8/Qrweaeur6QPuX+PxQt+5/LFC2RkZODu4cngoaPI5+ikc5yZmZkc3jKdi4dXk5IUh4tPSeq0GYWto9dr1zt7YCknd/9JYlw4+VwDqfXZcJy9iqp/fv7QSoJPbeZJyGVSkxPp/ctJTMysdI7z9P6lHN+p2p6jWyB1Wg/Hxbtotumvnt7GwU3TiI18iJ2jF9Wbf4dvUDX1z6+d3cnZgysIu3+Z5MQYugzdgJO79vLJCbvKpfH5tivWJYtg4uLIqZZf83jTntevU7UshSZ9j0Uhf5JDQrk5YTYPFq/XSOPZsx0+A7pinD8fcReucrnfWGJPXtQ5ToBDO5ex/+8FxMdG4OIRQPPOP+Dhp31fhoXcZPuaGTy4fYXoiEc07TCYqg07vlOeOXV45zIObJlPfGwEzh4BNOs0FA/f7PM8f3w7O1bPIDriIQ5OnjRsO4CCxbPKPD42gi3Lp3Dj4mGSnsbjHViaZp1+IF9+r3eK80PZnwDVghSU8NXDxBBCIjLZdlJJVEL26T3yQYWCCpxt9bA002PVwQyuPcx853yzs3/bCnZuWkRcTCRungVo3XUw3v5BWtM+CrnJ3ytmc+/2FaLCQ2nV+TtqNf4827y3r5/PhqXTqdmoHZ91GfT2wb1gz9ZVbFu/hNiYSDy8/GnffSA+BYpoTfvw/i3WL5vD3VtXiQwPpe0XA6j7STuNNMqMDDas+J2jB7YRGxOJja0DlWs2oclnXdF7h7fOHt65jP2bF6jPoeadXn8cnT+2g+3Pz6H8njRqM4CCJaqqf/78HLp+4QhJT+PxCSxFs05DyefsqXOMB7cvZ8/fC4mLicDVM4BPvxiCl5/2Mg8NucmWlb8RcucKUeGPaNFpEDUaddBIs3P9H5w/sZvHD+9gaGSCd4FiNP28P04u3jrH+L7p5eFk4ZSUFFJSUjSWGRsbY2xs/Era1NRUTp8+zZAhQ9TLFAoFtWvX5ujRo1rz37RpExUqVKBXr15s3LiRfPny0a5dOwYPHoy+vn7u/jLPY3ovuX6gUlNT30u++vr65M+fHwMDqXdpc+jAPubPm02bdh2ZMmMuXj6+jB4+mJiYaK3pL104T5VqNRk7YQoTJ8/EwSEfo4YNIjIiXJ0mNPQhPwzsi6ubB+MmTmHqrHl81vZzDI2M3inWE7vmcXb/Euq0GUX7gaswNDJlzcyupKelZLvO1dNb2b9uAhUa9qLD9+txdAtkzcyuJMZHqtOkpybhXagK5er1eKf4AIJPbWXvmglUbtyLLj+otrdyRlcS4yK1pn9w6wwb//yWYpU+pcvQDfgXr8XaOb0If3hdnSYt5SlufiWp0fy7d44PQN/cjLgL17jUZ3SO0pt6uVFm01wi9x/nUOmm3JmxiKC543CoU1mdxrlVAwr+MoQb437jUNnmxF+4Srktf2KUz07nOM8e3camJT9Tt+XX9P9xNS6eAfz+01fEx2rfl6mpSdg7utOobX8sbbRX/N82z5w4d3Qbfy+dSJ0WX9Nv3BpcPAL546cvScgmz7vXz7Js5kDKVm9Bv/FrKVy6FoumfENYyA1AVeFdOOUbop6E0HnATPqNX4utgzO//9iV1OSnOsf5oexPgIoF9ShbQI+tJ5XM35VBWjq0q6GP/mv+ahoa6PE4GradVuZqvtqcOryDNYsm07jVV/zw83LcvAowY9zXxMVGaU2fmpKMg5Mrzdv3xSqbffnc3ZuX+GfXGlw9C7xdUFocP7STFfN/pWmb7oya8hfuXgWYPPob4mK0x5mSkky+/G606tgba1t7rWm2rlvEvu1r+PzLQfw4YzWtOn3DtvWL2b1lpc5xnju6jU1//aw6h8avxsUjgHmvOY7uXj/L0mfnUP8f11CkVE0WTvmG0BfPocl9iHzygM7fzqD/j2uwdXBh7oSupOh4Dp0+sp31i3+hwac9GDRxFa6eBZg1/jXnT0oyDk5ufNKuX7ZlfvPKKarUa8O345fSa9jvZGSk89u4r3SO8f/dhAkTsLa21vhMmDBBa9qIiAgyMjJwctJsfHRyciIsLEzrOrdv32bNmjVkZGSwdetWhg8fzuTJkxk3blyu/y7PfdQVgerVq9O7d2/69euHg4MD9erVA+DSpUs0aNAACwsLnJyc6NChAxEREer11qxZQ1BQEKamptjb21O7dm0SExMZNWoUixYtYuPGjejp6aGnp8f+/ftfGRr0fAjPnj17KF26NGZmZlSsWJFr165pxDdu3DgcHR2xtLSkW7dufP/99xQvXvyNv9fhw4cpWrQoJiYmlC9fnkuXLql/tnDhQmxsbNiwYQP+/v6YmJhQr149QkJCss0vNTWV3r174+zsjImJCZ6entke+LrYuH41des3pFbdBrh7eNGzd3+MjY3Zs3Ob1vQDBg2lYeOm+Pj64ebuQa++35GpzOTC+bPqNEsXzadk6bJ07voVPr7+ODu7UrZ8JWxsbHWOMzMzkzP7FlO+fk/8itUmn2sgDTv9TELsE26e353teqf2LCCo4mcEVWiJg7MfddqMxtDIhEtH16rTlKrZmXJ1v8TZ6927/07sXkCxSp9RtGJLHFz8qN9uNIaGJlw4slZr+lN7F+NTuArl6nbDwdmXqp/0I79HIU7v/0udpkj5ZlRu1BvPwArvHB9A+I6DXB85lccbs99vL/L8sg1Jdx4QPGgiCVdvc2/WUsLW7sC7b2d1Gu9+XQj5cxUPFq0jIfgWF78eScbTZNw7t9Q5zoNbFlG+5qeUrd6c/G5+tOw6EkMjE07sX6c1vYdvEE3af0eJig0xMNBe6XzbPHMU57aFlKvRijLVWuDk5keLL0ZiaGzCiQPa8zy0fQkBRStTvXFXnFx9qd+qD65ehTi8cykAEWH3uH/zPC2+GIG7bxCOLt606DKStLQUzh7VfdLah7I/AcoGKPjnspLrDzN5EgMbjymxNIVAt+xbnG+FZrL/opJrD17tBXiXfLXZ/fcSKtVuQcWazXBx96Xdl8MwNDbhyN4NWtN7+RWhZccBlKlcHwNDw2zzTU56yvxpP/B5jxGYmVu+VUza7Ny4lKp1m1Gl1ie4uvvQsecQjIxN+GfPJq3pffwL07pzX8pVqZdtmd+8doESZatRrHRlHJxcKFOxNoWLl+P2jcs6x3lg6yLK1XjpODI24WQ259A/2/8ioFhlajT5QnUOfdYHV+9CHN65DFCdQ/dunqflFyPweH4OfTGCtNQUzul4Du3bvJgKtVpSvkZznN18ad19BEZGphzdt15rek+/IjTr8C2lKjXAwFD7vvx66BzKV2+Gs7sfbl4BfN5rHNERoYTcvqJTjP8KhV6efYYMGUJsbKzG58UW/3elVCpxdHTk999/p1SpUrRu3ZqhQ4cyZ86cXNvGyz7qigDAokWLMDIy4vDhw8yZM4eYmBhq1qxJiRIlOHXqFNu3b+fx48d89tlnAISGhtK2bVu++OILgoOD2b9/Py1atCAzM5PvvvuOzz77jPr16xMaGkpoaCgVK1bMdttDhw5l8uTJnDp1CgMDA7744gv1z5YuXcr48eOZOHEip0+fxsPDg9mzZ+fodxo4cCCTJ0/m5MmT5MuXjyZNmpCWlqb++dOnTxk/fjyLFy/m8OHDxMTE0KZNm2zzmz59Ops2bWLVqlVcu3aNpUuX4uXllaNY3iQtLY1bN69TtHgp9TKFQkGx4qW4djVnF6LUlBQyMtKxsFD90VIqlZw6eQwXV3dGDRtEp7YtGNjva44dOfROscZGPiAxLhzPgKwyNTa1xNmrGI/unNW6TkZ6Ko9DLuMZmLWOnkKBR2BFHt3Wvs67yEhPJez+ZbwKam7Pq2BFHmazvUe3z+H10g2+d6HKPLx9Ltfj05VN+eJE7NXsSg3fdQjb8sUB0DM0xLpkYSL2HMlKkJlJxN4j2JQvodM209NTeXDnCv5FsvaNQqGgQJHy3Ltx/j+V58M7V/AvUl4jT/8iFbh345zWde7dPKcRA0CBopW4d1MVQ3qaqnfUwDCru1uhUGBgYMSda2d0jvND2J8ANuZgaarHnbCsG/qUNHgYCa4Oug89ya1809PSuH87mIJFy6mXKRQKCgaV4/a1CzrHB7Dijx8pUrIKBYuWf3PiN0hPS+PurasUfinOQsXKcvMd4vQLKMqVCycJe3gPgPt3rnMj+DxFS2b/9/a1cT47hwq8dBz5v+Y4unfjnMY5BxBQtJL6nMs6h7JuwN/lHEpPTyPk9hUCgjTP84Cg8ty9rvux/rLkp6oxamYW1rmW5/8TY2NjrKysND7ahgUBODg4oK+vz+PHjzWWP378mPz582tdx9nZmQIFCmgMAypYsCBhYWHvbdTKRz9Wxd/fn59//ln9fdy4cZQoUYIff/xRvWz+/Pm4u7tz/fp1EhISSE9Pp0WLFnh6qsb5BQVljc8zNTUlJSUl20J+0fjx46lWTTUm9/vvv6dRo0YkJydjYmLCjBkz6Nq1K126dAFgxIgR7Ny5k4SENw8kHTlyJHXq1AFUFR03NzfWr1+vrsykpaUxc+ZMypUrp05TsGBBTpw4QdmyZV/J7/79+/j7+1O5cmX09PTUv3duiI+LRalUYmOr2VJvbWPLg5D7Ocpj0YLfsbWzp1gJVWUiNiaG5KQk1q1eTvuOXejY5UvOnj7BxPEjGfvTFIoE6dbqnhinGnpkZqXZXW1maU9iXIS2VUhKiCZTmYG5peY65pb2RIXd1imO13n6fHtWr24vMpvtJcRFYG7l8Er67H6nvGDs5EDKY814Uh5HYGhticLEGENbaxQGBqQ8iXwpTSTmAT46bTMxLgalMgNLa819aWFtz5NHd/47ecar8rSw1ixDCyt7njzSXubxMRFYvBSDpbUD8TGqfezo4o2NvTPbVv5Ky66jMDI25Z9ti4mNCiM+Jlxblm+O8wPZnwAWps/yT35pe8mZWJjonG2u5ZsQH41SmYHVy2VoY0/Yw7s6x3fy0Hbu37nKkJ+W6pzHi+KfHZtWNprD86yt7Qh7cFfnfBu27ExSUiI/9P4UhUKBUqmkRfuvqVCtgU75ZZ1DL58T2R9H8TERWo+7+BjVNcjRxRsbB2e2rpjKp11HYmRiysGtqnMoLvrtz6HEuGdlbvNqmT9+h2P9RUqlkrULJ+ITUAIXD/9cyfNjZmRkRKlSpdizZw/NmjUDVPt4z5499O7dW+s6lSpVYtmyZSiVShQKVVv99evXcXZ2xugdhzZn56OvCJQqVUrj+/nz59m3bx8WFhavpL116xZ169alVq1aBAUFUa9ePerWrcunn36K7Us3sjlRtGjWJCRnZ2cAnjx5goeHB9euXePrr7/WSF+2bFn27t37xnwrVMhq1bCzsyMgIIDg4GD1MgMDA8qUKaP+HhgYiI2NDcHBwVorAp07d6ZOnToEBARQv359GjduTN26dbPdfnaTad6HtauWcejAPsZNnKI+STIzVeNzy5avyCfNWwHg4+vH1eDL7Ni6KccVgSsnNrFr+Uj19xZfz83l6IX4b9I3MKRT/+ms+n0YI7+sgEKhj1+RCgQWq0JmZvbDXj5URTz1aFQmq4N8+YGMPIwmb0RFhLFqwc/0HT4HQ6P3c73OLScP7+Loge18NWAcLu6+hNy5xrL5U7Cxy0flmo3zOjxAdQ517jeNVfOGM+LLiigU+vgXKa86h/hvnkOr/xxPaMhN+o1ZlNehvN4H9GbhAQMG0KlTJ0qXLk3ZsmWZOnUqiYmJ6kbejh074urqqh5u3bNnT2bOnEnfvn355ptvuHHjBj/++CN9+vR5bzF+9BUBc3Nzje8JCQk0adKEiRMnvpLW2dkZfX19du3axZEjR9i5cyczZsxg6NChHD9+HG/vt5tlb/jCOM3nTzpQKrOfZJZXSpYsyZ07d9i2bRu7d+/ms88+o3bt2qxZs0Zr+gkTJjB6tOYk0JEjR9K6Q/dX0lpaWaNQKIiJ1pwYHBsTja3d6yd5bli7krWrlzNm/CS8vH018tTX18fdQ7Pnws3dk+DLOX+CjF/Rmhpj9jPSVd1yT+MisbDOev7v0/hIHN0CteZhamGLnkJfY2IwQGJ85Cut8LnB7Pn24nK+PQsrh1da/99XfLpKeRyBsZNmPMZODqTFxqNMTiE1IhplejrGjvYvpbEnJUy3ng1zKxsUCv1XJuIlxEZmO3E1T/K0VOWZEKv5eybERWJprT1PSxuHVyYSx8dGaMTg5l2YARPWk/Q0noz0NCys7Jg+ojVu3tqf9vLGOP/D+/P6w0weRmbd/Bs8u88wN4GEF1rvzU30CIvW/SYuISl38rWwtEWh0Cfu5TKMiXzjRODs3L99hfjYKH4c1Fa9TKnM4GbwGfZvW8nM5SdQvOVTSyyfHZsvTwyOjY3CKpuJwDmxcuF0GrXsRLkqqnl97l5+RISHsmXtAp0qAlnn0MvnRPb709LGIZvjLuv3cvMpzIAJ6zTOoWnD2+DuU/jtY7R6VuYx2spc93353Ko/x3PpzAH6jl6Irf2bRzSInGndujXh4eGMGDGCsLAwihcvzvbt29UTiO/fv69u+QfVUyZ37NhB//79KVq0KK6urvTt25fBgwe/txg/nGrVv6RkyZJcvnwZLy8v/Pz8ND7PKw16enpUqlSJ0aNHc/bsWYyMjFi/XjVZx8jIiIyMd29NCggI4OTJkxrLXv6enWPHjqn/Hx0dzfXr1ylYMOtRj+np6Zw6dUr9/dq1a8TExGikeZmVlRWtW7dm3rx5rFy5krVr1xIVpf2pD28zmcbQ0BBfvwJcOJ81ZlKpVHLh3BkCAgtlG8+61StYtfwvRo6diF+BgFfy9CsQwMMHmhOgHz0MeatHhxqZWGDr6Kn+2Dv7YW6Vj3vXssaqpyQlEHr3PC7e2seh6xsY4eRemPsvrJOpVHL/2lFcfHQbu/46+gZG5PcozN2rmtu7d/Uortlsz8WnOHevHtNYdjf4CK4+xXM9Pl3FHDuHfU3N8bgOtSoSfewcAJlpacSeuYxDzRfGvevpYV+jAjHHdJuLYWBghJt3IW5cyto3SqWSG5eP4+mv2/Cy95Wnq3chbl7WzPPmpWN4+hfXuo6nX3FuXNYs8xuXjuLp92oMpmaWWFjZER52lwe3L1O4VE2d4/yv7s/UdIhOyPqEx0F8Uibe+bPG7RsZgKs9PIzQvSIQk5g7+RoYGuLhU5CrF7PeUKpUKrl68QQ+Abo9NjUwqBzDp6xh6KSV6o+nbyHKVmnI0Ekr37oS8DxOL99ArlzQjDP4wkn8dIwTIDU1+ZVHSCoU+jr3Vj0/h268fA695jjy9C+ucdwBXL94VOs5pz6HQu/pfA4ZGBji7lOI65eOa8R4/dIxvAro/pCJzMxMVv05ngsn9vLNiD9xcHTTOa9/jZ5e3n100Lt3b+7du0dKSgrHjx9XD8sG1cNjFi5cqJG+QoUKHDt2jOTkZG7dusUPP/zw3h4dCtIj8IpevXoxb9482rZty6BBg7Czs+PmzZusWLGCP/74g1OnTrFnzx7q1q2Lo6Mjx48fJzw8XH0T7eXlxY4dO7h27Rr29vZYW+s24eabb76he/fulC5dmooVK7Jy5UouXLiAj8+bxzuPGTMGe3t7nJycGDp0KA4ODurxaaC6Uf7mm2+YPn06BgYG9O7dm/Lly2sdFgSqF2I4OztTokQJFAoFq1evJn/+/Nm+6S67Z+pmp2nzVkyb8hN+/gH4Fwjk741rSU5Jplad+gBMnTQBe3sHOnRR9SisW72cZUsWMmDQUBwd8xP9rEJiYmqKqalqEG7zlq2Z9NNYCgcVJahoCc6cPsHJ40cZN/HXHMf1Mj09PUrW6Mix7bOxdfTE2t6Nw5unYWHtiF+x2up0q6Z1wq9YHUpWVz2ju3StLmxbPBgnjyI4exXl9N5FpKUkUaR8C/U6ibHhJMZFEBOumhcR8eg6RsbmWNo5Y2pu81Zxlq3dhc0LB+Psqdreqb2LSE1NomhF1fb+XjAISxsnqjf/VhVfzY4sm9yB47vm4xdUjSsntxJ67xL1249R55mUGENcVCgJMU8AiHqsGpNqbuWAhXW+t9yTqseHmvt5qL+bebthVSyQ1KhYkkNCCRg3ABNXJ853UbWC3Pt9BZ5ftydwwkBCFq7FoUZ5nFs14OQnX6nz+B979x0eRdUFcPi3m94T0hNCeiP03nsvKiAoRQgiVlREVFBpgqIISlNA6UjvIL0jvXcIkFASCKT3nt3vj8CGhXSiIR/nfZ59YGfPzJzM7NzZO/femVtTF1B9/k/Enb5E/MkLuH0yAF0TI0IXlfzuMc06D2DFrK9x8QigkldVDm5bQkZ6KvWadwNg2e8jsbCyo3Pvz4CcQYcPw4IByM7KJD42gnu3r2JgaIyNg2uRllmiPDsGsnLOSCq6V8HFsyr/bF9MRnoqdR8tc/msEVhY2dHpzWEANOnwFrMmDODAlgX412zOuaNbCQu5xOuDclvyzh/fjqlZBSxtHAm/e51NSyYSUKc1vtUalzzPcrI9AU4EqWgSoCQmUUVckpoW1ZQkpsK1J+4I1K+lkmthak7dyJmmpwsVnuhVamkK9paQmgEJKUVfblG06foWC2eOwtWzMm5eVdi7ZSkZ6ak0avkqAAumf4ultR3d+uZ0KcjKzCRcsy2ziIuJIPTWNQwMjbFzrIShkQnOlby01qFvYISJmcUz04uj3at9mTttLG5elfHwDmDn5mWkp6XSpHVXAP6cOhpLazt6vjVEk+f90JBHeWYSGxPJ3ZAgDIyMsXd0AaBGnab8vWY+1rYOOLt4cOdWEDs2LaVp61dKnGfzTgNYMftrKnoEUMmzKv9sW0JG2hPH0O8jsahgR6c3c76bTTv04/fxgezfspDKNZpx9ui2nGPonbGaZZ4/tgMTcyusrB0JD73BxsUTqVKnVYmPoZZd+vPXb99QySMAV6+q7N+6hPT0VBq0eA2AxTO/xrKCHa/0GQrkDDB+8GifZ2VlEh8TQdjtnH1u65BT/q6a9z2nD21l8JfTMDQyIeHROCFDY1P09Z9jQIwoN6Qi8BQnJycOHz7MV199Rbt27UhPT8fV1ZUOHTqgVCoxNzfn4MGDTJ06lYSEBFxdXZkyZQodO+YMUho8eDD79++nTp06JCUlsW/fvhLdYadv376EhIQwfPhw0tLS6NWrF4GBgZw4caLQeX/88Uc+/fRTbty4QY0aNdi8ebPWIBNjY2O++uor+vTpw71792jatCnz5s3Ld3lmZmZMmjSJGzduoKOjQ926ddm6datWc9bzaNK8JfEJcSxfsoDY2FjcPTwZ891PWFrldA2KjIxA8cS6tm3ZRFZWJpN+GKu1nDf69Kd3v0AAGjRqyvtDPmPtqmXMnT0Tp4oufPXNOCoH5P3glaKq13YwmRmp7Fw2mvTUBJw9a9Pjo7lad1eJiwolNTm3q5Nf7U6kJMZw+O/ppCRGYuvsz+sfzdXqenPu0AqObp2peb/i174AdOg3kSoNcysMReFfJ2d9/2ye/uiBYv688XHu+hJiwrWuplX0rMUrgyZzcNNUDm78BSs7N3q8/xu2zrn3EL9xfi9bF+e26mycm3MybNx5CE27flys/AAsaleh4Z4lmveVJ38NQOjidVwYNBIDR1uMXBw1n6feDuPkK+9RecpI3D7uT1rYAy6+9y1Ru3LvBBW+ehv6thXwGfNJzgPFzl/lRJd3yHhqAHFx1GzYkeSEGHasmfnoAT5+DB4xR9PtJC4qXOsBRgmxkfwy8nXN+/1/L2D/3wvw9K/Lh6MXFmmZJVGjYUeSE2PYsWZGzkO1XP1456s5mq5BcdHa+9zNpyZ9PprEjtXT2bZqKjYOrgwYNgMHl9wBgomxkWz+axJJ8VGYWdpSu+mrtOn2fM+5KC/bE+DIVTV6umo611ViqA93I9Us259N9hO9N61MFRgbAI/6fDtVUNC/de6Vu3a1cv5/PkTFpuOqIi+3KOo0bk9iQiybV8wiIS6Kim6+fPzN75puIjFR4SiUudsyLjaC77/IvTvcrk2L2bVpMd6Va/P5d/mX/8+rfpN2JMbHsmH5bOJjo6nk7sOwMTOweJRndOQDre9mXEwkY4b11bzfvmEJ2zcswTegFiO+/wOAvu9+wfqls1ky50cS4mOxtLKhRfvuvNrr2e6nRVWjYUeSHn2PEuMeHUMjco+h2Gjt7enmU5O+H01i++rpbFuZcwwFDpuB4xPHUEJcJJseH0NWttRp8gptupf8GKrdqANJCTFsWfUbiXFROLv58eHXszXdl2KfOn7iYyL46cuemvd7Ni9kz+aFeFWuw6djFwBwaGfOsxemj32bJ/X9cLymgiH+vynU/48jv/5PtW3bFgcHB5YsWVJ4cD4WLlzI0KFDiYuLK73Eiuhq8L3/fJ3F5e/pzJ9Fu7V9mRncBhbsK+ssCjewJWzR8y08sIx1zgzi7zNZZZ1GobrU0mXTqRd7EOsrdXTKzbYcv/zFz3NUb132XUwt6zQK1bKqEUeuJpZ1GgVq5G/G5tMv/j7vWluXnef/ndtElqZ21f+dO9gURdrakrfsPy/DHp+V2br/LdIi8IJKSUlh9uzZtG/fHh0dHZYvX87u3bvZtWtXWacmhBBCCCH+D0hF4AWlUCjYunUr33//PWlpafj6+rJ27VratGlT+MxCCCGEEP+PytHtQ8sDqQi8oIyMjNi9u/T7qAQGBhIYGFjqyxVCCCGEEOWLVKuEEEIIIYR4CUmLgBBCCCGEKB+UJbufv8ibtAgIIYQQQgjxEpIWASGEEEIIUT7IYOFSJVtTCCGEEEKIl5C0CAghhBBCiPJBIWMESpO0CAghhBBCCPESkoqAEEIIIYQQLyHpGiSEEEIIIcoHpVzDLk2yNYUQQgghhHgJSYuAEEIIIYQoH2SwcKmSFgEhhBBCCCFeQlIREEIIIYQQ4iUkXYOEEEIIIUT5IE8WLlWyNYUQQgghhHgJSYuAEEIIIYQoH+T2oaVKtqYQQgghhBAvIakICCGEEEII8RJSqNVqdVknIYQQQgghRGHSdswrs3Ubth9UZuv+t8gYAfGfmbb5xa9zftpVwQc/x5V1GgWa9YUl3y3NKus0CjW6ry5/n3nx8+xSS5cter5lnUahOmcG8fa4iLJOo0Dzx9hx4HJKWadRqOYBxly8+bCs0yhUVS97Zm598cvNIZ0UrD6mKus0CtSzgZLxy1/88mhUb10+mhxX1mkU6rfhlmWdgiglUhEQQgghhBDlg9w+tFTJ1hRCCCGEEOIlJC0CQgghhBCifFAoyjqD/yvSIiCEEEIIIcRLSCoCQgghhBBCvISka5AQQgghhCgf5MnCpUq2phBCCCGEEC8haREQQgghhBDlgloGC5cqaREQQgghhBDiJSQVASGEEEIIIV5C0jVICCGEEEKUD/Jk4VIlW1MIIYQQQoiXkLQICCGEEEKI8kFaBEqVbE0hhBBCCCFeQlIREEIIIYQQ4iUkXYOEEEIIIUS5IM8RKF3SIiCEEEIIIcRLSCoCxXT79m0UCgXnzp17ruW4ubkxderUUsnpv7Zw4UIsLS3LOg0hhBBCvGwUyrJ7/R+SrkH/BxYuXMjQoUOJi4sr61Sei1qt5uSOGVw5vpr01AQc3WvRrPsYLG3dCpzv4uGlnNs/j5TEKKwd/Wja7VvsK1XTfL7h97e4H3JSa57KDd6gxevjSpRnl8aGNKmmj5GBgpD7WSzbmUpknCrf+Pb1DajhrYeDtQ6ZmWqC72ez4UAqD2Nz52lSTZ+6/vq42OtgZKBg2PR4UtPVJcrvsRbVlNT0UmCoB6GRaraeVBGTmH98JTto5K/EsYICM2MFKw9kExSmnYOfi4La3gocKygwNlAwZ2sWD2NLnuOhncvYv3kBifFROFXypVvg11TyqpZn7IPQm2xfM4OwkCvERt3n1be+olmn/s+1zMJUaFIHj88HYVGrCoZOdpzq8SEPN+0peJ5m9ag8eQSmlb1JCw3n5sRZhC1erxXj+kEfPIYNwsDBloQL17g8dDzxJy+WKMcnvdbChGa1DDE2VHIzNJPFWxKJiMnON75TE2Nq+xngaKNDRhbcDM1kze4kHkTnPc9nfSyo6m3AjBVxnA3KKFGO+7atZOeGRcTHRVPRzYfe73yFu3eVPGPv3w1m44rfuRt8lejIcHoNHE6brn21YjatmM3fq+ZoTbN3dmP8DO1tXlzb/l7HprUriIuNwdXdk0Hvf4q3b+U8Y3dt38yBvTsIvR0CgIeXL30GDM43fs7MyezatonAwUPo8lqv58pTrVZzfPsMLh9dTXpaAo5utWjZs/By88KhpZzZm1Nu2jj50az7tzi45h4ne1eNJvT6UZITItDTN8bRvSaNugyngr1HsXM8tnsph7bNJyk+CgcXP7r0+4aKnvkfk5dObGf3uunERd3D2t6Vdr0+x7d6c62YiPvB7Fw5hVtBJ1FlZ2Pn7Envj6dhae1U7Pye1Lyqkpqej8rNKDXbTqqISco/vpItNPRX4miVU26uOphN0L1ny+7iLrcwnRsb0rhq7nloxa6Cz0Pt6hlQw0cP+wo6ZGapCbmXzYaDqUQ8cR7S1YHuLYyo7aeHno6CK7czWbk7lcSU5zsXiRfb/2f1RpRIdnY2KlX+Bcm/7ey+uVw4tITmPcbS45NV6Oob8fef75CVmZ7vPDfObeXwph+p0/Yjeg5dh42TL3//+Q4pidFacZXr9yRw9D+aV6MuX5Qox3b1DGhZy4Blu1KYtDSR9Az4pKcJujr5z+PtosuBsxlM+iuRaauT0FHCxz1N0dfLjdHXU3D5Vibbj6WVKK+nNaqsoJ6vgi0nVMzbkU1mFvRtqYNOAUe8vq6Ch3Gw9WT+3wE9XQiNULPn7PN/T84e3camJZNo1+NDPvthNU6uvvzx43skxkfnGZ+RkYq1nQude3+GmaVNqSyzMDomxiRcCOLSJ0WrNBq5VaTupjlE7z/OoTqvcmvGIqrOmYBN2yaaGMeeHfH/eSQ3JvzGoXrdSLxwjfpb5qFvW6FEOT7WsbExbeobsXhLIhPmxpCeoebzfpYFfjd9XfXYezKVCfNimbIkDh0lDOtnqfXdfKxtAyOe9+fAyUM7WL1gCl16vce3k5fh4ubDtO8+JCEuJs/4jPQ0bO0r0u2tTzDPZ58DOLl48vO8XZrXl9/Pf648Dx/cw6I/f6Nnn0AmTZ+Lm7sXE0YNJz4u71rv5YtnadKsNWMnTuOHKbOwsbVj/KjhREdFPhN7/MhBbly7QgXr/P+e4jizdy7nDy6hZc+x9Bq6Cj0DIzbOLrjcvH52K/9s+JF67T/izc9zys1Nc7TLTbuKAbTp/QP9Rmzh1ffmglrNxtmDUKnyr1jm5eLxrWxb/hMtX/2ID8etxcHFl4WTB5OUkPcxeffGWVbNGk7tZj348Lt1+NdqzbJpH/Mw7LomJvrhXf6c0BcbJ3cGjVzEkAkbaPHKB+jqGRQrt6c18ldQz0fB1pMq5u/KKTf7FFJu6ukqeBgL207nXyaWZLkFaVvPgBY1DVixK4WflyaSkQlDXi/8PHTwbAaTlyYyY3USOjrPnodeb2lEVU895m1K4deVSViYKhn8qknJkvw3KRRl9/o/JBWBfKhUKiZNmoSXlxcGBgZUqlSJ77//XvN5SEgILVu2xNjYmOrVq3P06FGt+deuXUtAQAAGBga4ubkxZcqUAtcXFxfHO++8g62tLebm5rRq1Yrz589rPj9//jwtW7bEzMwMc3NzateuzalTp9i/fz8DBw4kPj4ehUKBQqFg7NixAKSnpzN8+HCcnZ0xMTGhfv367N+/X7PMx118Nm3aROXKlTEwMODu3bvExsbSv39/rKysMDY2pmPHjty4ceP5N2oB1Go1F/5ZTO027+NepTU2Tr60fvMnkhMiuHVpd77znT+wkMr1e+JfrwcVHLxo3mMcunqGXDu5VitOV98IY3NbzUvf0LREebaqbcC2Y2lcuJnFvUgVC7cmY2GqpIZ3Hr+cHpm5JpljlzMIj1ZxL1LF4m0pWFsoqWSfW2rvPZ3OzhPp3Aov3kk2P/X9lPxzScX1MDURcbDhqAoz45wr+vm5eV/NvvOqZ1oBnnTxlpqDl9SEPHj+K0QHtyyiQavXqdeiGw4VvegxaAx6+oac2L8uz/hKnlXp2nc4NRt1QldXv1SWWZjIHQe5PmYqDzfm/x18kuu7b5J6K4yrX/5E0rUQ7vy+lAdrd+D+aaAmxn3oQELnrSJs0TqSrgZz8cMxZKek4RLYo0Q5Pta2vhGbDyZzLiiDsIhs5m5IwNJMSS2//H8c/bo0nsPn07gfmU3owyzmb0zAxlIHN0ft77OLvS7tGxozf2MBTUpFsGvzXzRp253GrV/FycWTvu99g76BIYf3bsgz3s07gNcHfEa9Jh3Q08v/GFPq6GBhZaN5mZlbPVeem9evok2HLrRq2wmXSm68O+RzDAwN2btzS57xQ78YTYcu3XD39MbZxZX3P/kStUrFxfOnteKioyKZN3san34xCh2d52+QV6vVnDuwmLrt3sejak652bZPTrkZcjH/7+y5/QsJaNiTyvVzys2WPcehq2/IleO55WaVRm/g7FkX8woVsXMJoEGnoSTFhZMYc69YOR7evog6zXtSu1l37Jy9eCVwLHr6hpw+mPcxeWTnYryrNqFpp0HYOXnSpsenOLr5c2z3Mk3M7rVT8anejA5vfIGTa2Ws7SvhX6sVpubWxcrtafV8lfxzWcX1eznl5sZjKsyMwK9i/uVmcLia/RcLLjdLstyCtKxlwPZjaVwIzuJ+lIpFj85D1b3yP0Z+W6t9HlqyLYUK5rnnIUN9aFhVn3X7U7kemkXow2z+2p6Cp7Mubo4F1DBEuScVgXyMHDmSH3/8kVGjRnHlyhWWLVuGvb295vNvvvmG4cOHc+7cOXx8fOjduzdZWVkAnD59ml69evHmm29y8eJFxo4dy6hRo1i4cGG+6+vZsycRERFs27aN06dPU6tWLVq3bk1MTM6Vsr59+1KxYkVOnjzJ6dOnGTFiBHp6ejRq1IipU6dibm5OeHg44eHhDB8+HIAhQ4Zw9OhRVqxYwYULF+jZsycdOnTQ+lGfkpLCTz/9xNy5c7l8+TJ2dnYEBgZy6tQpNm3axNGjR1Gr1XTq1InMzMx/YUvnSIgJIyUxEhfvRpppBkZm2FeqxoM75/KcJzsrg8h7l6nokzuPQqmkonfDZ+a5fmYz80c3YMXPXTm6dQqZGanFztHGQomFqZJrd7I009Iy4FZ4Nu5ORT+pGxnkFP4paf9Oc6ulKZgZKbR+rKdnwr0oqGjzYlzRyMrKIOzWFbyrNNRMUyqV+FRpwJ0b5wuY879dZnFZNqhB1F7tiwKRuw5h1aAGAAo9PSxqBRC150hugFpN1N4jWDaoWeL12loqsTTT4UpI7jGamq4mJCwTT5f8fxw8zcgg55SQnJp7dVNfF97rYc5fWxNJSC55S1BWZiZ3g6/iX62+ZppSqcS/Wn1Cgi6UeLkAEeF3+WJQW77+oAtzf/2a6MjwEi8rMzOTkJvXqVajjlaeVWvUJuja5SItIyM9nezsLEzNzDXTVCoVM6ZM4NUeb+Li6l7i/J6UEP2o3PR5qtx0rcaD2+fynCc7K4OIsMta8yiUSlzyKDcfy0xP4erxdZhXqIippUOR88vKyuD+7ct4Bmgfk54BDQm9mfe6Qm+e14oH8K7SRBOvUqkIOn8AGwc3Fv78DhOHNGb2uDe4crpolfX8WJrklJu3ni43o8H5OcrN0l6u9aPzUNBT56HbJTwPJT86D1Wy10VXR6F1fnsYoyImQVWs5YryR/ZuHhITE5k2bRozZ85kwIABAHh6etKkSRNu374NwPDhw+ncuTMA48aNIyAggJs3b+Ln58cvv/xC69atGTVqFAA+Pj5cuXKFn3/+mcDAwGfWd+jQIU6cOEFERAQGBjlX7yZPnsyGDRtYs2YN7777Lnfv3uWLL77Az88PAG9vb838FhYWKBQKHBxyC+i7d++yYMEC7t69i5OTkybn7du3s2DBAn744Qcg56T3+++/U716dQBu3LjBpk2bOHz4MI0a5Zwoli5diouLCxs2bKBnz56lso2flpKY04RuZKZ9RcfI1IaUxKg850lLjkWtysbY9Kl5zGyIjbilee9dqwtmVk6YmNsRHX6do1smExdxm46BM4qVo7lJTsH59I+hxGSV5rPCKICerYy4GZZzJeffYGqY82/yU3WdpDQ1pkb/yiqLLTkhDpUqGzML7X1namFNxP1b+cz13y+zuAzsbUh/qP19TX8YhZ6FGUpDA/SsLFDq6pIeEf1UTDQmvsXve/2YuWnOD/inv5sJySosTIp2vUcB9O5gyo27GdyLzG2ZerODGTdDMzlXwjEBjyUlxqJSZWNuqd0FyszSmvB7t0u8XHefKgR+/B0OTq7Ex0axedUcfv7mbcZOW4OhUfG7NSQmxKNSZWNhqd2qYGlZgXuhd4u0jL8WzMaqgg3VatTWTNuwZhlKHR06vfJ6sXPKz+Ny8+ky0NjUhuR8ys3Ux+XmU2Wt8VPlJsCFQ8s4snkymRkpWNq589oH89HJpzUu7/xyjknTPI7JqPC8j8mk+ChMzG2eiU+Mz/l7khOiyUhL4eDfc2nT4xPa9/qc6xcPsXzGJ7w9YiHufvWKnJ/WOh6VjclP9c5MTlNrytQXYbma81DKU+ehlOKdh3q0NCI4LIvwR+chcxMFmVnqZ8amJRTj/PafUco17NIkFYE8XL16lfT0dFq3bp1vTLVquQOdHB0dAYiIiMDPz4+rV6/y6quvasU3btyYqVOnkp2djY6OdjPb+fPnSUpKwtpau7BMTU0lODgYgGHDhvHOO++wZMkS2rRpQ8+ePfH09Mw3v4sXL5KdnY2Pj4/W9PT0dK316Ovra/0tV69eRVdXl/r1c6/aWVtb4+vry9WrV/Nd39PrSE/X7p+aU8HJPYFcP7OZ/WvGaN53HjS7SMsuiYAGb2j+b+3oi7GZLZvmBBIfdRcLm0r5zlfXX48+7Yw1739f+xwjux55s60RTjY6TF72fF0snlTFTUGXerkF4/L9pdO9SLy4GlQ1oH8XM837qcvin3uZ/Tqb4myny8T5uf3ga/jo4++mx9g5zzEi/F9WtVbu+IuKbj64+1RlxHudOHV4J03adPvP81m/6i8OH9zD2B+no6+fc2En+EYQWzeuYdL0uSieo59x0OnN7FuVW252HfzvlZsAvrW7Usm3EckJkZzdN59ti4by+ifLn7sv/vNQq3N+qPrXakXjDoEAOLr6E3rjLCf2rixyRaCKq4LOdZ8oNw+8mOVmXX89erd94jy07vnPQ2+0yTkP/bK89M5DovySikAejIwKv2z6ZH/VxwV7SQfaJiUl4ejoqNV//7HHt+kcO3Ysffr0YcuWLWzbto0xY8awYsUKunXL+0SXlJSEjo4Op0+ffqbiYWqa2z/eyMjouU5MeZk4cSLjxmkPrhwzZgxWtXNPYG6VW/LGsNwKSHZWztXG1MRoTMztNNNTk6KwdvLPcz2GJlYolDqkJGlfXU1NjMLYPP+BeI/vKBQffafAisCFm5ncDs8tKB8PxDI3UZKQnHvSMDNREhZR+EnkjdZGVPHQ45cVScQllV63oOthauZE5a7/cZ4mRpD0xFUoU0MFD2JfjLs/mJhbolTqPDOINyk+Ot+BwGWxzOJKfxiFgb32ugzsbciMT0SVlk5GVCyqrCwM7KyfirEm/UHeV3Dzci4og5Cw3B/nuo9KcnMTJfFJueWQuYmSuw+znp79GX07mlLd24AfF8YSm5g7v7+7PrYVdJg5Qvtv+qiXBdfvZjJpUVyRczY1s0Kp1HlmYHBiXDQWls/Xt/tJxiZm2DtWIuJBaInmNzO3QKnUeWZgcFxcDJZWBQ/o3rh2OevXLGP097/g5p57oebq5fPEx8fyfmBui6pKlc3ieb+zZeMaZi1YVaTc3ANaYj/82XIzJSkaE4vccjMlKQrbfMpNo8fl5lM3VEjJo9w0MDLDwMgMS1s3HFyr88c39Qm5uAufWl2KlK+xWc4xmZTHMWlqkfcxaWphQ3JC1DPxZo/ijc0sUeroYuukfSHM1smDO9fPFCkvgOv31Nx74u5Yuo/qBCaG2uWmyXOWm0mpz7fcfM9Dxk+dh4yLdh7q9eg89OtK7fNQQrIaPV0FRgYKrVaBnPPdi3HeeEweKFa6pH0lD97e3hgZGbFnT8G3CcyPv78/hw8f1pp2+PBhfHx8nvlRDlCrVi0ePHiArq4uXl5eWi8bm9zC0sfHh88++4ydO3fSvXt3FixYAORc1c/O1i4AatasSXZ2NhEREc8s88kuRHnlnpWVxfHjxzXToqOjCQoKonLlvG+F97SRI0cSHx+v9Ro5cqRWjL6hKRY2rpqXlb0Xxma2hN3I7V+dkZbEw7sXcHCtked6dHT1sXUO4N4T86hVKsJuHst3HoCo+9cAMDazyzcGcvpxRsapNK/waBXxSSp8K+XWnw31wd1Rh1v3C/6x9UZrI2p46zF1ZRLR8aXbJSgjC2KTcl+R8ZCYqsbdPrew1NcFZxsIi3oxCnRdXX0qulfmxqVjmmkqlYobl4/j6l39hVlmccUdO4d1qwZa02xaNyL22DkA1JmZxJ+5jE2rJ/pAKxRYt2xI3LGzRV5PWoaaiNhszet+ZDZxidlU9si9QGGor8Cjoh7BoQWP7enb0ZRafgZMWhxH1FO3H9xyKIUxs2IYOzv3BbBiRxLzNyYUOV8AXT09Knn6c+1CbtmiUqm4euEEHr4lu71rXtJSU4h8GIaFVckqf3p6enh4+XDxXO5AX5VKxcVzZ/D1C8h3vg1rlrF2xWK+/e5nvLz9tD5r3qo9U2YuYPKMeZpXBWsbXun+Jt+On1zk3PQNTbG0ddW8KjjklJuh158qN+9cwMGtRp7L0NHVx65iAGHXtcvN0BsFl5s5gWpN5aModHX1cXILIOSK9jEZcuUYLl55r8vFqzrBT8QD3Lx8RBOvq6uPs3sVoh5ody2KenAbS5ui3zr0mXIz4VG56fBUuWkN956j3IxLfr7l5nsectU+D7kV4TzUq7UR1b30mLbq2fPQ3YdZZGWrtc5vdlZKKpgrC12uKN+kRSAPhoaGfPXVV3z55Zfo6+vTuHFjIiMjuXz5coHdhR77/PPPqVu3LuPHj+eNN97g6NGjzJw5k99//z3P+DZt2tCwYUNee+01Jk2ahI+PD/fv32fLli1069aNgIAAvvjiC15//XXc3d0JCwvj5MmT9OiRc5cRNzc3kpKS2LNnD9WrV8fY2BgfHx/69u1L//79mTJlCjVr1iQyMpI9e/ZQrVo1zfiGp3l7e/Pqq68yePBg5syZg5mZGSNGjMDZ2fmZ7k75MTAw0Ix10JZ/oadQKKjWtD+n98zGwtYN8wrOnNg+HRNzO9yrtNHEbZwdiEeVNlRt0g+A6s0D2btiBLYVq2BXqRoX/llEVkYqfnW7AxAfdZcbZ/+mkn8zDI0tiQ6/zuFNE3HyqIONk2+R/p4n7T2dTqeGBkTGZhMVr6JrEyPik1Scu5H7Y+vTXiacu5HJgbM5J8w32xhR11+f2euTSM9Ua/pbpqaryXxUvpqbKDA3UWJnmVM3d7ZRkpYJMQmqEg0qPn5NRdMqSmISVcQlq2lRTUliClwLzV3WW62VXAtVc/J6zjQ9XaiQ29sES1Owt4LUdEhIyZlmqA8Wjwa/AVibKwA1SanP9oEtTLPOA1gx62tcPAKo5FWVg9uWkJGeSr3mOa1cy34fiYWVHZ17fwbkDDx8GJbTVS47K5P42Aju3b6KgaExNg6uRVpmcemYGGPildtqZOxeEfPqfmTExJMWGo7vhGEYOttzfuBXANz5YwWuH/bFb+IXhC5ci03LBjj27MjJV97TLOPW1AVUn/8TcacvEX/yAm6fDEDXxIjQRSW7s9Fju46n0qWpCQ+js4mMy6ZbS1PiElWcuZbbTW/4W5acuZbO3pM5lyn7dTKlQVVDpq+IJy1djfmj8QSp6Soys3L6ByckP7uu6PjsZyoNRdG2az8WzBiNq1dl3L2rsHvzMjLSU2ncKqdsmT/tWyyt7eje7xMgZ4BxeFjOvfmzsjKJi4kg9FYQBoZG2Dnm7JfVC3+hWt1mWNs6ER8TwaYVs1EqldRr0qHY+T3WtVsvZv4yEU9vX7x8/NmycTXpaam0bNsJgOlTvsfa2oa+gTn7df3qpaz8az5DvxyFrZ0DsTE5V8ANjYwwMjLGzNwCM3MLrXXo6OhiaVUB54r5t0oWRqFQUKN5f07tmo3lo3Lz2LacctOjam65uf73QDyqtqF605xys0aLQHYvG4GdSxXsXatx7kBOuVm5/uNyM5Qb57ZSybcxRqYVSIp7wOk9f6KrZ4Crf/M8c8lP4w4DWPvnSJzcq1DRoypHdiwmIz2V2k1zjsk1c77C3Mqedr2GAdCoXX/mTuzPoW0L8K3enAvHt3L/1mVeG5jbyty049us/P1z3Hzr4OFfnxsXDhF0bj9vj1xU4m0JcCJIRZOAR+Vm0qNyMxWuPXFHoH4tlVwLU3PqxhPl5hM3obM0BXtLSM3ILTeLstzi2HcmnQ4NDIiIzSY6XkWXxjnnofM3c89Dn/Q04fzN3PPQG22MqOOnz5wNSaRnqDE3fnQeysg5D6VlwNGLGfRoaURympq0DDW9WhkRci+L26V0NzvxYpKKQD5GjRqFrq4uo0eP5v79+zg6OvL+++8Xad5atWqxatUqRo8ezfjx43F0dOS7777Lc6Aw5BTmW7du5ZtvvmHgwIFERkbi4OBAs2bNsLe3R0dHh+joaPr378/Dhw+xsbGhe/fumu43jRo14v333+eNN94gOjqaMWPGMHbsWBYsWMCECRP4/PPPuXfvHjY2NjRo0IAuXQpu1l2wYAGffvopXbp0ISMjg2bNmrF169YCb99XGmq2fIesjFT2rxlNRmoCju616TL4T63+qAnRd0lNzm2y967RibSkGE7smEFKYiQ2Tv50eedPjM1yrgYqdfUIu3GE848qCKaWjnhUbUedNh+UKMedJ9LR11PQp70xxgYKgu9lMWNNMllPlJO2ljqYGuVOaF4zJ/9hvc20lrVoawrHLucU0k2rG9Clce7Isc/7mD0TUxxHrqjR11XTpb4SQ324G6Fm6b5ssp/47WZlqsDYAB5X0JwqKBjQNrfFqn3tnP+fC1ax6VjOjL4VFbzaMDfm9SY5/z9wQcWBi8X7YVizYUeSE2LYsWYmCXFROLv6MXjEHE03nriocK1uawmxkfwyMneg5f6/F7D/7wV4+tflw9ELi7TM4rKoXYWGe5Zo3lee/DUAoYvXcWHQSAwcbTFycdR8nno7jJOvvEflKSNx+7g/aWEPuPjet0TtOqSJCV+9DX3bCviM+STngWLnr3KiyztkPDWAuLi2HU7BQE/BgK5mGBsquXE3k1/+itP6btpV0MHMOLchuFXdnL7HIwK1B8bO25DA4fOl80yLJ9Vt0p7EhFg2LZ9FQlw0Fd19+WTUb5g/6hoUE/UAxRMDAeNiIxn/+Zua9zs3LmbnxsX4BNRm+Pi5AMRGP2TuLyNJTozH1NwKL/8ajPhxMWYWJX8uQ+NmrUmIj2PFX/OJi43BzcOLb76brOkaFBX5EOUT382dWzeSlZXJ5B9Gay2nZ59A3uj7donzKIpard4hMyOVfatGP3oQY21eeU+73IyPukvaE+WmT81OpCbFcHz7DJITIrF19ueV93LLTR09fe6HnObcgcWkpyZgbGaNk0cdXv90+TODjAtTtX4nkhNi2bNuOknxUThW8mfA8D80XYPiYsK19nkl75r0ev9ndq+dxq41v2Jt70qfT2dgXzF3vFvlOm15JXAMB//+gy1//YCNozu9P56Gm0/tZ9ZfHEeuqtHTVdO57qNyM1LNsv2Fl5v9W+eWie1q5fz/fIiKTcdVRV5ucex6fB5qZ4zRo/PQb2u1z0M2ljqYPHEealYj5/vw2Zva56El23LPMWv2paJSw+BXjNHVVXD1Vs4DxV44/6dP+C0rCvXjkTdC/MumbX7xv2qfdlXwwc9xZZ1GgWZ9Ycl3S1/8ptrRfXX5+8yLn2eXWrps0St+69B/rXNmEG+PiyjrNAo0f4wdBy6nlHUahWoeYMzFmw/LOo1CVfWyZ+bWF7/cHNJJwepjZfcwyqLo2UDJ+OUvfnk0qrcuH02OK+s0CvXbcMsyW3fy0Q1ltm6Thq+V2br/LdIiIIQQQgghygW1tAiUKtmaQgghhBBCvISkRUAIIYQQQpQPcvvQUiUtAkIIIYQQQryEpCIghBBCCCHES0i6BgkhhBBCiHJBBguXLtmaQgghhBBCvISkRUAIIYQQQpQPMli4VEmLgBBCCCGEEC8hqQgIIYQQQgjxEpKuQUIIIYQQonyQwcKlSramEEIIIYQQLyFpERBCCCGEEOWCWgYLlyppERBCCCGEEOIlJBUBIYQQQgghXkLSNUgIIYQQQpQPMli4VMnWFEIIIYQQ4iUkLQJCCCGEEKJcUCODhUuTtAgIIYQQQgjxEpIWASGEEEIIUS6oZYxAqZKtKYQQQgghxEtIoVar1WWdhBBCCCGEEIWJO7u3zNZtWbNVsef57bff+Pnnn3nw4AHVq1dnxowZ1KtXr9D5VqxYQe/evXn11VfZsGFDCbItGukaJP4zR68mlHUKhWrob862s5llnUaBOtbUY/2J7LJOo1Dd6umw6dSLn+crdXR4e1xEWadRqPlj7Nii51vWaRSoc2YQf/3z4l9b6tdUwZjFL/ZxDjCuvx6rjqrKOo1C9WqofOH3e7+mCvZfSi3rNArVoooRZ29ElXUaharpbVN2Ky9HXYNWrlzJsGHDmD17NvXr12fq1Km0b9+eoKAg7Ozs8p3v9u3bDB8+nKZNm/7rOZafrSmEEEIIIUQ58csvvzB48GAGDhxI5cqVmT17NsbGxsyfPz/febKzs+nbty/jxo3Dw8PjX89RKgJCCCGEEKJcUCsUZfYqjoyMDE6fPk2bNm0005RKJW3atOHo0aP5zvfdd99hZ2fHoEGDSryNikO6BgkhhBBCCFGI9PR00tPTtaYZGBhgYGDwTGxUVBTZ2dnY29trTbe3t+fatWt5Lv/QoUPMmzePc+fOlVrOhZEWASGEEEIIIQoxceJELCwstF4TJ04slWUnJiby1ltv8eeff2Jj89+NwZAWASGEEEIIUS6U5XMERo4cybBhw7Sm5dUaAGBjY4OOjg4PHz7Umv7w4UMcHByeiQ8ODub27dt07dpVM02lyrlZgK6uLkFBQXh6ej7vn/AMqQgIIYQQQghRiPy6AeVFX1+f2rVrs2fPHl577TUg54f9nj17GDJkyDPxfn5+XLx4UWvat99+S2JiItOmTcPFxeW588+LVASEEEIIIUT5UMxBu2Vp2LBhDBgwgDp16lCvXj2mTp1KcnIyAwcOBKB///44OzszceJEDA0NqVKlitb8lpaWAM9ML01SERBCCCGEEKKUvfHGG0RGRjJ69GgePHhAjRo12L59u2YA8d27d1Eqy3a4rlQEhBBCCCGE+BcMGTIkz65AAPv37y9w3oULF5Z+Qk+RioAQQgghhCgXynKw8P8j2ZpCCCGEEEK8hKRFQAghhBBClAtqys9g4fJAWgSEEEIIIYR4CUmLgBBCCCGEKBdkjEDpkq0phBBCCCHES0gqAkIIIYQQQryEpGuQEEIIIYQoH8rRk4XLg2K1CKjVat59910qVKiAQqHg3Llz/0pSY8eOpUaNGv/Ksp8UGBjIa6+9VmBMixYtGDp06L+ei5ubG1OnTv3X1yOEEEIIIQQUs0Vg+/btLFy4kP379+Ph4YGNjc1zJ6BQKFi/fr3WD/Lhw4fz8ccfP/eyRfmxe+sqtq3/i/i4aCq5edNv8Bd4+ATkGXvvbjDrls3hdvA1oiPD6f32Z7R/pY9WjCo7m/Ur/uDoge3Ex0VjaWVDk1ZdeKXXIBTPcTXhnx3L2bt5AYnxUThV8qXHwK9x9aqaZ2x46E22rZ5JaMgVYqPu81r/r2jR6S2tmOCrp9i7eQGht66QEBvJ259Po1rd1iXO77Gju5ZxYOt8kuKjcHTx5ZX+3+DiWS3f+AvHt7Nr7Qxio+5hbe9KxzeG4Vejuebz9LRktq/8lcun95CSFEcFW2catetHg9ZvPleeh3cu48CW+STGR+FYyZfXBnxDpQLyPH98OztW5+RpY+9Kp97D8H8iz8T4KLYs/4UbFw+TmpKIu18dXhvwNbYObs+VJ8BrLUxoVssQY0MlN0MzWbwlkYiY7HzjOzUxprafAY42OmRkwc3QTNbsTuJBdN7zfNbHgqreBsxYEcfZoIxi5VahSR08Ph+ERa0qGDrZcarHhzzctKfgeZrVo/LkEZhW9iYtNJybE2cRtni9VozrB33wGDYIAwdbEi5c4/LQ8cSfvFis3J52cu9Sju6YR1J8FPYufnTo/S3OHvnv8yuntrN/wzTiou5Rwd6V1j2G410td58f2DiDyye3khDzAB1dPRxdA2jZbSjOHtWfK0+AltWV1PZWYqgPdyPV/H0sm5jE/ONd7RQ0DlDiaK3A3FjB8n1ZXAtVaz5XKqB1TSXezkqsTCEtE0LC1ew+k01iavHzO757KYe25RznDpX86NzvGyoWsC0vndjOnnXTc7algyvte36OT/Xcbbnuz5GcPbxBax6vKk0YMPzP4if3hPKyz/dtW8GujYuIj4umopsPbw76CnfvvMv3+3dvsmnFLO6GXCE6MpyeA4fTpks/rZjNK2fx96o5WtPsndz4bsaGEue44++1bF63jPjYGCq5ezHwvc/w8q2cZ+ye7Zs4uHcbYXduAeDu5cub/d/TxGdlZbFyyR+cO3WUiAf3MTYxoUr1uvQOfJ8K1rYlzvHfppZe7aWqWFszODgYR0dHGjVqhIODA7q6/07PIlNTU6ytrf+VZYvSo1arycrKeu7lHD+0kxXzp/Lam+8w7pcluLh5M3ncxyTExeQZn56ehq2DMz37D8HCKu/vyZZ1i9m3fS393v2CH2asoteAj9m2fgm7t6wscZ5njmxjw5JJdHj9A4ZPXI2zqy+zJ75HYnx0nvGZGalY21Wka5+hmFvmXWlOT0vFydWX1wd+U+K8nnb+2Db+XvYTbbp9yMfj1+BYyY95k94lKZ8871w/y4rfv6BO8+58Mn4tAbVbs2TqxzwIvaGJ2bJ0Etcv/MMbH/zEsJ/+pnH7/mxa/D1XzuwtcZ7njm5j89KfaNv9Q4ZOWINTJT/m/ph/nrevn2XZzC+o16I7Q79fS0Cd1iz6JTdPtVrNwl8+JiYilMBhMxn6/VqsbBz544dBZKSllDhPgI6NjWlT34jFWxKZMDeG9Aw1n/ezRFcn/3l8XfXYezKVCfNimbIkDh0lDOtnib7es7FtGxihfnZykemYGJNwIYhLn4wrUryRW0XqbppD9P7jHKrzKrdmLKLqnAnYtG2iiXHs2RH/n0dyY8JvHKrXjcQL16i/ZR76thVKnOflE1vZtepHmnX9iMGj12Hv4suyqe+QnJD3Pg+9eYZ1f3xOjSavM3j0enxrtmHVb0OIuHddE1PBwY0OfUbx3rhNDPhqKRbWziz9dRDJiXmXH0XVJEBJfX8lm49n8+fWLDKz4K02uugWcNbU04UHsWq2HM+7sqenC44VFBy4kM3sLVms3J+NjTn0blnAFykfF49vZduKn2j52kd8MG4tDi6+LJo8mKR8tuXdG2dZPXs4tZv14IPv1uFfszXLpn/Mw7DrWnHeVZvy5dSDmlevDyYXO7cnlZd9fvLwDtYsnELnXu/xzc/Lqejqw/TxH5IQn/cyMzLSsLF3plu/T/Mt3wGcXDyZNHe35vXl9wtKnOORg7tZMncGr/d+m4nT5uPq7sXE0cOIj4vNM/7KxTM0bt6WUROn893kOVjb2vHD6M+IiYrM+RvS07gdHET3NwOZOG0+w77+gfv37jJ5/FclzlGUP0WuCAQGBvLxxx9z9+5dFAoFbm5uQE4rQZMmTbC0tMTa2pouXboQHBysmS8jI4MhQ4bg6OiIoaEhrq6uTJw4EUCzjG7dumkt8+muQY+78EyePBlHR0esra356KOPyMzM1MSEh4fTuXNnjIyMcHd3Z9myZUXubjNu3DhsbW0xNzfn/fffJyMj/6txsbGx9O/fHysrK4yNjenYsSM3btzQilm7di0BAQEYGBjg5ubGlClTtD6PiIiga9eumlyXLl1aaI779++nXr16mJiYYGlpSePGjblz547W9nnS0KFDadGiheZ9YmIiffv2xcTEBEdHR3799ddnuj0tWbKEOnXqYGZmhoODA3369CEiIkIrB4VCwbZt26hduzYGBgYcOnSo0NwLs2PjMpq3e42mrV/B2cWDAR+MRN/AkIN7NuUZ7+EdwJuBn9KgaTt0dfXzjLkZdIGa9ZpTo04TbO2dqNuoNQE16hNy43KJ89y/ZTENW71O/RbdcKjoSc93RqOvb8jx/evzjK/kWZVX+w2nVqNO6OSTZ+WaTen8xidUq9emxHk97dC2hdRr0ZM6zbpj7+zFawPHoG9gyKmD6/KMP7xzCT7VmtC88yDsnD1p9/onOLlV5uju3O/lnRtnqdX0NTz961HB1pn6rXrhWMmX0OCSXx0+uG0h9Vv2pG7z7thX9KL722PQMzDkxIG88zy0fQm+1ZrQossg7J096dDzE5zdKnN4Z06eUQ/ucPfmebq/PRoXz6rYObnTfeAYMjPTOXt0a4nzBGhb34jNB5M5F5RBWEQ2czckYGmmpJafQb7z/Lo0nsPn07gfmU3owyzmb0zAxlIHN0ftmoCLvS7tGxozf2MBl5oLEbnjINfHTOXhxt1Find9901Sb4Vx9cufSLoWwp3fl/Jg7Q7cPw3UxLgPHUjovFWELVpH0tVgLn44huyUNFwCe5Q4z2O7FlKzaU9qNOmBrZMXnfuNQ0/fkHOH1uYZf2L3EryqNKFRh0HYOnnS8rVPcXStzMm9ud/NqvW74lG5EVa2Ltg5e9PujRGkpyYRERZU4jwBGvgrOXhBRVComodxsO5QNmbG4Fcp/xbFm/fV7D2n0moFeFJ6Jizenc3lO2qiEyAsSs2WEyqcbZRYmBQvvyM7FlGneU9qNe2OnbMXXQeMRU/fkDP5HOdHdy3Gq2oTmnQahJ2TJ216fIqjqz/Hdy/TitPR1cfM0lbzMjKxKF5iTykv+3z35iU0adOdxq1ew8nFk77vfYu+gSFH9mzIM97NqwqvDxhG3SYd0NPLo3b/iFJHBwsrG83L1NyqxDlu2bCSVu270qJtZypWcuedj75A38CA/bv+zjP+4y/G0q5zd9w8fHB2ceW9j0egVqm4dP4UAMYmpnwzYRoNm7bGqaIr3n5VePv9YYTcDCIq4kGJ8xTlS5ErAtOmTeO7776jYsWKhIeHc/LkSQCSk5MZNmwYp06dYs+ePSiVSrp164ZKpQJg+vTpbNq0iVWrVhEUFMTSpUs1P/gfL2PBggVay8zLvn37CA4OZt++fSxatIiFCxeycOFCzef9+/fn/v377N+/n7Vr1/LHH39o/YjNz549e7h69Sr79+9n+fLlrFu3jnHj8r+qFhgYyKlTp9i0aRNHjx5FrVbTqVMnTaXk9OnT9OrVizfffJOLFy8yduxYRo0apZVrYGAgoaGh7Nu3jzVr1vD7778XmGtWVhavvfYazZs358KFCxw9epR33323WF1chg0bxuHDh9m0aRO7du3in3/+4cyZM1oxmZmZjB8/nvPnz7NhwwZu375NYGDgM8saMWIEP/74I1evXqVatfybd4siKzOT28HXqFytnmaaUqkkoHo9goNK/iPTy7caVy6c5MG9nMrS3VvXuXH1PFVrNSpZnlmZhN26gk/VBlp5+lRtwO3r50ucZ2nLysrg3u0reAVo5+kV0JA7N8/lOc+dm+fwCmioNc2namPu3Mj9u1y9a3L1zD7iYx6iVqsJvnKcyAe38a7auOR53rqCdxXtPL2rNOTOjfzz9K7yVJ7VGnPnZk6eWZk5FXhdvdwf50qlEl1dfW4FaX/Xi8PWUomlmQ5XQnIvPKSmqwkJy8TTJf8fAE8zMsgpbpNTVZpp+rrwXg9z/tqaSEKyKr9ZS51lgxpE7T2qNS1y1yGsGtQAQKGnh0WtAKL2HMkNUKuJ2nsEywY1S7TO7KwMwu9cxr1y7jGoUCpx929IWMi5POcJCzmHu7/2MesR0Jiw4Lzjs7MyOHNwJQZGZthX9CtRngBWpmBmrCAkPHefpGfCvUg1LralO1DRUB9UajVpxegNlpWVwf3bl/GonHs8KJVKPAMaEprPtgm9eR7PytrHj1fVJtx9Kv72tRP8+HFjpo7oyKZFY0lJyvtqc1GUl32elZnJ3eCr+Ferr5mmVCrxq1afkOsXSrTMxyLC7/LlO2355oPOzJs6kpjI8BLneOtmEFVr1NXKsWqNOly/dqlIy0hPTyMrOwsTM/N8Y1JSklAoFBibmpUoz/+CWqEos9f/oyL37bGwsMDMzAwdHR0cHBw003v00L46NH/+fGxtbbly5QpVqlTh7t27eHt706RJExQKBa6urppYW9ucPmiWlpZay8yLlZUVM2fOREdHBz8/Pzp37syePXsYPHgw165dY/fu3Zw8eZI6deoAMHfuXLy9vQv9u/T19Zk/fz7GxsYEBATw3Xff8cUXXzB+/HiUSu160o0bN9i0aROHDx+mUaOcgmrp0qW4uLiwYcMGevbsyS+//ELr1q0ZNWoUAD4+Ply5coWff/6ZwMBArl+/zrZt2zhx4gR16+Yc0PPmzcPf3z/fHBMSEoiPj6dLly54enoCFBj/tMTERBYtWsSyZcto3Tqn//mCBQtwcnLSinv77bc1//fw8GD69OnUrVuXpKQkTE1NNZ999913tG3btsjrLzi3OFSqbCwstbsbmFtUIDzsdomX27nHAFJTkxg5pCdKpRKVSkWPvh/QqHnHEi0vOSEWlSobMwvtrkhmFtY8vHerxHmWtpRH29PUQrup2tTcmsj7IXnOkxQXhelTf5ephQ1J8VGa96/0/4Z188cw8dOWKHV0USgUdB/0HR5+dUqUZ3IBeUbkk2diHnmaWdiQGJeTp52TO5bWjmxb+Ss9Bo1F38CIf7YtJj7mAYlxkSXKE8DcNKccePqHekKyCguTol1LUQC9O5hy424G9yJzu4282cGMm6GZnCvmmIDnZWBvQ/rDKK1p6Q+j0LMwQ2logJ6VBUpdXdIjop+KicbE16NE60xJikWtysbUXHsfmpjbEPUg72MoKT4Kk6fiTc1tSI7Xzv36+X2s++NzMjNSMbOwpd+w+RiblfzKq6lRzgk/Ke2pfNJyPysNukpoW0uHS7fUpGcWHv9Y7nH+9LaxJio8/22Z1/H25HHuVbUJ/nXaYmVTkZiIu+xeO5XFU97j3VHLUSqL332pvOzzpMRH5bul9nrNLax5cO92iZYJ4O5dlcAh32Hv5EZ8bBR/r57Nz9++zZipazA0Kl4TUEJC3udKC8sK3Au7W6RlLFs4C6sKNlStkXe5nZGRzrIFs2jUrA3GxsVsohLl1nN38r9x4wajR4/m+PHjREVFaVoC7t69S5UqVQgMDKRt27b4+vrSoUMHunTpQrt27Yq9noCAAHR0cgsiR0dHLl7MuWIcFBSErq4utWrV0nzu5eWFlVXhhUL16tUxNjbWvG/YsCFJSUmEhoZqVVoArl69iq6uLvXr5141sLa2xtfXl6tXr2piXn31Va35GjduzNSpU8nOztYso3bt2prP/fz8sLS0zDfHChUqEBgYSPv27Wnbti1t2rShV69eODo6Fvr3AYSEhJCZmUm9erlX3S0sLPD19dWKO336NGPHjuX8+fPExsZq7cvKlXMHIz2ubOUnPT2d9PR0rWkGBvl3ofg3nDi8m2MHtvPesAk4u3hw99Z1ls3/BcsKtjRp1eU/zeX/wZGdf3H35nn6f/YbVjZO3Ao6xcZF4zG3tMW7SslaWUqbjq4eAz6bzqo/vmXMuw1RKnXwqtIQv+pNUauL3gO/QVUD+nfJvRo2dVn8c+fWr7Mpzna6TJyfe3W1ho8+/m56jJ1T8iuuIoebX33eHb2elKRYzv6zmrVzhvL216ue+UGZn6ruCro2yD2/LN2b/yDw0qJUQM/mOev8O58xBf+1ag06a/7v4OKDg4svv37ZjlvXTjzTmlDWnnef/xeq1Modc1PRzQd3nyqMfL8Tpw7vpEmbbv9pLhtXL+HIwd2MnjgTff1nz8dZWVlM+3EUatQM+uiL/zS34pInC5eu564IdO3aFVdXV/7880+cnJxQqVRUqVJF08++Vq1a3Lp1i23btrF792569epFmzZtWLNmTbHW83QfPIVCofmh+jJYsGABn3zyCdu3b2flypV8++237Nq1iwYNGqBUKp/5ofPk+ImiSE5Opn379rRv356lS5dia2vL3bt3ad++/TNjJkxMCr5SMHHixGe6V40ZM4b2bwx7JtbMzBKlUof4pwYGJ8TH5DsQuChWLZxGpx4DaNA0p9Lp4uZFdGQ4f69dWKKKgIm5FUqlzjMDgxPjowscKPZfM360PZOeunqWlBCNaT55mlraPDNA98mrh5kZaexYPZW3hs7Q3EnIsZIv9+9c45+tC0tUETApIE8zi7zzNMsjz8T4KMye+LsqugcwbOJ6UlMSyc7KxNS8AtNHv0FF9ypFzu1cUAYhYbk/zh/fE8HcREl8Um6ZY26i5O7DwgfL9+1oSnVvA35cGEtsYu78/u762FbQYeYI7b/3o14WXL+byaRFcUXOubjSH0ZhYK+9XgN7GzLjE1GlpZMRFYsqKwsDO+unYqxJf6C9z4rK2NQKhVLnmcGsyQnPXql+zNTC5plBpUkJUZg8Fa9vYEwFe1cq2LtS0bMGv33dnrOH1tCk03tFyi0oVM29qNx9qfPod4apISQ9cTcfU8OcwcDPS6mAXs11sDRRsHBXVrFaA+DJ4/zpbRNd4LbMs1zIJx6ggp0LxmZWxDy8W6KKwIu8z7XWafaofI/TXm9CfDQWpVi+G5uYY+9YicgHocWe19w873NlfFwMllYFD+DfvG4ZG9f8xTcTpuLq7vXM548rAZERDxn1w3RpDXjJPFe1Kjo6mqCgIL799ltat26Nv78/sbHPXt0yNzfnjTfe4M8//2TlypWsXbuWmJicL7Oenh7Z2c93NcTX15esrCzOnj2rmXbz5s08c3na+fPnSU3NLemPHTuGqakpLi4uz8T6+/uTlZXF8ePHNdMeb4PHV8z9/f05fPiw1nyHDx/Gx8dH060pKyuL06dPaz4PCgoiLi6u0Fxr1qzJyJEjOXLkCFWqVGHZspxBXra2toSHa/c7fPIZDx4eHujp6WmNwYiPj+f69dy7MFy7do3o6Gh+/PFHmjZtip+fX5HGWORl5MiRxMfHa71GjhyZZ6yunh5unn5cuZCbm0ql4sqFk3j65n3btqJIz0hH+dRVg7wqTEWlq6tHRffK3LiUu+9VKhXXLx3Hzef5b1NYWnR19XF2q8zNK8c001QqFTcvH8PVq0ae87h61eDm5WNa025cOoqrd87flZ2dRXZ21jNjUnK2Z8kq47q6+ji7V9Zar0ql4ualY7h655/njbzy9Hp2+xsZm2FqXoHIB7cJC7lMQO1WRc4tLUNNRGy25nU/Mpu4xGwqe+RejDDUV+BRUY/g0IJ/wfXtaEotPwMmLY4jKk57W205lMKYWTGMnZ37AlixI4n5GxOKnG9JxB07h3WrBlrTbFo3IvbYOQDUmZnEn7mMTasnfvwpFFi3bEjcsbOUhI6uPo6uAdy+mjs2Qa1ScevaMSp61MhznooeNbh1VXssw60rR6jomXe8ZrlqFdmZRe9ulZEFMYm5r8h4SExR4+GYW4YY6IGzrYLQyOerCDyuBFQwU7BoVxap6YXP8zRdXX2c3AIIeeo4D7lyDJd8to2LV3WteIDgy0eoVMC2jI95QGpSHKaWJbuV5Iu8z5+kq6dHJU9/rl48oZmmUqm4duEEHj7PNw7uSWmpKUQ+DMPCqviVC109Pdy9fDUDfR/neOn8aXz88r/QsWnNUtatWMjIcVPw9H62S/HjSkD4/VC+/X4qZubPNzhclD/PVRGwsrLC2tqaP/74g5s3b7J3716GDdO+6vvLL7+wfPlyrl27xvXr11m9ejUODg6arjBubm7s2bOHBw8eFOmHe178/Pxo06YN7777LidOnODs2bO8++67GBkZFTqgNiMjg0GDBnHlyhW2bt3KmDFjGDJkyDPjAwC8vb159dVXGTx4MIcOHeL8+fP069cPZ2dnTXegzz//nD179jB+/HiuX7/OokWLmDlzJsOHDwfQdJF67733OH78OKdPn+add97ByMgo3xxv3brFyJEjOXr0KHfu3GHnzp3cuHFDM06gVatWnDp1isWLF3Pjxg3GjBnDpUu5g4fMzMwYMGAAX3zxBfv27ePy5csMGjQIpVKp2T6VKlVCX1+fGTNmEBISwqZNmxg/fnzxdsQjBgYGmJuba70K6hrU/tU+HNi1gUN7/+Z+6C0Wz/6R9LRUmrbuCsAfU8eweslMTXxWZiZ3QoK4ExJEdlYmsTGR3AkJ4mF47lWWGnWasHnNAs6dOkTkw/ucPraPHZuWUbt+ixL9TQAtOvfn6N41nDiwkQf3glk9bzwZ6anUb/4aAH/9NpLNy3/NzTMrk7Db1wi7fY3s7EziYx4SdvsakQ9y+3Omp6VoYgBiIu4RdvsasVElG1AG0KRjICf3r+H0PxuIuBfMhoXjyEhPpXaznKbolbNHsH3lL5r4xu3e4vrFQxzcuoCI+yHsWjeTe7cu0bBNXwAMjUxx96vL1uWTCb56gpiIME4dXM+ZQ5sIqF3yux016xjI8X1rOHVwAw/vBbNuQU6edZvn5Ll81gi2rsjNs0mHtwi6cIgDW3Ly3Ll2JmEhl2jcrq8m5vzx7QRfOUF0RCiXTu3hz4nvEFCnNb7VSjao+bFdx1Pp0tSEGj76ONvp8E43c+ISVZy5lvsrbvhblrSqm3sc9+tkSsNqhsxZl0BauhpzEyXmJkr0HrUwJCSruBeZrfUCiI7PfqbSUBgdE2PMq/thXj1nsKSxe0XMq/th6JLTfdB3wjCqL/hJE3/njxUYu7vgN/ELTHw9cH2/D449O3Jr2kJNzK2pC3AZ1Avnt17D1M+DKr+NRdfEiNBFed+VpigatA3kzMHVnD+8nsj7wWz9ayyZ6alUb9wdgA3zvmLP2ty7rNVr8xbBlw9xdMd8osJDOLBxBvdvX6Zuq5x9npGewt51vxAWfI646HuE377EpgVfkxD7EP86HUqcJ8CxqyqaVVXiW1GBnSV0a6xDYgpcu5tbERjQVod6vrnnCn1dcLDKeQFYmSpwsEJzRyClAt5ooYOTtYK1h7JQKnJaGUwNc1shiqpR+wGcPrCas4c2EHE/mM2Lc46fWk1zjp81f3zFztW5x0/Dtv25cekQh7ctIPJ+CHvXz+T+rcvUb5PzDJb0tGS2r/iZ0JvniI28R/CVoyyb9hEV7CrhXaVJnjkURXnZ5226vsWh3es4um8T4WEhLPvjezLSU2nUKufcvmD6t6z/a7omPiszk9Bb1wi9dY2srCzioiMIvXWNiPDc8n3Nol+4fvkUURH3CL52jtmTPkOp1KFuk5Ll2fm1N9i7YzMH9mzlXuht5v0+mfS0NJq3yenS9duU8SxfOEsTv3HNX6z660/e/3QktvaOxMVGExcbTVpqzu2Us7Ky+HXiNwTfvMbHw8egUqk0MVnF7FXwX1KjKLPX/6Pn6hqkVCpZsWIFn3zyCVWqVMHX15fp06dr3bbSzMyMSZMmcePGDXR0dKhbty5bt27V/NCeMmUKw4YN488//8TZ2Znbt2+XKJfFixczaNAgmjVrhoODAxMnTuTy5csYGhoWOF/r1q3x9vamWbNmpKen07t3b8aOHZtv/IIFC/j000/p0qULGRkZNGvWjK1bt2q6LtWqVYtVq1YxevRoxo8fj6OjI999953W3XcWLFjAO++8Q/PmzbG3t2fChAmawcV5MTY25tq1ayxatIjo6GgcHR356KOPeO+9nCbQ9u3bM2rUKL788kvS0tJ4++236d+/v2YMBeRUyN5//326dOmCubk5X375JaGhoZrtY2try8KFC/n666+ZPn06tWrVYvLkybzyyiuFbfrnVr9JOxLj41i/fA7xsdFUcvfh8zHTsXg0cCs68oFWhS42JpIxw3If3LJ9w19s3/AXvgG1GPl9zsNb+r37BeuWzmbJnJ9IiI/F0sqGFu2782qvd0qcZ61GHUlOiGXb6pkkxEXh7OrHeyNma7qmxEaFo3iiFSI+JoLJI17XvN/390L2/b0QT/86fDxmIQB3gy/x2/jcQdoblkwCoG6zV+n74fclyrN6g44kJ8awa+2MRw8+8+PtL+ZoutzERWvn6epTkzc/mMTONdPZsXoqNvauvDV0Bg4uuYPt+3w0me2rfmXlrC9JSYrHysaJ9j0/pX7rN0qUI0CNhjl57ljzKE9XP975Kv883Xxq0uejSexYPZ1tq6Zi4+DKgGHaeSbGRrL5r0kkxUdhZmlL7aav0qbb+yXO8bFth1Mw0FMwoKsZxoZKbtzN5Je/4sh6ojHTroIOZsa5+baqmzP2aESg9lileRsSOHz+qVGoz8midhUa7lmieV958tcAhC5ex4VBIzFwtMXIJXdMUertME6+8h6Vp4zE7eP+pIU94OJ73xK1K/d2wOGrt6FvWwGfMZ/kPFDs/FVOdHmHjIi87/9eFAH1OpGSFMOBjTNISojE3sWfPkP/1HQTSYi+r3Wsu3jVotvgyexbP5V963+lgp0bvT6aiZ2zDwBKpQ5R4be4cOQTUpJiMTKxxMm9KoFfLcXOufCbRRTk0GUVerrQtaFOzgPFItT8tTuLrCfqaFZmCowNcysGTtYKBrbPPa12qKsD6HD2pooNR7IxNwY/l5zvyIddtbu7LtiRxe2HRW9tqFq/E8mJsexZPz3nwYGV/On/+R+abRkfHa7VKlrJuyY93/uZ3eumsWvtr1jbu9LnkxnYV8zdlg/Dgjh3eANpKYmYWdriVaUxrbt/gq5e3rc/Loryss/rNm5PUnwsm1bMIiEuioruvnzy7e+YPzoPxUSFa+UZFxvBhOG5D1TctWkxuzYtxiegNp9/Nw+A2OiHzP11JMmJcZiaW+HlX5MRExdjZlGyZ3E0ataGhPg4Vv81l7jYGFw9vBnx3RRN16CoyIcolLk57tq6nqysTH6d+K3Wcnr0fpuefQcREx3J6eM5x/xXnwRqxYz6YQYB1Woh/v8p1CXtK/GCCwsLw8XFhd27d2vulCNyJScn4+zszJQpUxg0aNB/ss6jV//d7g6loaG/OdvOvrhXQgA61tRj/YkXY3BhQbrV02HTqRc/z1fq6PD2uJJ1g/svzR9jxxY938IDy1DnzCD++ufFP6X0a6pgzOIX+zgHGNdfj1VHX/yxcL0aKl/4/d6vqYL9l0rw+Ob/WIsqRpy9UbJxOP+lmt5lNzbuftDz3dL1eTj5ll5XsRfFv/No4DKwd+9ekpKSqFq1KuHh4Xz55Ze4ubnRrFmzsk7thXD27FmuXbtGvXr1iI+P57vvvgN45g5HQgghhBDi5fB/UxHIzMzk66+/JiQkBDMzMxo1asTSpUsLfOLfy2by5MkEBQWhr69P7dq1+eeff7CxeXHueCOEEEIIUZD/1wd7lZX/m4rA41tfirzVrFlT605FQgghhBDi5SZPZRBCCCGEEOIl9H/TIiCEEEIIIf6//b/exrOsSIuAEEIIIYQQLyFpERBCCCGEEOWCWiHXsEuTbE0hhBBCCCFeQlIREEIIIYQQ4iUkXYOEEEIIIUS5IIOFS5e0CAghhBBCCPESkhYBIYQQQghRLshg4dIlW1MIIYQQQoiXkLQICCGEEEKIckHGCJQuaREQQgghhBDiJSQVASGEEEIIIV5C0jVICCGEEEKUCzJYuHTJ1hRCCCGEEOIlJC0CQgghhBCiXJDBwqVLWgSEEEIIIYR4CUlFQAghhBBCiJeQQq1Wq8s6CSGEEEIIIQoTHBJSZuv29PAos3X/W2SMgPjPnAyKK+sUClXX15JZ28s6i4J90AGWHXrx6+99mij4+0xWWadRqC61dDlwOaWs0yhU8wBj/vrnxd7v/Zoq2KLnW9ZpFKpzZhCrj6nKOo1C9WygLDfH+q7z6WWdRoHaVjdg8+kXvzzqWluX9SeyyzqNQnWrp1PWKYhSIhUBIYQQQghRLqjVMli4NMkYASGEEEIIIV5CUhEQQgghhBDiJSRdg4QQQgghRLmglmvYpUq2phBCCCGEEC8haREQQgghhBDlgjxZuHRJi4AQQgghhBAvIWkREEIIIYQQ5YK0CJQuaREQQgghhBDiJSQVASGEEEIIIV5C0jVICCGEEEKUC9I1qHRJi4AQQgghhBAvIWkREEIIIYQQ5YK0CJQuaREQQgghhBDiJSQVASGEEEIIIV5C0jVICCGEEEKUC2q1dA0qTdIiIIQQQgghxEtIKgJl5Pbt2ygUCs6dO1fWqQghhBBClAtqFGX2+n8kXYP+A4GBgcTFxbFhwwbNNBcXF8LDw7GxsSm7xF4gu7asZsv6pcTHRlPJ3Zv+736Op09AnrFhd0NYu3QOt4KDiIoIp9+goXR4tbdWTGpKMmuWzuHUsQMkxMfi5uFDv8HD8PSu/Fx5qtVqjm2bzsWjq0lPTcDJvRateo7Fys6twPnO/7OUU3vnkZIQiY2zHy17jMLBtRoAaclxHN02g7tBh0iIDcfYpAKe1drQsNOnGBiZlSjPE3uXcmT7PJLio3Bw8aNjn29x9qiWb/zlk9vZt2EacVH3sLZ3pc3rw/Gu1jzP2L8Xj+H0gZW0f3MkDdoOKFF+jx3auYz9mxeQGB+FUyVfugV+TSWvvPN8EHqT7WtmEBZyhdio+7z61lc069T/uZZZFPu2rWTnhkXEx0VT0c2H3u98hbt3lTxj798NZuOK37kbfJXoyHB6DRxOm659tWI2rZjN36vmaE2zd3Zj/Iz1Jc4R4OTepRzdkbPP7V386NC74H1+5dR29j/a5xXsXWndQ3ufH9g4g8snt5IQ8wAdXT0cXQNo2W0ozh7VS5RfhSZ18Ph8EBa1qmDoZMepHh/ycNOegudpVo/Kk0dgWtmbtNBwbk6cRdhi7e3k+kEfPIYNwsDBloQL17g8dDzxJy+WKMfHju1eyqFt8zXHT5d+31DRM/9teenEdnavm645ftr1+hzf6trHT8T9YHaunMKtoJOosrOxc/ak98fTsLR2KnGe5eU4P7B9BXs2LyQhLgpnVx96vj0SN6+qecaGh97k75W/EXrrKjGR9+kx4Atadn5LK+afnSv5Z+cqYiLvA+BQ0ZOOr79HQM2mz5Xn4Z3L2P93TtnhWMmXbgMKLjvOH9vB9tUziI26h42DK53fHIZ/zWaazxPjo9iy/BeuXzhCakoiHn61eW3AN9g6upY4x6O7lnFga85309HFl1f6f4NLAd/NC8e3s2ttTo7W9q50fGMYfjVy9/mIt/I+J3Z883Oadx5U4jxF+SEtAoXIyMj4V5aro6ODg4MDurpSFzv2zy6WzptGtzcHMeHXRVRy8+KnMZ8SHxeTZ3x6ehq2Ds680f9DLKys84yZO/MHLp07wQefjWXi9KVUqVGfH0cNISY64rlyPbXnT84eXELrXmN587NV6OkbsX72ILIy0/OdJ+jMVg6un0iD9h/R54v12Dr5sX7WIFISowFIio8gOT6Cpq9+xVsj/qZd34ncvvoPu5Z/U6IcL53Yys6VP9L8lY94b8w67F18+evXd0hOiM4zPvTmGdb+8Tk1m77Oe2PW41uzDStmDiEi7PozsVfP7CIs5DxmlnYlyu1JZ49uY9OSSbTr8SGf/bAaJ1df/vjxPRLj884zIyMVazsXOvf+DDPLvCvQxV1mYU4e2sHqBVPo0us9vp28DBc3H6Z99yEJ+Xw3M9LTsLWvSLe3PsE8nxwBnFw8+XneLs3ry+/nlyi/xy6f2MquVT/SrOtHDB6ds8+XTS14n6/743NqNHmdwaNz9vmq34YQcS93n1dwcKNDn1G8N24TA75aioW1M0t/HURyYt5/e2F0TIxJuBDEpU/GFSneyK0idTfNIXr/cQ7VeZVbMxZRdc4EbNo20cQ49uyI/88juTHhNw7V60bihWvU3zIPfdsKJcoR4OLxrWxb/hMtX/2ID8etxcHFl4WTB5OUz7a8e+Msq2YNp3azHnz43Tr8a7Vm2bSPefjE8RP98C5/TuiLjZM7g0YuYsiEDbR45QN09QxKnGd5Oc5PH9nO+sU/0/H19/nqp5U4u/ry2/fv53+cp6dhY1+RV/p8mu8xZFnBnlf7DOXLH1fwxcTl+FSpxx+TPiU89GaJ8zx3dBub/ppE2+4fMvT71ThV8uXPAsqO29fPsnTmF9Rr0Z3PflhDldqtWPjLx4SH3gByLhotnPIJ0RFhBH4+g89+WIOVjRNzJg4iPS2lRDmeP7aNv5f9RJtuH/Lx+DU4VvJj3qR3SconxzvXz7Li9y+o07w7n4xfS0Dt1iyZ+jEPHuUI8M2MA1qv1wdPQKFQUKVuuxLlKMofqQg8pUWLFgwZMoShQ4diY2ND+/btAbh06RIdO3bE1NQUe3t73nrrLaKiojTzrVmzhqpVq2JkZIS1tTVt2rQhOTmZsWPHsmjRIjZu3IhCoUChULB///5nugbt378fhULBnj17qFOnDsbGxjRq1IigoCCt/CZMmICdnR1mZma88847jBgxgho1ahT4NxWWu0qlYtKkSXh5eWFgYEClSpX4/vvvNZ8fOXKEGjVqYGhoSJ06ddiwYUOpdmvatnE5Ldu9SvM2XXGu5MHAD0dgYGDIgd2b84z39K5Mn4Gf0LBZO/T09J/5PCM9jZNH9vFm4BD8qtTEwcmFHn0GY+9YkT3b1pU4T7VazdkDi6nf7gM8q7bB1tmP9v0mkRwfQfDF3fnOd2b/Aqo06kVAgx5YO3jRutc4dPUNuXxsLQA2Tj50GTQDjyqtsLSphItPQxp1HsqtS3tRZWcVO89jOxdSq1lPajbpga2TF13eGoeeviFnD63NM/747iV4VWlC4w6DsHXypFW3T3F0rcyJvUu14hJiH7Jt2QS6D/4Zpc7zV2APbllEg1avU69FNxwqetFj0Bj09A05sT/vfVTJsypd+w6nZqNO6Oo+u99LsszC7Nr8F03adqdx61dxcvGk73vfoG9gyOG9G/KMd/MO4PUBn1GvSQf09PTyXa5SRwcLKxvNy8zcqkT5PXZs10JqNu1JjUf7vHO/nH1+Lp99fuLRPm/0aJ+3fC1nn598Yp9Xrd8Vj8qNsLJ1wc7Zm3ZvjCA9NYmIsKA8l1mYyB0HuT5mKg835n+sPMn13TdJvRXG1S9/IulaCHd+X8qDtTtw/zRQE+M+dCCh81YRtmgdSVeDufjhGLJT0nAJ7FGiHAEOb19EneY9qd2sO3bOXrwSOBY9fUNOH8z7O3Rk52K8qzahaadB2Dl50qbHpzi6+XNs9zJNzO61U/Gp3owOb3yBk2tlrO0r4V+rFabmeV/IKIrycpzv/XsxjVr3oGHL13Cs6Mmbg0ehr2/E0X0b8ox39apCt7c+p07jjujmUb4DVK3TgoBaTbFzdMXeyY1Xen+CgaExt25cKHGeB7Yuon7Lp8oOA0NOHsh7v/+z/S98qzehZde3sXf2pEOvT3B2r8zhnTn7PerBHe7cPE+Pt0dTybMqdk7udH97NJkZ6Zw7urVEOR7atpB6LXpSp1l37J29eG3gGPQNDDmVz3fz8M4l+FRrQvPOg7Bz9qTd65/g5FaZo7tz97mZpa3W68rpvXj418PazqVEOf4XpGtQ6ZKKQB4WLVqEvr4+hw8fZvbs2cTFxdGqVStq1qzJqVOn2L59Ow8fPqRXr14AhIeH07t3b95++22uXr3K/v376d69O2q1muHDh9OrVy86dOhAeHg44eHhNGrUKN91f/PNN0yZMoVTp06hq6vL22+/rfls6dKlfP/99/z000+cPn2aSpUqMWvWrAL/lsJyBxg5ciQ//vgjo0aN4sqVKyxbtgx7e3sAEhIS6Nq1K1WrVuXMmTOMHz+er7766nk2r5aszExu3bxGQI16mmlKpZKA6nW5ea1kzfvZ2dmoVNno6WtfbdPXNyDoyvkS55oQHUZKQiQuPrn7z8DIDAfX6oTfOpt3LlkZRIRe1ppHoVRSyacR4bfzngcgIy0JfUPTYp+Is7MyuH/nMh7+2uvzqNyQsOBzec4TGnwOj8ra30nPgMZa8WqVivVzv6RR+0HYOXsXK6e8ZGVlEHbrCt5VGmqmKZVKfKo04M6Nku2j0l5mVmYmd4Ov4l+tvtby/KvVJySo5D84ACLC7/LFoLZ8/UEX5v76NdGR4SVeVnZWBuF3LuNeWXufu/s3JCzkXJ7zhIWcw91fe597PLXPn17HmYMrMTAyw76iX4lzLQ7LBjWI2ntUa1rkrkNYNagBgEJPD4taAUTtOZIboFYTtfcIlg1qlmidWVkZ3L99Gc8A7e+QZ0BDQm+ey3Oe0JvnteIBvKs00cSrVCqCzh/AxsGNhT+/w8QhjZk97g2unC5ahSgv5ec4zyQ05Cq+VRtopimVSnyr1ufW9ZKXxU9SqbI5dXgbGempuPuUrNtaVlYG925dweepssO7gLLjzo1zeFdpoDXNt1pj7tw4l7PMzJzeBE9WZpRKJbq6+twKOlOyHG9fwStAe1t6BTTkTj7fzTs3z+H11HfTp2rjfP+mxPgorp0/SN3mJa9Ii/JH+qXkwdvbm0mTJmneT5gwgZo1a/LDDz9ops2fPx8XFxeuX79OUlISWVlZdO/eHVfXnL5/Vavm9n80MjIiPT0dBweHQtf9/fff07x5Tv+9ESNG0LlzZ9LS0jA0NGTGjBkMGjSIgQMHAjB69Gh27txJUlJSvsubOXNmgbk7Ojoybdo0Zs6cyYABOf1APT09adIkp/l92bJlKBQK/vzzTwwNDalcuTL37t1j8ODBhf4tRZGYEIdKlY2FpXZTvoVlBcLv3SnRMo2MTfD2q8qGlfNxruiGhWUFjhzcyY2gS9g7VixxrsmJkQCYmGlfxTM2syY5MSqvWUhNjkWtysY4j3liIkLynicphuM7fqdKozeKnWNKYs76TJ660mhibkNU+K0850mKj3om3tTchqSE3L/p0LY/USp1qN/mradnL5HkR/vdzOKp9VpYE3E/7zz/62UmJcaiUmVj/tR308zSmvB7t0uUI4C7TxUCP/4OBydX4mOj2LxqDj9/8zZjp63B0Mik2MtLScrZ509fXTYxtyHqQfH2eXK89vf4+vl9rPvjczIzUjGzsKXfsPkYmz1f60VRGdjbkP5QO5/0h1HoWZihNDRAz8oCpa4u6RHRT8VEY+LrUaJ1piTmfIdM8/gOFXz82DwTn/hoWyYnRJORlsLBv+fSpscntO/1OdcvHmL5jE94e8RC3P3q5bXYQvIsH8d5UkLOMWRmqb1ec0trHpbwOH/s3t3rTPnmLbIyMzAwNGbw8Kk4VvQs0bKS89nvZgWUHYlxUXmWNYlxOd9HOyd3LG0c2bpiKq8PGoO+oREHty4mPuYBCbGRxc4x97v51HfN3JrI+3mfS5LiovL4LtuQFJ/3+erMPxsxMDQmoE7bYuf3X/p/vTJfVqQikIfatWtrvT9//jz79u3D1NT0mdjg4GDatWtH69atqVq1Ku3bt6ddu3a8/vrrWFkV/4RZrVruoB9HR0cAIiIiqFSpEkFBQXz44Yda8fXq1WPv3r35Lq+w3OPi4khPT6d169Z5zh8UFES1atUwNDTUWmdB0tPTSU/X7jNvYFDyvrAl8f5nY/lz+gQ+HtgFpVIHN09fGjZtx+3ga0VexrVTm9izcozm/avvzSkgunSkpyWx4Y/3qODgSYOOQ/719RXF/duXOL57Ce+NXotCIQXw86paK7ePe0U3H9x9qjLivU6cOryTJm26lWFmz3Lzq8+7o9eTkhTL2X9Ws3bOUN7+etUzPyhF/tRqNQD+tVrRuEMgAI6u/oTeOMuJvStLVBH4N5S349zeyZ2RP68mNSWJs8d2seS3b/l03PwSVwZKm46uHoFDp7Hqz1GMfrcRSqUO3lUa4Fe9KWrUZZ1enk4dXEeNRl2eaU0X/9+kIpAHExPtq3JJSUl07dqVn3766ZlYR0dHdHR02LVrF0eOHGHnzp3MmDGDb775huPHj+Pu7l6sdT/Zr/hxYaxSqUrwVxQt95CQvK8kPI+JEycybpz2gMAxY8bQuffQZ2LNzC1RKnWeGRgcHxfzTCtBcdg7VuTbibNJS0slNSUZqwo2zJj0DbYORb9Dh0eVVji45jY1Z2flNPUmJ0ZjYpE7iC4lMRpb57y7SxiZWKFQ6mgGBj85j4mZ9pWdjLQkNsx6B30DE7oO+g0dnfz7mOfH2CxnfU8PGExOiHrmStJjphY2z8QnJURh+ugq590bp0lOjObXL1tpPlerstm58ieO7VrE0En5V0TzY/Jovz89EC8pPjrfgcD/9TJNzaxQKnWeGRicGBeNhWXp/RA2NjHD3rESEQ9CSza/ac4+f3owa0n2uclT8foGxlSwd6WCvSsVPWvw29ftOXtoDU06vVeiXIsj/WEUBvba+RjY25AZn4gqLZ2MqFhUWVkY2Fk/FWNN+oO8r3gWxtgs5zv09ODLpPjoQrZl1DPxZo/ijc0sUeroYuuk/QPV1smDO9eL30UkZ5nl4zg3Nc85hh5fJX8sIS66wMH0RaGrq4etQyUAKnlU5m7wJfZvXUrvd0cXe1km+ez3xPj88zSztMmnrMn9Plb0CGDYxHWkpiSSnZWJqXkFpo16ExePvO+IV5Dc7+ZT37WEaEzzydHU0iaP73Le35FbQaeIDL9F74+mFDu3/5o8UKx0yRiBIqhVqxaXL1/Gzc0NLy8vrdfjSoNCoaBx48aMGzeOs2fPoq+vz/r1Obe509fXJzs7+7nz8PX15eTJk1rTnn5f3Ny9vb0xMjJiz568b+Pn6+vLxYsXta7wF7bOkSNHEh8fr/UaOXJknrG6enq4e/lx+XzuMlUqFZcvnMTLL+/byxWHoaERVhVsSE5K4OLZY9Su16zwmR7RNzTF0tZV86rg4IWxuS2h13P7LaenJfHgznkc3fPuk6yjq4+dS4DWPGqVitDrR3F0y50nPS2JdbMGodTV45XBs0p8NxEdXX2cXAMIuaq9vpCrx6joWSPPeVw8a3DrqnZf7JArRzTx1Rq+wgdjN/L+mPWal5mlHY06DKLfsLklylNXV5+K7pW5cemYZppKpeLG5eO4epesn29pL1NXT49Knv5cu3Bca3lXL5zAw7fktyN9WlpqCpEPw7CwKtkPIx1dfRxdA7j91D6/de0YFT1q5DlPRY9n9/mtJ/Z5ftRqFdmZ/86d1J4Wd+wc1q20+2DbtG5E7LFzOblkZhJ/5jI2rZ7oA61QYN2yIXHH8h9/UxBdXX2c3AIIuaL9HQq5cgwXrxp5zuPiVZ3gJ+IBbl4+oonX1dXH2b3KM920oh7cxtKmZLcOLT/HuR4uHv4EXdI+hq5fOl7i/vz5UatUmn75xZWzjypz47L2fr9ZQNnh6l1Dq6wBuH7xKK7eNZ6JNTI2w9S8ApHhdwgLuUxA7VbPxBQpR7fK3LzydI7HcM3nu+nqVYObl7VzvHHpaJ5/08n963B2D8DJ9b8ZAyReHFIRKIKPPvqImJgYevfuzcmTJwkODmbHjh0MHDiQ7Oxsjh8/zg8//MCpU6e4e/cu69atIzIyEn9/fwDc3Ny4cOECQUFBREVFkZmZWaI8Pv74Y+bNm8eiRYu4ceMGEyZM4MKFCwU24xaWu6GhIV999RVffvklixcvJjg4mGPHjjFv3jwA+vTpg0ql4t133+Xq1avs2LGDyZMnA+S7XgMDA8zNzbVeBXUN6vhqb/bv3MjBPVu4F3qLBbN+Ij0tjeatuwAw+9exrFz0myY+KzOTOyHXuRNynaysTGJiIrkTcp0H93OvqF44c4zzp48S8eA+F88e5/tvPsTR2ZVmbboWfYM/RaFQULN5f07snEXwxT1E3Q9ix19fYmJhh2fVNpq4tTMHcO7gX5r3tVoM5NLRVVw5sZ6YB8HsWT2WzIxUKtfvDuRUAtb//jZZ6Sm07f09GWlJJCdEkpwQiUpV/Apkg3aBnDm4mnOH1xN5P5i//xpLZnoqNRrnrG/93K/YvTb3qk/9Nm9x89IhjuyYT1R4CPs3zuD+7cvUa5Vz/3tjUyvsKvpovZQ6upha2GDjULK+2ADNOg/g+L41nDywgYf3glk7/zsy0lOp1zyne8yy30eyZfmvmvicwXJXuXf7KtlZmcTHRnDv9lWiHtwp8jKLq23Xfvyzez1H9m0iPCyEpXN+ICM9lcatXgVg/rRvWffX9NwcMzMJvRVE6K0gsrIyiYuJIPRWEBHhdzUxqxf+QtDlU0RF3Cf42jlm/TQMpVJJvSYdSpQjQIO2Ofv8/KN9vvXRPq/+aJ9vmPcVe57Y5/XavEXw5UMcfbTPDzza53Uf7fOM9BT2rvuFsOBzxEXfI/z2JTYt+JqE2If41ylZnjomxphX98O8es4PDWP3iphX98PQJacLpO+EYVRfkNtyeeePFRi7u+A38QtMfD1wfb8Pjj07cmvaQk3MrakLcBnUC+e3XsPUz4Mqv41F18SI0EUlvztY4w4DOHVgNWcObSDifjCbFo0jIz2V2k1zvkNr5nzFzlW/aOIbtevPjYuHOLRtAZH3Q9izfib3b12mQZs+mpimHd/m0vHtnNy/iuiHdzi2aylB5/ZTr3XvZ9ZfVOXlOG/VpT9H9qzl2P6NPAgLYeXcCaSnp9KgxWsALJ75NRuXTdPEZ2VlEnb7GmG3r2mOobDb14h8kHsMbVw2jZtXThEdcY97d6+zcdk0blw5RZ2mnUucZ/NOj8qOgzllx7r535GRlkrdR2XH8t9HsnVFbnnUtEM/gi4cZv+WhUTcC2HHmt8IC7lE43a5+/38sR3cvHKC6IehXDq1lz8mvkOVOq3wrda4RDk26RjIyf1rOP3PBiLuBbNh4aPvZrOcHFfOHsH2lbnfzcbt3uL6xUMc3LqAiPsh7Fo3k3u3LtGwjfazTdJSk7h4YocMEn5JSdegInBycuLw4cN89dVXtGvXjvT0dFxdXenQoQNKpRJzc3MOHjzI1KlTSUhIwNXVlSlTptCxY0cABg8ezP79+6lTpw5JSUns27cPNze3YufRt29fQkJCGD58OGlpafTq1YvAwEBOnDhR4twBRo0aha6uLqNHj+b+/fs4Ojry/vvvA2Bubs7mzZv54IMPqFGjBlWrVmX06NH06dNHa9zA82jQtC0J8XGsXfYH8bHRuHr48OXYqZpnBERFPkShyK2zxsZE8s3Q3MFsW9cvZev6pfhVqcW3P+TcRSklJYlVi38nJioCEzNz6jVsSc+3Pnju5zbUaT2YrIxU9qwcnfNAMY/adHt/rtYV/LjoUFKTYzXvfWt1IjUphqNbp+c8UKyiP6+9P1czwDAi9DIP7uTcxWHheO1BWgNH78HCungDnKvU60RKYgz7N8wgKSESBxd/+n72p6Y5OD7mvlYlzsWrFt0HT2bf+qnsXfcrFezceHPITOwq+hRv4xRTzYYdSU6IYceamY8eNOTH4BFzNN144qLCtfJMiI3kl5Gva97v/3sB+/9egKd/XT4cvbBIyyyuuk3ak5gQy6bls0iIi6aiuy+fjPoN80fN/zFRD1Aoc7+bcbGRjP/8Tc37nRsXs3PjYnwCajN8fM5V1djoh8z9ZSTJifGYmlvh5V+DET8uxsyi5F3hAup1IiUphgMbc/a5vYs/fYbm7vOE6Gf3ebdH+3zf+px93uujmdg55+xzpVKHqPBbXDjyCSlJsRiZWOLkXpXAr5aW+G4yFrWr0HDPEs37ypO/BiB08TouDBqJgaMtRo8qBQCpt8M4+cp7VJ4yEreP+5MW9oCL731L1K5Dmpjw1dvQt62Az5hPch4odv4qJ7q8Q8ZTA4iLo2r9TiQnxLJn3fSchzZV8mfA8D802zIuJlxrn1fyrkmv939m99pp7FrzK9b2rvT5dAb2Txw/leu05ZXAMRz8+w+2/PUDNo7u9P54Gm4+tZ9Zf1GVl+O8dqMOJCXEsmXV7yTGReHs5stHX8/SPoaeKN/jYyL48cvcu9rt2byIPZsX4VW5DkPH5jxvIyk+hsW/fUtCbCSGxqY4u/rw4Tez8a+mfYec4qjRsCNJj8qOxLgonFz9eGfEHE0Xr9jocBTK3O3p5lOTvh9NYvvq6WxbORUbB1cCh83A0SX3+EiIi2TTX5NIio/CzMqWOk1eoU3390ucY/UGHUlOjGHX2hmPHpjox9tf5OYYFx2utS1dfWry5geT2LlmOjtWT8XG3pW3hs7AwUX7GD5/dCugpkbDklek/ksqGSxcqhTqxyOZRLnUtm1bHBwcWLJkSeHBpWTp0qUMHDiQ+Ph4jIyMijzfyaC4fy+pUlLX15JZ28s6i4J90AGWHXrxD9s+TRT8fab4z0H4r3WppcuByyV7wM9/qXmAMX/982Lv935NFWzR8y3rNArVOTOI1cdKPvbqv9KzgbLcHOu7zuf/UMUXQdvqBmw+/eKXR11r67L+xPN3Jf63daunU2brPnej+HddKi01vG3LbN3/FmkRKEdSUlKYPXs27du3R0dHh+XLl7N792527dr1r6538eLFeHh44OzszPnz5/nqq6/o1atXsSoBQgghhBDPS24fWrqkIlCOKBQKtm7dyvfff09aWhq+vr6sXbuWNm3aFD7zc3jw4AGjR4/mwYMHODo60rNnT60nDwshhBBCiPJHKgLliJGREbt3l/xplCX15Zdf8uWXX/7n6xVCCCGEEP8eqQgIIYQQQohyQZ4jULrk9qFCCCGEEEK8hKQiIIQQQgghygU1ijJ7lcRvv/2Gm5sbhoaG1K9fv8Bbvv/55580bdoUKysrrKysaNOmTYHxpUEqAkIIIYQQQpSylStXMmzYMMaMGcOZM2eoXr067du3JyIiIs/4/fv307t3b/bt28fRo0dxcXGhXbt23Lt371/LUSoCQgghhBBClLJffvmFwYMHM3DgQCpXrszs2bMxNjZm/vz5ecYvXbqUDz/8kBo1auDn58fcuXNRqVTs2bPnX8tRKgJCCCGEEKJcUKsVZfYqjoyMDE6fPq11i3elUkmbNm04evRokZaRkpJCZmYmFSqU/MnzhZG7BgkhhBBCCFGI9PR00tO1n6JtYGCAgYHBM7FRUVFkZ2djb2+vNd3e3p5r164VaX1fffUVTk5O/+rzoqRFQAghhBBClAtlOVh44sSJWFhYaL0mTpz4r/ydP/74IytWrGD9+vUYGhr+K+sAaREQQgghhBCiUCNHjmTYsGFa0/JqDQCwsbFBR0eHhw8fak1/+PAhDg4OBa5n8uTJ/Pjjj+zevZtq1ao9X9KFkBYBIYQQQghRLpTlGAEDAwPMzc21XvlVBPT19aldu7bWQN/HA38bNmyY7983adIkxo8fz/bt26lTp06pb7+nSYuAEEIIIYQQpWzYsGEMGDCAOnXqUK9ePaZOnUpycjIDBw4EoH///jg7O2u6F/3000+MHj2aZcuW4ebmxoMHDwAwNTXF1NT0X8lRKgJCCCGEEEKUsjfeeIPIyEhGjx7NgwcPqFGjBtu3b9cMIL579y5KZW7nnFmzZpGRkcHrr7+utZwxY8YwduzYfyVHqQgIIYQQQohyQVXWCRTTkCFDGDJkSJ6f7d+/X+v97du3//2EniJjBIQQQgghhHgJSYuAEEIIIYQoF4r7YC9RMGkREEIIIYQQ4iUkFQEhhBBCCCFeQgq1Wq0u6ySEEEIIIYQozJGriWW27kb+ZmW27n+LjBEQ/5m2fU+XdQqF2rW0Np9OK7tCpiimfWpG53culXUahdoytwrjl2eVdRqFGtVbl4s3HxYeWMaqetkzZnFmWadRoHH99Vh97MW/p0fPBkq26PmWdRqF6pwZROs3T5R1GoXas6JeuSg3OwZeKOs0CrVtYTWadD1Q1mkU6tDm5mWdgiglUhEQQgghhBDlggwWLl0yRkAIIYQQQoiXkFQEhBBCCCGEeAlJ1yAhhBBCCFEuqJGuQaVJWgSEEEIIIYR4CUmLgBBCCCGEKBdUctP7UiUtAkIIIYQQQryEpEVACCGEEEKUCzJGoHRJi4AQQgghhBAvIakICCGEEEII8RKSrkFCCCGEEKJckCcLly5pERBCCCGEEOIlJC0CQgghhBCiXFDL7UNLlbQICCGEEEII8RKSioAQQgghhBAvIekaJIQQQgghygWVPEegVEmLgBBCCCGEEC8hqQgUk0KhYMOGDWWdhhBCCCHES0etVpTZ6/+RdA0qRxYuXMjQoUOJi4v7V5Z/+/Zt3N3dOXv2LDVq1PhX1lGYAT0c6djSFlMTHS5fT2L6/Lvce5ieb3yX1jZ0bWOLva0BAHfCUvlrfTgnzycAYG+jz1/TquY57/hpwRw8EVfsHDs20KdhFT2MDBTcup/N6n1pRMblfxsDTycdWtXWx8VOiYWpkrmbU7kYkqUVo68HXRsbUM1DF2MjBTHxKg6ez+Twxcxi5/dYv1ftaN/UChNjHa7eTOG3v+5zPyIj3/hOLSrQqUUF7K31ALhzP53lmyM4fSkpz/hxn7pSp6oZ42fe4di5xBLn2byqkpqeCgz1IDRKzbaTKmLyXiUAlWyhob8SRysFZsYKVh3MJujes9u/uMvNz7a/17Fp7QriYmNwdfdk0Puf4u1bOc/YXds3c2DvDkJvhwDg4eVLnwGD842fM3Myu7ZtInDwELq81qv4yT2lZXUltb2VGOrD3Ug1fx/LJqaAXeNqp6BxgBJHawXmxgqW78viWmjutlQqoHVNJd7OSqxMIS0TQsLV7D6TTWJqyXI8tnsph7bNJyk+CgcXP7r0+4aKntXyjb90Yju7100nLuoe1vautOv1Ob7Vm2vFRNwPZufKKdwKOokqOxs7Z096fzwNS2unYudXoUkdPD4fhEWtKhg62XGqx4c83LSn4Hma1aPy5BGYVvYmLTScmxNnEbZ4vVaM6wd98Bg2CAMHWxIuXOPy0PHEn7xY7PzyEtjTmU6tbDE10eVSUCLT5t3m3oP8y82ube14pY2dVrm5ZN09TpyL18RYWejxXj8Xalc1x8hQh7DwNJauv88/J2JLlGN5KTff6mZPh+YVMDHW4cqNZGYuvsf9h/mXm51bVqBzK2vsbfQBuHMvjWUbIzh1MffA+2mEB9X8TLXm27IvmpmL7pU4z0F93ejazgEzE10uXk1g8u83CAsv2kHZ73UX3h/gwaqNYUyfG6yZ/kp7R9o2t8PH0xQTY106vHmIpOTsEucoygdpESgnMjNLXrBlZORfiL1I3uhiz2vt7Zi24A4fj75GWrqKiSO80dPLvxYeFZPJvBX3+Oibq3z07VXOXU5k3DBPXJ0NAYiMzqDXh+e1XovW3CclNZsTjyoLxdG6tj7Nauizam86v65MISNTzfuvGaOrk/88+npwLyqbNfvzPzF3a2qAv6suS3akMXFxMvvPZdKjhQFV3AtYcAFe72BD19bW/PbXfYb9EExauorxn7mhp1vAtozNZOHaB3w6PphPJwRz4VoSo4ZUopKTwTOxr7W1pjTu4NbIX0E9HwVbT6qYvyubzCzo01IHnQJKJj1dBQ9jYdtpVakuNy+HD+5h0Z+/0bNPIJOmz8XN3YsJo4YTH5f3j6HLF8/SpFlrxk6cxg9TZmFja8f4UcOJjop8Jvb4kYPcuHaFCtY2xUsqH00ClNT3V7L5eDZ/bs0iMwveaqOLboHbEh7EqtlyPO+TvZ4uOFZQcOBCNrO3ZLFyfzY25tC7Zcm+lxePb2Xb8p9o+epHfDhuLQ4uviycPJikhOg84+/eOMuqWcOp3awHH363Dv9arVk27WMehl3XxEQ/vMufE/pi4+TOoJGLGDJhAy1e+QBdvWe/t0WhY2JMwoUgLn0yrkjxRm4VqbtpDtH7j3OozqvcmrGIqnMmYNO2iSbGsWdH/H8eyY0Jv3GoXjcSL1yj/pZ56NtWKFGOT3rzFUe6dbBn6tzbDPn2MmnpKn4c6VtwuRmdwZ/LQ/ng60t8+M1lzl5O4Lvh3rhWNNLEjPjIAxdHQ779+QaDv7zEPydiGTXUCy8342LnWF7KzZ6dbHmlrQ0zFt1j6Hc3SUtXMeFz94K3ZWwmC1Y/4OOxN/hk7A3OX01i9Keuz5Sb2/ZH0+fTK5rX/JXhJcoRoG8PF17v4szk32/w7vCzpKZl88t3VdEvIM/H/LzNeKWDIzdvPXtVxMBAyfEzpD2EQgAA90NJREFUMSxZfbfEuf0X1Oqye/0/KhcVgRYtWvDxxx8zdOhQrKyssLe3588//yQ5OZmBAwdiZmaGl5cX27Zt08yTnZ3NoEGDcHd3x8jICF9fX6ZNm6b5PC0tjYCAAN59913NtODgYMzMzJg/f36B+URFRdGtWzeMjY3x9vZm06ZNWp9funSJjh07Ympqir29PW+99RZRUVGaz7dv306TJk2wtLTE2tqaLl26EBycWyu/ffs2CoWClStX0rx5cwwNDVm6dCkDBw4kPj4ehUKBQqFg7NixeeY3duxYatSowdy5c3F3d8fQ0LBI63V3dwegZs2aKBQKWrRoofls7ty5+Pv7Y2hoiJ+fH7///nuB26gkunWwZ+mGBxw9Hc+t0FR+mnULa0s9Gte2zHeeY2fjOXE+gXsP07n3IJ0Fq++TmqbC38sEAJUaYuOztF6N61hy4Hgsaen5/5DMT/Oaeuw8kc6lkCzuR6n4a2caFiYKqnrm37h29U42W49mcCE4K98Yd0cdTlzN5Oa9bGIS1Ry9lMn9SBWVHEp2Qnu1jTUr/47g2LlEboelM2V+GBUsdWlY0zzfeU6cT+TUxSTuR2Rw/2EGi9dHkJauws9D+8Tv4WJIt7Y2TFtQ8qtZj9XzVfLPZRXX76mJiIONx1SYGYFfxfxPaMHhavZfVBEUln+pXJLl5mXz+lW06dCFVm074VLJjXeHfI6BoSF7d27JM37oF6Pp0KUb7p7eOLu48v4nX6JWqbh4/rRWXHRUJPNmT+PTL0aho1M6DbMN/JUcvKAiKFTNwzhYdygbM2Pwq5T/33zzvpq951RarQBPSs+ExbuzuXxHTXQChEWp2XJChbONEguT4ud4ePsi6jTvSe1m3bFz9uKVwLHo6Rty+uC6POOP7FyMd9UmNO00CDsnT9r0+BRHN3+O7V6midm9dio+1ZvR4Y0vcHKtjLV9JfxrtcLU3Lr4CQKROw5yfcxUHm7cXaR413ffJPVWGFe//ImkayHc+X0pD9buwP3TQE2M+9CBhM5bRdiidSRdDebih2PITknDJbBHiXJ8UveO9vy1/j5HTscRcjeVn34LwcZKnyZ1rPKd5+iZOE6ci+feg3TCwtOYvzKM1DQVlb1zd2qAjynrdzwkKDiZ8Ih0lq6/T3JyNj7uxd/x5aXcfK2dDSs2PeTY2QRuh6Ux+c9QrK30aFQr/3Lz+LlETl5I5P7DDO49zGDR2oekpanw89IuN9MzVFrnoZS04p9/Huv5ijOLV93h0PFogm8nM+HXa1hXMKBpg4IvKhgZKhnzuR+TZlwnMenZ7bp60z3+WhPK5WvFv0gmyq9yUREAWLRoETY2Npw4cYKPP/6YDz74gJ49e9KoUSPOnDlDu3bteOutt0hJSQFApVJRsWJFVq9ezZUrVxg9ejRff/01q1atAtD8uF60aBEbN24kOzubfv360bZtW95+++0Ccxk3bhy9evXiwoULdOrUib59+xITEwNAXFwcrVq1ombNmpw6dYrt27fz8OFDevXKbfZPTk5m2LBhnDp1ij179qBUKunWrRsqlXbBMGLECD799FOuXr1Ky5YtmTp1Kubm5oSHhxMeHs7w4cPzzfHmzZusXbuWdevWce7cuSKt98SJEwDs3r2b8PBw1q3LOTkvXbqU0aNH8/3333P16lV++OEHRo0axaJFi4q6+wrlYKuPtZUeZy/nFkApqSquBSdrnZwKolRAiwZWGBoouXIzOc8YbzdjvNyM2b4/Ks/PC2JtrsDCRMn1u7lXT9My4M6DbNxLeOJ57FZ4NlU9dLEwyfnR5lVRB1srJUF3it8s62CjRwVLPc5dzd0GKakqgkJS8fM0KmDOXEoFNKtrgaG+kqvBKZrpBvoKvhhckVnL7hObkP8JuigsTcDMSMGtB7k/QtMz4V40ONuUvC9maS03MzOTkJvXqVajjmaaUqmkao3aBF27XKRlZKSnk52dhalZ7g8JlUrFjCkTeLXHm7i4uhc5n4JYmYKZsYKQ8NwyJD0T7kWqcbEt3X6thvqgUqtJK2ZDY1ZWBvdvX8YzoKFmmlKpxDOgIaE3z+U5T+jN81rxAN5VmmjiVSoVQecPYOPgxsKf32HikMbMHvcGV04X7Ud8abBsUIOovUe1pkXuOoRVgxoAKPT0sKgVQNSeI7kBajVRe49g2aDmc63b0c4Aayt9zlzMLTeTU7O5ejOJyj6mBcyZS6mAlg0r5JSb13OvEl++nkTLhtaYmeigeBSjp6fg3JXi/UgsN+WmrT4VLPU4eyV3G6SkqggKTsHPs+jnoOb1LTA0UHLtZorWZy0bWLFiRmVmTfAh8HUHDPRLdlw62RtiU8GAk+dyWyWTU7K5cj2BKn75V1gAhr3vzZFTMZw6H1eidYv/T+VmjED16tX59ttvARg5ciQ//vgjNjY2DB48GIDRo0cza9YsLly4QIMGDdDT02PcuNymXXd3d44ePcqqVas0P8pr1KjBhAkTeOedd3jzzTe5c+cOf//9d6G5BAYG0rt3bwB++OEHpk+fzokTJ+jQoQMzZ86kZs2a/PDDD5r4+fPn4+LiwvXr1/Hx8aFHD+2rQPPnz8fW1pYrV65QpUoVzfShQ4fSvXt3zXsLCwsUCgUODg6F5piRkcHixYuxtbXVTCtsvY9jra2ttdYxZswYpkyZosnF3d2dK1euMGfOHAYMGFBoLkVRwTKnX3psvHYXqNj4TKwefZYfNxdDpo/1Q19PSWpaNuN+DebuvbQ8Yzu0sObOvVSu3Mi7olAQs0cnm8QU7auniSlqzWclteZAOm+2MuS7d0zJzlajVsOKPWkE3y/+Cc3KIuewfvqHelxCFlYWBW9LV2cDpoz0yNmW6Som/H6X0PDcpvnBbzhyNTjlucYEPGb6qE6S/NSuSk5TY2pY9stNTIhHpcrGwlL7yqqlZQXuhRat6fyvBbOxqmBDtRq1NdM2rFmGUkeHTq+8XvRkCmFqlPP9S3rqb05Ky/2sNOgqoW0tHS7dUpNezN6KKYlxqFTZmFpoX6k3tbAmKvxWnvMkxUdhYm7zTHxifE5FPjkhmoy0FA7+PZc2PT6hfa/PuX7xEMtnfMLb/2PvrqOjOLsADv+ycXdPiCckuLu7FinuRT9KgaKlRUspxYpWKK7FXYsUKC7FCW4BEiDuG9l8fwQ2LEmIQBso9zlnz8nO3nnn7uxmdt55Zb5aikfh8nlLMh/07W1QPtW8sKB8GoquuSkKA310Lc1R6OigfBb2WkwYxn6eb7Vty7c4bnq4GjJ3YoD6uDluxi0evHLc/HbWbcYM8mbLojKkpKhITFIx7sdbPHnDmK2sfHDHzSjN42ZEdIr6tey4uxjw42gv9XFz4twHPHySsZ8OnYjkaVgS4ZEpeLga8FkbB1wc9Plu3oM852llmT4WISLytc88Mkn9WlbqVLPF18uE3kP+zvM23zdpMn3oO/XBVASKF88YTKatrY21tTXFimUMArW3twfg2bNn6mU//fQTixcv5uHDhyQkJJCUlJRpEOzQoUPZsmUL8+bNY/fu3Vhb59yc/GouxsbGmJmZqbd78eJF/vzzT0xMMl+NuXPnDr6+vty6dYuxY8dy6tQpQkND1VfkHz58qFERKFu2bKYycsvNzU2jEgDkeruviouL486dO/Ts2VNd6QJISUnB3Nw8y3WUSiVKpeaPhb6+Zn/J2pWtGNyzkPr56Gm3c//mXvPoiZJ+XwdibKhNtQoWDO/nztDvbmaqDOjpalG7shWrtuSub2YZPx3a1c44c5y/LZ+jI3Ohegld3By1+W1bPBExaXg5afNpLQOi4hK4GfTmH7WaFcwZ0CVjUOT4OXn/cXnpcUgSX3x7B2NDBVXKmDPkMxdGTr1HULCSCiVMKV7YmIHf3sm5oCwUddOiSbmMRsjfD/+3B6FtXreSY0cOMP6HOejppX//79y6wa6tG5g6ZyFaWvn/MSvmoUWzihlXU1cd/Of3pUIL2tRI3+aObMYU/NvSXnTa9S9dmyoNuwPg6OZP0K3znD649l+pCPyb6lSx5sve7urnX0+5mX1wDoKeJNJn5BWMjbSpXsGKkf09GTIhUF0Z6NHWBRNjbYZ9d52o6GSqlLNk7CBvBo8P5F5Q9sfCD+W4WauSBV90c1Y/Hzfzfr7zeBSs5POxtzA21KZqOXOG9nJlxA931JWB3YfD1bH3HyUSHpnMDyO9cLTVI/j5m5vW6tWwY/jnvurnI77N+wBzOxt9BvX25suxl0hK/o92dBf59sFUBHR1Na9uaGlpaSx7+aP68uR2zZo1DBs2jBkzZlCpUiVMTU2ZNm0ap06d0ijn2bNn3Lx5E21tbW7dukXDhg3zlcvL7cbGxtKsWTOmTJmSaT1HR0cAmjVrhpubGwsWLMDJyQmVSkXRokUzDeo1Ns5HJ9w3rJvb7b4qNja9mXTBggVUqFBB4zVt7aybdSdPnqzRGgPprQrQTP38xN+RXL+TcVX+5SBWS3NdwiMzrshYmuty54FmE+vrUlLT1Fepbt2Px8/TmJYN7Ji9WPOqbfUKlujrK9j3V3hWxWRy5W4KD0IyctTRTs/R1EiL6FeubpkaafH4ef77e+pqQ9PK+izakcC1++k/Xk9CVTjbKqhdWo+bb/jRhfQ+qjfuZZycq/elmY7G1S0LMx3u5lBWSmoawS9mFrr9IBFfd0M+qWvNvBVPKF7YGEdbPdbN8ddY5+v+hbh6K55R07K+qvvSzcdpPA7L+HF+OYjV2EDzSraxgRYhEfn/sYpNeDflmpqZo1BoZxoYHBkZjoXlmwd5bt34O5s3rGbspB9x9/BSLw+8epGoqAj6dW+jXqZSpbJ80c/s3LqBX5asy1VuN4LSeBya8dm+HARtYpDx/l8+f5t9+ZJCC9rW0MbCWIul+1Ly3BoAYGRqgUKhTWyU5pXx2KgwTMyz7ttsYm5DXHRopnjTF/FGphYotHWwdfLSiLF18uTBzX/nyqfyaSj69pr569vbkBwVgypRSVJoBKqUFPTtrF+LsUYZkrcuisfPRRB4O6Priq5u+gefftzM+FDyfNy8F4+flzGtGjkwc+F9HO31adnQns+GXebBo/Qv1N2HCRQrbMon9e2Zteh+tuV+KMfNk+ejuf5Kt8eM3yDN46almQ53HmbdwvyS5nEzAV8PQz55Meg4Ky+362ifc0Xg6Okwrt08q36u9/Izt9AlLCJjXUsLPW7fzXpaND9vE6ws9Vg0K6NlUkdbixJFzGnV1JnarY6gyv9H8a9TSV3mnfpgKgJ5dezYMSpXrkz//v3Vy14dGPvSZ599RrFixdRXvOvWrYu/v3+muNwqXbo0GzduxN3dHR2dzLs3LCyMGzdusGDBAqpVqwbA0aNHc1W2np4eqan5uxKXm+3q6aU3K766DXt7e5ycnLh79y6dOnXK1bZGjRrFkCFDNJbp6+vT9LMr6ucJiSoSEjVbDcIikilVxJQ7D9IP4EaGCgp7GbN9f+YZV95ES4ssZ09oWMOGE39HERWTu77tymRQRr16xEkjKk6Fr6s2j0PTj5r6euDmoM3Rt5iuTqGdflB+fUYCVVr6e8lJglJFwmvTgoZHJlPC35i7Qek/YIYGCvw8Ddl1KHeVoJe0tDJ+IDfsDuWPvzRPin/+1ocFa4M5fTHnrkJJKZD02u9UTEIaHg5aPH0xjaCeDjhbw7lb+T/SR8a9m3J1dXXx9Pbl8oVzlK+U/j+jUqm4fOFvGjVtme16WzasZtPaFYyeOB1vn8Iar9Wo3UBjzAHAd2OHUb1WfWrVa5zr3JJSyDQtaEx8Gp6OCkIiXnw3dcHZVoszN9/uF/5lJcDKVIulf6SQkLeeIWo6Ono4uRfh7rWTBJSpC6Tvz7vXTlKhbtbHFlfvEty5dpLKDTK6IN6+ehxX75LqMp09ihIaolkJDQ25j4VN3qcOzY/IkxewbVRdY5lNncpEnLwAQFpyMlF/X8WmdqWMaUi1tLCuVYkHP6/M07ayPm4mUbqomfrE38hQgb+3Cdv3PcuqiGwptLTUs+MY6KWfbKa9dsalUoFWDiMLP5jjZqKKhMTMx82SASbcfXHib2SgwM/LiJ1/Zj2rVXa0XtmXWfEqZPhiezn/FiUkpPI4QfN3PzRcSdkSlty+l17hMjLUJsDXjC27nmRZxtmLkXT5/IzGsq8H+/HgUQKrNjz8oCoB4t37z1YEfHx8WL58OXv37sXDw4MVK1Zw5swZ9cw4kN516MSJE1y6dAlXV1d27txJp06dOHnypPqkOK8+//xzFixYQIcOHRgxYgRWVlbcvn2bNWvWsHDhQiwtLbG2tua3337D0dGRhw8f8tVXX+WqbHd3d2JjYzlw4AAlSpTAyMgII6PcTeWWm+3a2dlhaGjInj17cHFxwcDAAHNzcyZMmMDAgQMxNzenYcOGKJVKzp49S0RERKYTfkg/6X+9K1BubN7zlI4tHHkcoiT4uZLunzoTFpnMsXOR6pipo3w4djaSrfvSKweftXPizMVonoUmYWiooHZlK0r4mzJqyi2Nsp3s9SlW2IRv3qILEsDh88nUL6/P80gVYdFpNK6kR1RcGpdfmdni81aGXLqdwl+X0n/k9HTB1jzj19PaXAtnGwXxyjQiYtJQJsGtRyl8UlWf5BQl4TEqvJ21Keevy5Yj+Tvr2ro/jPZN7HjyNImQ0CS6tLAnPDKFE+czBvpNGurOib+j2fFneuWgWyt7zl6O4Xl4MoYGCmpWsKCYnzFjZt0H0vvKZjVA+HlYMk9D8/eDfvqGiqpFFITHqIiMTaNmcQUxCXD9lRmBOtdScP1RGmdfnMTr6oDVKz3vLEzA3gISkiA6Pvfl5kazlm2Z9+NkvHz88Pb1Z+fW9SgTE9Qn7XNmTMLa2oZO3fsCsHn9KtauXMzgEWOwtXMgIjz9BMLA0BBDQyNMzcwxNdPsUqetrYOFpRXOLoV4GycDVVQvpiAsOo2I2DRql9QmJh6uP8x4z93qaRP4MI3TN9J/+fV0wMo0owxLEy0cLNNISIKouPRKQLua2jhaabHqYAoKLdTjLBKSIDWPJxBVGnZj44JROHkUxcWzGMf3LidJmUCZaukVqw3zR2JmaU/9tunHlcr1u7JwcleO7l6CX4kaXDq1iyf3rtKiR0aLY7VGn7H256G4+5XF078Cty4d5caFQ3w2Kn+TGWgbG2HsnfFZGHm4YFaiMEnhUSQGBeP33RAMnO252GMkAA9+W4Nb/04UnjycoKUbsalVEcc2jTjTvK+6jHuzllBi8RQiz10h6swl3Ad2Q8fYkKBlWc+WlBebdj+lU0snHoUkEvJMSY+2LoRGJHH0bEalfdpoP46eiWDr3vTKQc/2Lpy+EMWzMCVGBtrUrmJNiQBTvpqcfiL58Ekij4IT+bK3O7+uDCI6NoWqZS0pU8yMb6bmvTvSh3Lc3PJHKO2b2fE4JImnoUl0aWVPWEQyx//OOG5OHuHB8XPRbD+Q/r/d/VMHzl6K4Vl4EkYG2tSsaEHxwsaMnpG+rx1t9ahZyYIzF2OIjkvBw8WQvh0duXw9lvuP3tzSkJ312x7TrV0hgp4kEPw0kV6d3QkLV/LXyYwWplnfFefIiVA27XxCQkIq9x5qthAlJqqIjk7WWG5loYuVpR7OTukVFU83E+ITUnj6XJnlLEPiv+E/WxHo27cv58+fp127dmhpadGhQwf69++vnmL0+vXrDB8+nEWLFuHq6grAzz//TPHixRkzZkyWXXtyw8nJiWPHjjFy5Ejq16+PUqnEzc2Nhg0bolAo0NLSYs2aNQwcOJCiRYvi5+fHnDlzNKbqzE7lypXp168f7dq1IywsjHHjxmU7hejrFApFjtvV0dFhzpw5fPvtt4wdO5Zq1apx6NAhevXqhZGREdOmTWP48OEYGxtTrFgxBg8enK99lJ21O55ioK9gcE83TIy0uXIzllFTbpH8Sp9GR3t9zEwzvrYWZrqM6OeOlYUucfGp3AtKYNSUW/x9RfNyacMa1oSGJ3Pu8ttNi3bgXBJ6utCujgGG+lrcfZLKr1viSXnlgo21uQLjVwZoFrLT5otPMypsLaunn0mdupbM6n3pPwTLdifSrIo+XRoaYGSgRUS0ip3Hlfm+Mc6GPaEY6Cv4oqvTixvjxDNm1n2SU17Zl7Z6mvvSVIehPV2wMtchLkHF/UeJjJl1nwvX8j6wOreOB6ahq5NGk3IZN8FafShV4wTT0kQLI33gxZ0LnKy06Fono1ta/dLpf1+8q2LbKVWuy82NKtXrEB0VyZqVi4mMCMfd05tvvp2u7hoU+vwpilcuP/6xayspKclM/36sRjltOnanXac3z0b2to5eVaGrA80qaae/52dprNyfQsqr+9JUCyODjO+Ak7UWPRpkfAcaltMGtDl/W8WW46mYGUFh1/STsf7NNLtELtmbwv2neatYFavQmLjoCA5smkNsVCiOhfzpNuw3ddegyPBgtBQZJ3+FfErRtt809m+czb4NM7G2d6PjoLnYu2T0mQ4oW4/m3cdxZMdv7Fz5PTaOHnT4YjbuvmUybT83zMsUpdKBFRnlT/8agKDlm7jUcxT6jrYYujqqX0+4/4gzzfsSMGMU7l90JfFRCJf7jiZ0X0aLa/D63ejZWuE7bmD6DcUuBnK6aS+SnuXtSnNW1mwLxkBfwZDe7pgY6XD5Rgyjfripcdx0sjfA3DTj87M01+Wrzz3Vx827D+P5avIN9fExNTWNr6fcoFcHVyYN98XAQMGTp0qm/HJX46ZjufWhHDfX73qOgb6CgT2cMTHS5urNOMbMuKf5G2T3+m+QDsP6uKqPm/eCEhg94x7nr6Y3fyanplEqwIQW9W0w0FfwPCyZo2ejWLMtby02r1q1MQgDA21GDPDFxFiHy9eiGDruskb/f2cHQyzM3jxg/HUtGjnxWUd39fOfp5QEYNKs6+w+8DTf+b5r/9U7/BYUrbS0/+otEsT7pl6nczkHFbB9q8owaPbbz4jzT5o9yJQmva7kHFjAdi4sysTf3/+rSGM66HD59vvzI5edYt72jFue/64U/4YJXXVZf/L972fQpqKCnbp+BZ1Gjpok36BO+9MFnUaODqwp/0EcNxt1v1TQaeRo99LiVG12uKDTyNHR7TVyDvqH7D5fcMfBRqXyVrn6EPxnWwSEEEIIIcR/i1y+frc+mBuKCSGEEEIIId4dqQgIIYQQQgjxEZKuQUIIIYQQ4oOgkjsLv1PSIiCEEEIIIcRHSFoEhBBCCCHEB0EGC79b0iIghBBCCCHER0haBIQQQgghxAdBbij2bkmLgBBCCCGEEB8hqQgIIYQQQgjxEZKuQUIIIYQQ4oOgksHC75S0CAghhBBCCPERkhYBIYQQQgjxQZDpQ98taREQQgghhBDiIyQVASGEEEIIIT5C0jVICCGEEEJ8ENKQ+wi8S9IiIIQQQgghxEdIWgSEEEIIIcQHQaYPfbekRUAIIYQQQoiPkFQEhBBCCCGE+AhJ1yAhhBBCCPFBkPsIvFtaaWmyS4UQQgghxPtv/UlVgW27TcX/XkcaaREQ/5qT16MKOoUcVSxszpkbkQWdxhuV87Mg4uLhgk4jR5YlavDn5YSCTiNHtYoZMm/X+389ZEBjLdadKLgfwNxoW0nB6qPv/77sWFWLOu1PF3QaOTqwpjw7df0KOo0cNUm+wfOrpwo6jTeyLVKB8Et/FXQaObIqXo34I+sKOo0cGVVvW2DblsvX79Z/r2ojhBBCCCGEyJG0CAghhBBCiA+CKk1uKPYuSYuAEEIIIYQQHyGpCAghhBBCCPERkq5BQgghhBDigyCDhd8taREQQgghhBDiIyQtAkIIIYQQ4oMgLQLvlrQICCGEEEII8RGSioAQQgghhBAfIekaJIQQQgghPggq6Rr0TkmLgBBCCCGEEB8haREQQgghhBAfhDS5s/A7JS0CQgghhBBCfISkIiCEEEIIIcRHSLoGCSGEEEKID4LcR+DdkhYBIYQQQgghPkLSIiCEEEIIIT4IMn3ouyUtArmkpaXFli1bCjqNt+bu7s6sWbMKOg0hhBBCCFHApEUgl4KDg7G0tCzoNN7amTNnMDY2Lug0Mtm/cz27t6wkKiIMV3cfOvcZhpdvkSxjHz28w+bVv3H/znVCnwXTseeXNGjeQSMmIT6OTavnc+7kIaKjInDz8KVT76F4+gS8VZ77dq5n5+ZVREWEUcjDh659hr4hz7tsXDWfe3duEPosmM49B9Pwk8x5blg1n7MnDxMdFYG7py+dew/B6y3z3LDnT1Zu/4PwyCi83VwY+lkHinh75Pz+jp1mzOyFVC9bgqkjPlcvT0tLY8G6bWw98BexcQkUK+zFiF6dKORo/1Z5Htq9hj+2LSM6MgwXN1/a9RyJh0+xLGOfBN1m+5pfeHD3GuHPg2nTfRh1mnbOtuw9mxezZdUcajfpSNseI94qz7S0NE7tmcvVE+tRJkbj6F6aWm3GYWHr/sb1Lh1dxd8HFxEfE4qNU2GqtxqNg1tx9esH140l6OYJ4qKfoatnhKNHKSo3HYaVvWeeczy1fxVHdy8mNioUh0KFadL5G1w8i2cbf+X0Hg5smkNk6GOsHNxo0GYoviVqqF/ftGAU549t0VjHu2hVug1bkOfcXnX64CqO71mUnqdrYRp1HI3zG/K8emYPf26ZTWToY6zt3aj76TB8itfIMnbH8nGcO7yWBu1HUbFet7fKE6B7G2ca17bFxFiHKzdimL3oPo9DlNnGN6tnR/O6dtjb6gPw4FECKzY95vSFKHWMpbkufTu7UqaYGYYG2jwKTmTV5if8dToiT7lZVS2L59CemJcuioGTHWdb9+fptgNvXqd6eQKmf4VJgA+JQcHcnvwLj5Zv1ohx+19HPIf0RN/BluhL17k6eCJRZy7nKbfXbdy9n9+37CI8Mgovd1e+7NWFAB+vLGMPnzzD8o3beRz8jJTUFFwcHWjfvBENa1ZRx1Rt1TXLdft3bUfHFk3yneeGPQdZtW3vi+OmK0M+60ARn6z/Fw+dOseyTbt4FPKMlNRUXB3s6dCsPo1qVFLHhEdG8dPKjZy+dJWYuARK+vswtGdHXN/iuLn2z1Ms23uUsKhYfF0dGNmhCUU9XHJcb8/pS4xasJ6aJQsz8/NO6uXxiUrmbNrHn+cDiYqLx8nGkg61K9KmZvl85/hPkzEC75a0CADJyck5xjg4OKCvr/8vZPPPsrW1xcjIqKDT0HDqr338vngWn7TrxYQfl+Pq4cP08QOJjgzPMj5JqcTW3pk2XT7H3NI6y5jF8yZx5cIp+nw5nklzVlO0VAWmjv2c8LBn+c7z5F/7WLVoNi3b9+S7mcso5O7NlHGDiMomT6UyEVsHZ9p17Z9tngvnfc+VC6f535fjmTxnFUVLVuCHMQPeKs99x88we/l6en3alGVTRuPj5srgSbMJj4p+43pPnoUyZ8UGSvr7ZHptxda9rNt9kJG9O7Pw+1EY6uszeNJslEk5/+9k5+yxvWxYNoOmbfry9dTfcXH3Ze53/YmOyu5zT8TG3pmWnQZhZmHzxrLv377CX/s24Ozmm+/8XvX3wYVcPLKCWm3G03bwOnT1Ddn6ay9SkrM/Kbx5fhd/bfmB8g0+p/3QTdg4+bFtfi/iY8LUMXYuRajb4Xs6f7WTT/ouhLQ0tv7aE5UqNU/5XT61i91rplCrxef8b8JGHFz9WDa9N7HRYVnGP7x1nvW/DqNM9db879tN+Jeqw+o5X/D00U2NOJ9i1Rgx64j60fZ/0/OU1+uunN7FH2t/oEbzz+k7bhP2rn6snNmLuGzyDLr9Nxt/G0qpap/Sd9xm/ErVZc28ATx7LU+AwL/38ejuRUwt7N4qx5faN3ekZUN7Zi28z4DRV0lUqvhhlB+6utnPYR4alsSC34P439dX6P/NVc5fjebbYT64uRiqY7763BNXRwNGT7tF7xFX+Ot0BGMGe+PtnrfjsraxEdGXbnBl4IRcxRu6u1Bu23zCDp3iaNlPuDd3GcXmf4dNvarqGMc2jfCfNopb3/3E0fItibl0nQo7F6Fna5Wn3F514OhJ5i1ZTY+2LVg0/Vu83Qsx5NtpRERmfTwyNTGha+vm/PrDGJbNnETj2tWYPG8Bp85fUsdsXTRH4zHq815oaWlRo2K5fOe5/9hp5ixbR882zVg6ZSw+bq58OWlWtsdNMxNjurVqwoJJo1gxfTxNalVh0s9LOHnhCpB+8WDk1J948uw5U0YMYNnUsTjYWjPw2xkkJGZ/3HiTvWcuM2Pdbvo2q8XqMf/D18WB/rOWER4d+8b1noRGMHP9Xkr5uGV6bca6PRy/cotJvT5l07cD6VS3ElN+38mhC4H5ylFk9tNPP+Hu7o6BgQEVKlTg9OnTb4xfv349hQsXxsDAgGLFirFr165/NL/3riKgUqmYPHkyHh4eGBoaUqJECTZs2ACk/2PVrVuXBg0akPaiShgeHo6Liwtjx45Vl7Fw4UL8/f0xMDCgcOHC/Pzzz+rX7t+/j5aWFmvXrqVGjRoYGBiwatUqABYvXkyRIkXQ19fH0dGRAQMGqNd7tWtQUlISAwYMwNHREQMDA9zc3Jg8ebI6NjIykl69emFra4uZmRm1a9fm4sWLb3zfI0eOxNfXFyMjIzw9PRkzZoxGBWX8+PGULFmSFStW4O7ujrm5Oe3btycmJkYdExMTQ6dOnTA2NsbR0ZGZM2dSs2ZNBg8erI55vWuQlpYWCxcupGXLlhgZGeHj48O2bdvUr6emptKzZ0/15+Hn58fs2bPf+F7yas/W1dSo34LqdZvhXMiT7v/7Cj19A47s355lvKdPAO17DKRi9fro6uplej1JmcjZE3/SrvsXFC5SGntHV1p26IOdoysHd2/Md567t/5OrfqfUONFnj36f4W+vgGHs8nTyyeAjj0GUukNeZ45/iftuw+gcNFSODi50rpjb+wdXTiwe1O+8/x9xz4+qVOVprWq4OHixMjenTDQ02PHn8eyXSdVpWLc3EX0btscJzvNk+y0tDTW7tpPj1ZNqF6uJD5uLowb0IPQiEiOnDmf7zz3b19BlbqtqFy7BU6uXnTsMxpdfQOOH9ySZby7d1Fadx1CuaoN0dHVzbbcxIR4Fs/+ms79xmJkbJrv/F5KS0vjwuHllKvfD89idbBx8qNexynERT/j7uX92a534dBSilRqQ0CF1lg5eFOrzQR09Ay4dirjO1i0cjucvcphZuWCnWsRKjYeTGxkMDHhj/OU4/G9yyhbow2lq7XCztmbZt3Go6tnwN9Hsv4endi3HO9iVanauCd2Tl7UbT0IRzd/Tu1frRGnraOHqYWt+mFobJ6nvF538o+llK7ehlJVW2Pr5E3TLhPQ1TPg/NGs/y9P7V+Bd9GqVGnYE1snL2q3HISjWwCnD67SiIuOeMru1d/Rqvc0FNrvpqG7VSN7Vm5+wvFzkdx9mMCUn+5iY6lH1bLZtwyf+DuS0xeieByi5FFwIovXPiIhUUWAT0YrbBFfEzbvfcqNO3EEP1OyavMT4uJS8fXIW0vt871HuDluFk+3Zv8dfJVbn/Yk3HtE4IgpxF6/y4OfVxGycS8eg7qrYzwG9yBo0ToeLdtEbOAdLvcfR2p8Iq7dW+cpt1et2b6HZvVq0qROdTxcnRnetzsG+vrsOHg4y/jSRf2pUbEs7i7OODvY07ZpA7zcXLkUmFH5s7a00HgcPfM3pYv64+yQ/0rg7zv20bxONZrWqoqHqxMj+nRGX0+PHQePZp1nkcLUrFAadxcnXBzsaNekLl5uLly8fhuAoOCnXLl1l+G9OxPg7YGbswMjendGmZTMvmOn8pXjyn3HaVWtLJ9UKY2Xkx3fdG6GgZ4uW479ne06qSoVXy/cQL/mtXGxyVyhu3jnIU0rl6SsnwdONpa0rl4OXxcHrt7L2zFIZG3t2rUMGTKEcePG8ffff1OiRAkaNGjAs2dZX+w7fvw4HTp0oGfPnpw/f54WLVrQokULrly58o/l+N5VBCZPnszy5cv59ddfuXr1Kl9++SWdO3fm8OHDaGlpsWzZMs6cOcOcOXMA6NevH87OzuqKwKpVqxg7diyTJk0iMDCQ77//njFjxrBs2TKN7Xz11VcMGjSIwMBAGjRowC+//MLnn39Onz59uHz5Mtu2bcPb2zvLHOfMmcO2bdtYt24dN27cYNWqVbi7u6tfb9OmDc+ePWP37t2cO3eO0qVLU6dOHcLDs77SCWBqasrSpUu5du0as2fPZsGCBcycOVMj5s6dO2zZsoUdO3awY8cODh8+zA8//KB+fciQIRw7doxt27axb98+/vrrL/7+O/sDxEsTJkygbdu2XLp0icaNG9OpUyd1riqVChcXF9avX8+1a9cYO3YsX3/9NevWrcux3NxISU7m/p3rFCmRcSVHoVBQpEQ5bt/IX3N0amoqKlVqppNvPT19bgW+uUL2pjzv3b5OkZIZzaXqPK+/ZZ56mi1Nenr63LiWvzyTU1K4cfch5Yr5a+RZrpg/l2/ezXa9xRt2YGVmSvPaVTO99uRZKGGR0ZQrnlGmiZERRbw93ljmm6QkJ/PwbiD+xSto5OlfrAJ3b1x6w5o5W7Pwe4qWroZ/8YpvVc5L0WGPiI95jqtvZfUyfUNT7N2KE3L/QpbrpKYk8ezRVY11tBQKXH0qEfIg63WSlfEEntqEmZULJhYOuc4vJSWJJ/ev4hmQ0SVBoVDgVaQSQXey3lbQ7Yt4vRIP4F2sKg9fi79//TQ/fFGFWV81Ytuy8cTH5q37yqtSU5J48uAqnv6a+8QzoBKPssvzzgU8AyprLPMqUkUjPk2lYvPCEVRu0BM758ytWfnhaKePtaUef1/OuBocl5BK4O1YAnxNclWGQgtqVbLCQF/BtZsZV2yv3oylViVrTI210XoRo6urxYVrb26xe1sWFUsSevCExrLn+45iWbEkAFq6upiXLkLogeMZAWlphB48jkXFUvnaZnJyCjfv3Kds8YzukwqFgrLFA7h643aO66elpXH20lUePgmmZEDhLGPCI6M4fu4iTepUz1eOL/O8cfcB5YpndMlUKBSUK+7PlVwc49LS0jhzOZCHT0Io9aJFNSk5BQC9Vy5aKBQKdHV1uBiY83vPlGNKCoEPnlDBP6OrkkKhoIK/F5fuBGW73m/b/8TK1JiW1cpk+XoJr0IcvnCDZxHR6e/j+l0ePA2lYpGsz3/eB2lpBffIqx9//JHevXvTo0cPAgIC+PXXXzEyMmLx4sVZxs+ePZuGDRsyfPhw/P39mThxIqVLl2bevHlvudey916NEVAqlXz//ffs37+fSpXSf6Q8PT05evQo8+fPp0aNGjg7OzN//ny6du1KSEgIu3bt4vz58+jopL+VcePGMWPGDFq1agWAh4cH165dY/78+XTrltFndPDgweoYgO+++46hQ4cyaNAg9bJy5bJuZnz48CE+Pj5UrVoVLS0t3NwymtuOHj3K6dOnefbsmbor0fTp09myZQsbNmygT58+WZY5evRo9d/u7u4MGzaMNWvWMGJERt9mlUrF0qVLMTVNv8rZpUsXDhw4wKRJk4iJiWHZsmWsXr2aOnXqALBkyRKcnJxy2u10796dDh3S+65///33zJkzh9OnT9OwYUN0dXWZMCGj6dnDw4MTJ06wbt062rZtm2PZOYmJjkSlSsXcQvNKhbmFFcGPHuSrTEMjY7z9irFt3WKcXDwwt7DixF9/cPvGZewdcu5Lmec8H+c/T5/CxdiydjHOLu6YW1hx/Mgf3LpxBXvH/OUZGR1LqkqFlYWZxnJLC1PuPwnOcp0L12+x7eBRVkwdk+XrYS+a8K3MNa+uW5mbqV/Lq9iYCFSqVMzMNbtMmVpYE/L4fr7KBDhzdA8P711n1A+rcg7OpfiY5wAYmWjmamRiQ1xMaJbrJMRFkKZKxcj0tXVMbYh4dk9j2aWjqzm+fTrJSfFY2HnQ4n+L0dbJ3IKUfX7p302T1/aliZk1ocH3slwnNioUE3ObTPGxURnvx7tYVfzL1sPSxoXwZw/Zv3EWy2f0pc+Y31EotHOdX0ae6fvE2EwzT2Mzmzfm+Xq8iZkNsdEZeR7dvQCFQpsKdbvkOafsWFqkn7xFRGl2fYuISla/lh0PV0PmTgxAT1dBQmIq42bc4sHjRPXr3866zZhB3mxZVIaUFBWJSSrG/XiLJ0/z110kt/TtbVA+1fy+Kp+GomtuisJAH11LcxQ6Oiifhb0WE4axX97HrABExcRkeTyysjDnweOsj0cAsXHxtOw9iKTkFLQVCob06Uq5kkWzjN3951GMDA2oUbFsvnIEiIx5cdw0fy1PczMePA55Y57N+w4nKSUFbYUWw3p1pnyJ9EqPu7MDDjZW/LJ6EyP7dMFQX581O/fxLCyCsMiobMvMTkRsfHqOZpoVUWszE+6HZH0cOn/rAVuO/s2asf2zLXdkhyZMXLGVBiOmoaOtQEtLizFdPqGMr3uecxSakpKSOHfuHKNGjVIvUygU1K1blxMnTmS5zokTJxgyZIjGsgYNGvyjk9W8VxWB27dvEx8fT7169TSWJyUlUapUxhWJNm3asHnzZn744Qd++eUXfHzSa+BxcXHcuXOHnj170rt3b3V8SkoK5uaaTdply2YcNJ49e8aTJ0/UJ9A56d69O/Xq1cPPz4+GDRvStGlT6tevD8DFixeJjY3F2lrzxyshIYE7d+5kW+batWuZM2cOd+7cITY2lpSUFMzMNA9K7u7u6koAgKOjo7p56e7duyQnJ1O+fMYVa3Nzc/z8/HJ8P8WLZwzUMzY2xszMTKPZ6qeffmLx4sU8fPiQhIQEkpKSKFmyZLblKZVKlErNH7V/e3xFny8nsGjuRAZ/1gSFQhs3Lz8qVqvP/TvX/9U8ctLvy/EsmPMdX/RoikKhjbuXH5X+xTzjEhKZMHcxo/p2wcLs7bvRFKTw0BDWLZnKoDG/ZmplyYsb57bz57px6ufNev/6LtLLll+ZZhTyq0xc9HPO/7mY3csG8+nA39HRLdgxScUrZgy6dHD1xcHVj5kj6nPv+ulMrQkF5cn9K5zav4K+YzeipZV93/2c1KlizZe93dXPv56SeQxCbgU9SaTPyCsYG2lTvYIVI/t7MmRCoLoy0KOtCybG2gz77jpR0clUKWfJ2EHeDB4fyL2ghHxv97/EyNCAJTO+IyExkbOXrjFvye842dtRuqh/ptidB49Qv1ol9PVyX3l+l3kumzaWhEQlZ68EMmfZWpztbShdpDA6OjpMHtaf739ZRoMeg9BWKChbzJ9KpYr+K4Nd4xKVjF60gTFdP8HSNPtuZ2sOnuTy3SBmDeiEo7UFf9+8zw+rd2BrYUbFgKwHdBe0gpw+NLvzm6zOcUJDQ0lNTcXeXnNwuL29PdevZ/0bHxISkmV8SEj2FdK39V5VBGJj05tPd+7cibOzs8Zrr+7k+Ph4zp07h7a2Nrdu3cq0/oIFC6hQoYLG+tramlewXp05x9DQkLwoXbo09+7dY/fu3ezfv5+2bdtSt25dNmzYQGxsLI6Ojhw6dCjTehYWFlmWd+LECTp16sSECRNo0KAB5ubmrFmzhhkzZmjE6b7WL1pLSwuVSpWn3LPypnLXrFnDsGHDmDFjBpUqVcLU1JRp06Zx6lT2fRwnT56s0YoA6S01Ddt/mSnW1MwChUI704DbqMjwbAfY5oa9owtffz8fZWICCfFxWFjZ8NPUr7Gzd8555Sy8MU+L/A+ks3d0YfTkX0l8kaellQ1zp36DrUPOLTlZsTAzQVuhIPy1K/URkTFYW2Tu3/346XOCn4cxfMpP6mWqF79SVdr3Y+2sb7F+cTUvPCoGG0sLdVx4VDQ+7q75ytPE1BKFQpvoKM2rjzGRYTkOBM7Ow7vXiIkK5/sRGTMzqVSp3A78m0O71zLv99MotHO+ku1RpBb2wzIqx6kpSQDEx4ZhbJ7RBzk+NhRbp8wnJgCGxpZoKbQ1BgYDxMeEYmSm+f70DU3RNzTFwtYdB7cS/PZNBe5e3odv6aY5v2nAyDT9uxn72r6MjQ7LdNX/JRNzG42r/znFA1jZuWJkakn404f5qggYmabvk9cHBsdFZ26deDXP1+Njo0MxebEPH946R1xMGDNH1Fa/nqZK5Y+1Uzi5bxmDpx7MVW7Hz0UQeDuj+46ubnqvWUtzXcIjM1oFLM11ufMg/o1lpaSmqa/u37oXj5+XMa0aOTBz4X0c7fVp2dCez4Zd5sGj9JP+uw8TKFbYlE/q2zNr0f1c5Zsfyqeh6Nu/9t2ztyE5KgZVopKk0AhUKSno21m/FmONMpsrzjkxNzXN8ngUHhmV5fHoJYVCgcuLmXV8PNx48OgJKzdtz1QRuHjtBg8fBzNhyOdZFZNrFqYvjpuvDQwOj4rOMc+XMwD5ehTi/qNglm/eTeki6d2YCnu5s3z6OGLj4klOScXS3JSeoyZR2Ms9zzlamhil5/jawOCw6FiszTJ3V3v0LJwnYZEMnpfROvry2F627zg2TxyErYUpczfv58f+HahWPP2ioa+LAzeCQljxx9H3tiJQkLI7vxk/fnzBJPQOvFcVgYCAAPT19Xn48CE1amQ9PRzA0KFDUSgU7N69m8aNG9OkSRNq166Nvb09Tk5O3L17l06dOmW7/utMTU1xd3fnwIED1KpVK1frmJmZ0a5dO9q1a8enn35Kw4YNCQ8Pp3Tp0oSEhKCjo6MxbuBNjh8/jpubG99884162YMHeetu4unpia6uLmfOnKFQoUIAREVFcfPmTapXz3/fyWPHjlG5cmX6989oWnxTywbAqFGjMjVt6evrc/5eYqZYHV1d3L0Kc+3SGcpUrAmkd4G6duksdRu3yXfe6u0aGKJvYEhcbDRXLpykbbcv8lWOjq4uHt6FuXrxDGUr1lDnefXSGeo1efs8DQwMMXiR5+XzJ2nfbUDOK2VBV0cHP89CnLlynRrlS6nzPHMlkDYNM3+33ZwcWDV9nMay+Wu2EJ+o5Mvu7bC3sUJHWxtrCzPOXA7E98WJf1x8Aldv36NV/ez/T99ER1eXQp7+XL98mpLla6vzvH75NDUbtc9XmYWLVWDMjxs0li3/aSwOzh7Ub9EjV5UAAD0DE/QMMn5Y09LSMDK1JejmCWyd009EkhJjefrgEsUqd8iyDG0dPexcivDo5gm8itVNL0elIujWSYpXzeHYlJamrnzkho6OHk7uRbh77SQBZdK3pVKpuHvtJBXqZL0tV+8S3L12ksoNMrpL3rl6nEJeJbPdTlR4CAmxkZhY2OY6t1dp6+jh5FaEu4EnKFw6Y5/cDTxJ+drZ5OlVknuBJzSmAr177TguL/IsXqk5nv6alZKVM3tRvNInlKzaMte5JSSqMs3kEhaRROmiZuoTfyNDBf7eJmzfl7cZvRRaWuqZhgz00isYaa9d0lSpQOsfHrEXefICto00fwts6lQm4uSF9JySk4n6+yo2tStlTEOqpYV1rUo8+Hllvrapq6uDr5c75y5dpXqF9D7qKpWKc5eu0apx3VyXo0pTqfvcv2rHgcP4ebnj41EoX/m9mqefpxtnLwdqHDfPXr7Op1kcN7PPM42kLGYhNDFOnxEqKPgp1+/cp0/7FnnPUUcHfzcnTgXepVapAHWOpwPv0q52hUzx7o42rB+v+Tvy05b9xCcmMbx9YxyszFAmp5CSmpqpNU1boaWuNAhN2Z3fZMXGxgZtbW2ePn2qsfzp06c4OGQ9DszBwSFP8e/Ce1URMDU1ZdiwYXz55ZeoVCqqVq1KVFQUx44dw8zMjG7durFz504WL17MiRMnKF26NMOHD6dbt25cunQJS0tLJkyYwMCBAzE3N6dhw4YolUrOnj1LREREpg/vVePHj6dfv37Y2dnRqFEjYmJiOHbsGF98kfnE8ccff8TR0ZFSpUqhUChYv349Dg4OWFhYULduXSpVqkSLFi2YOnUqvr6+PHnyhJ07d9KyZUuNLkkv+fj48PDhQ9asWUO5cuXYuXMnmzdvzhSX077r1q0bw4cPx8rKCjs7O8aNG4dCoXirJnMfHx+WL1/O3r178fDwYMWKFZw5cwYPj+znpM+umQwyVwQAGn7SkQWzJ+Dh7Y+nTxH2bl+DMjGBanXTr4jOnzkOS2s72nZNv+qTkpzM46B76r8jwp7z4O5NDAwNsXdMP1G9/PcJ0gBH50I8DX7E2qVzcHR2p1qdZvneF40+6cD8Wd/i4e2Pl28Ae7atQZmYSI066Xn+OnM8lla2tOuWRZ4pyYSHp+epb2CIg1N6npf+PklaWhqOzm48DQ7i96VzcXR2o3rd/OfZoWk9Jv60BH9PNwK8PVi7az+JyiSavJiHe8K8xdhaWdC/Yyv09XTxKqTZSvLyR+vV5e0a12Xppl24OtrhZGfDb2u2YmNpQfVy+RtECFC3WReWzhuDm1cA7t5FObhzFUnKBCrX+gSAJXNGY2FtR8tOA4H0/Rn8KL0SmpqSQmT4M4LuXUffwAg7x0IYGBrjXEhzgJueviHGpuaZlueFlpYWJWt05ey+X7GwdcfMypmTu+dgbGaHZ7GMk5nNP3fHs1hdSlRLv7dByZrd2b/6K+xci2LvVpwLh5eRkpRAQIX0sUlRoUHcurCLQn5VMDSxIjYyhHMHFqCjq4+bf94qWJUbdGPTglE4exTF2bMYJ/5YTpIygdLV0k+GN/w2EjNLe+q3ST8GVqrXlUU/dOXY7iX4lqjB5VO7eHLvKp90T7/SpUyM488tP1OkbD1MzG0Jf/6QP9ZOx8quED5FMw8oz62K9buzZdFXOLkXxdmjOCf3LyNZmUDJKun7ZPPCkZha2lG39VAAKtTtwtKpXTm+dzG+xWty5fROnty/SrOu3wJgZGKJkYnmLD4KbR1MzG2wcchfv/aXNu1+SqeWTjwKSSTkmZIebV0IjUji6NmMAdPTRvtx9EwEW/emVw56tnfh9IUonoUpMTLQpnYVa0oEmPLV5CcAPHySyKPgRL7s7c6vK4OIjk2hallLyhQz45upeeuOpG1shLF3xgmwkYcLZiUKkxQeRWJQMH7fDcHA2Z6LPUYC8OC3Nbj170ThycMJWroRm1oVcWzTiDPN+6rLuDdrCSUWTyHy3BWizlzCfWA3dIwNCVqW/1nM2jdryKS5Cyjs7YG/jyfrtv9BglJJk9rplZKJs+dja21Jv87pY85WbNxOYS8PnBzsSE5J5sS5S+w9fJxhfTTvCxEXn8Cfx08zoHvHfOf2qvTj5mIKe7lRxNuDNTv3k6hU0rTWi+Pm3EXpx81O6TMoLdu8C39PN5wd7EhOTub4+cvsOXKSEb0zKrUHTpzF0swEextr7jx8xMwla6hevhQVSmR975mcdK5XmbGLNxHg7kxRD2dW7z9BQlISn1QpDcDoRRuwszRjYKv66Ovq4u2s2cXE9EXvh5fLdXV0KOPrzqwNezHQ08XRyoJzN++x48QFhrRtlK8c/w0FWUfJ/vwmMz09PcqUKcOBAwdo0aIFkF55O3DggMaslK+qVKkSBw4c0Jjtcd++fepxs/+E96oiADBx4kRsbW2ZPHkyd+/excLCgtKlS/P111/z/Plzevbsyfjx4yldOv2LP2HCBP744w/69evH2rVr6dWrF0ZGRkybNo3hw4djbGxMsWLFNHZqVrp160ZiYiIzZ85k2LBh2NjY8Omnn2YZa2pqytSpU7l16xba2tqUK1eOXbt2oVCkX9LZtWsX33zzDT169OD58+c4ODhQvXr1TP2+XmrevDlffvklAwYMQKlU0qRJE8aMGZPnpqYff/yRfv360bRpU8zMzBgxYgRBQUEYGBjkqZxX9e3bl/Pnz9OuXTu0tLTo0KED/fv3Z/fu3fku83UVqtUjOjqCTat/e3GjLl+GjZuNuUV6E3V46FP1vgWICH/O2C8zbiS1e8tKdm9ZSeGipRk1Kb0/d3x8LOtX/ExE6DOMTc0oW6k2n3b+n3pQeX5UrFaP6KhINr7I083TlxHjZ6m7MIU+f4qWlmae3wzOGMC4a/Mqdm1eReGipRn9/S/qPNct/5nwF3mWr1SLNl3eLs96lcsRGR3DgnXbCIuMxsfdhZlfD1R38QkJDc9z5bDLJw1IVCr5Yf5KYuPjKV7Ym1lfD0Jf780DJ9+kbJUGxERHsH3NL0RHhuLi7scX3/yMmfpzD0ZLkZFnZMQzJg3PaC3Yt205+7YtxyegDEO/XZTvPHKjdO1eJCcl8Oe6sSgTonH0KEPzvgs0+vFHhT4kMS7jJNG3VGMSYsM5tWcucdHPsXX2p3nfBRiZpnfP0NbV48ndc1w4vBxlQjRGptY4eZbl00G/ZxpknJNiFRoTFxPBgc1ziI0KxbGQP12H/qbuchMVFozile9mIZ9StOk7jf2bZrNv40ys7d3oOHAu9i7p911QKLR5+ugGF45tITE+BlMLW7yLVqFOq4HoZDEVbm4VLd+Y+JhwDm2ZS2z0cxxc/en05YKMPMOfaHw3Xb1L06r3dP7cPIuDm2ZiZedO+wHzsHN5N/eHeJM124Ix0FcwpLc7JkY6XL4Rw6gfbpKcnHEW4mRvgLlpxv+ApbkuX33uiZWFLnHxqdx9GM9Xk29w7sXsQ6mpaXw95Qa9OrgyabgvBgYKnjxVMuWXuxo3HcsN8zJFqXRghfp5wPSvAQhavolLPUeh72iLoauj+vWE+48407wvATNG4f5FVxIfhXC572hC92VMjxm8fjd6tlb4jhuYfkOxi4GcbtqLpNcGEOdFnaoViYyOYeHvm9Jv1OVRiBljhmP1osvN09AwFK/8nycolcxYsIxnYeHo6+nh5uzI2EF9qVNVcxaw/UdPkpYGdau+m9nB6lYpT0R0LAvXbn1x3HRl5jeDNfN85buZmKhk2sJVPAuLQF9PFzdnR8Z/0ZO6VTLG6YVFRDJn2VrCI6OxsTSnYY3KfNY6d13+stKgXDEiYuL4ZesBwqJj8XN15KdBXdVdg0LCozT+z3Pjhz5tmbtpH18vXE90XAKO1hZ83qIubWrk/54MIsOQIUPo1q0bZcuWpXz58syaNYu4uDh69OgBQNeuXXF2dlZPQT9o0CBq1KjBjBkzaNKkCWvWrOHs2bP89ttv/1iOWmlp0v7zXxUXF4ezszMzZsygZ8+eBZ0OJ6/nfaaEf1vFwuacuRFZ0Gm8UTk/CyIuZj0H9/vEskQN/rz8/g9+rFXMkHm73v/D4IDGWqw78fZjgv5JbSspWH30/d+XHatqUaf9m2/q8z44sKY8O3VznvChoDVJvsHzq/mbG//fYlukAuGX/iroNHJkVbwa8UfezfTc/ySj6m8/a2B+zf+jwDZN3/p5X2fevHlMmzaNkJAQSpYsyZw5c9TjWGvWrIm7uztLly5Vx69fv57Ro0dz//59fHx8mDp1Ko0bN35H7yCz965FQOTf+fPnuX79OuXLlycqKopvv01vPv/kk08KODMhhBBCiI/PgAEDsu0KlNXEMm3atKFNm7cfe5hbUhH4j5k+fTo3btxQ903766+/sLHJ3ywsQgghhBDiv0sqAv8hpUqV4ty5cwWdhhBCCCHEP0I6tL9b//CEZUIIIYQQQoj3kbQICCGEEEKID4K0CLxb0iIghBBCCCHER0haBIQQQgghxAdBJS0C75S0CAghhBBCCPERkoqAEEIIIYQQHyHpGiSEEEIIIT4IaQU6WlirALf9z5AWASGEEEIIIT5C0iIghBBCCCE+CDJ96LslLQJCCCGEEEJ8hKQiIIQQQgghxEdIugYJIYQQQogPgkpV0Bn8t0iLgBBCCCGEEB8haREQQgghhBAfBBks/G5Ji4AQQgghhBAfIWkREEIIIYQQHwSVtAi8U9IiIIQQQgghxEdIKgJCCCGEEEJ8hLTS0mTYhRBCCCGEeP/N2FJwp61DW2gV2Lb/KTJGQPxrrt4OLugUclTE25Edf6cUdBpv1LS0DhduPS/oNHJU0seW44ExBZ1Gjir7m7L+5Ps/MXWbigpW/vV+X7fpXE2LfReVBZ1GjuqV0GfQ7Pf/uzl7kCnPr54q6DRyZFukAjt1/Qo6jTdqknyDxD+WFHQaOTKo34OIi4cLOo0cWZaoUdApiHdEKgJCCCGEEOKDkFago4X/ey0CMkZACCGEEEKIj5BUBIQQQgghhPgISdcgIYQQQgjxQZD7CLxb0iIghBBCCCHER0haBIQQQgghxAdBJr1/t6RFQAghhBBCiI+QVASEEEIIIYT4CEnXICGEEEII8UFQyWjhd0paBIQQQgghhPgISYuAEEIIIYT4IMhg4XdLWgSEEEIIIYT4CEmLgBBCCCGE+CBIi8C7JS0CQgghhBBCfISkIiCEEEIIIcRHSLoGCSGEEEKID4JK+ga9U9IiIIQQQgghxEdIKgLviUOHDqGlpUVkZGS2MUuXLsXCwuJfy+ml8ePHU7JkyX99u0IIIYQQr0pTFdzjv0i6Bon3wu4dm9mycQ2REeG4e3jTq99AfPz8s4zdt2cHhw7u5eH9ewB4efvSqVtvjfi5P07mzwN7NdYrWbocYydOe6s8j/6xmkPblxATFYpTIT9adv+aQt7Fs4wNCbrNng1zeXT3GhGhT/iky0iqN+76VmXm1t4dG9m+6XciI8Jx8/CiR98v8fYLyDL2wJ5tHDm4h6AHdwHw8PajQ9e+6viUlBTWrviN82dP8izkCUbGxhQtUZaO3f+HlbVNvnM8sGsduzevICoyjELuPnTqPRxP36JZxj5+eIfNq3/l/p3rhD0PpsNnQ6jfvKNGjCo1lS1rfuPE4d1ERYZhYWlD1drNaNa2J1paWvnO8+T+VRzdvZjYqFAcXAvTtPM3uHhl//lcOb2H/ZvmEBn6GGt7N+q3HYpfiRoaMc+e3OGPtTO4d+MMqtRU7Jy96PDFbCysnfKd55mDqzixdxGxUaHYuxamYYfROHtmn+e1s3s4tGU2kaGPsbJ3o07rYfgUz8jz8Na5XD2zi+jwELR1dHF0K0KtloNx9iyR7xwBDu9Zw4HtS4mODMXZzZc2n43C3btYlrHBQbfZsfYngu4FEv78Ca27DadWky4aMX/9sZa//lhH+PMnADi4eNHo074UKVXtrfIEaFRRj0pFdTHU1+Lek1TW/5nI88jsuyV4OWlTu4wernYKzE0ULNyewOW7KRoxerrQrIo+xT11MDLUIjxKxZGLyRy7nJzn/Dbu3s/vW3YRHhmFl7srX/bqQoCPV5axh0+eYfnG7TwOfkZKagoujg60b96IhjWrqGOqtsp8fALo37UdHVs0yXN+AFZVy+I5tCfmpYti4GTH2db9ebrtwJvXqV6egOlfYRLgQ2JQMLcn/8Kj5Zs1Ytz+1xHPIT3Rd7Al+tJ1rg6eSNSZy/nK8aU1R86x7MApQqPj8HW246tP61HMPev/ya0nLzF21S6NZXo62pyZORyA5NRU5u04wtGrd3kUFompgT4V/NwY9ElN7MxN853jhj1/snL7H4RHRuHt5sLQzzpQxNsjx/X2HTvNmNkLqV62BFNHfK5enpaWxoJ129h64C9i4xIoVtiLEb06UcjRPt85ig+LtAiIAnf0yEGWLPiZth27M33OAtw9vPh2zHAiIyOyjL9y+QJVq9fh28kzmTzjJ6xt7ZgwZhhhoc814kqVKc+iFRvVjyEjxr5VnudP7GbbiqnUb92fL79fj5ObH7/90JeYqLAs45OSErC2c6VJhy8xtcj6hDmvZebG8SMHWL5wHq079OCH2Ytw8/Dm+7FDiMpmf169fJ7KNeoydvJcJk6fj7WtPZPGDiH8xf5MUiZy785NWrfvxg+zFzPk60kEP37ItIkj853jqaN/sGbxTD5p35vxP67E1d2XGRO+IDoyPMt4pTIRWwcX2nQdgLmldZYxuzYt4889G+jcZwTfz11Pm25fsHvzcvbvXJvvPC+f2sXu36dQ65PP6T9hIw6ufiyd3pvY6Kw/n4e3zrPul2GUqd6a/t9uwr90HVbP/oKnj26qY8KePmTBd52wcfKg56hlDPhuCzWb/w8dXf1853n19C72rfuB6s0+p/fYTdi7+rF6Vi/isskz6PbfbPptKCWrfkrvsZvxK1WXdT8N4NnjjDytHNxp2HEMfSdso9vIVZhbO7NqZk/iYrL+jHLj3PE9bF4+jUaf9mPklLU4u/nx06R+2f8PKROxsXehecdBmGXzP2RhZc8nHQcz4oc1DJ/8O75Fy/Pb1EEEB93Od54AdcroUb2kHusOKpm5Np6k5DT6tTBCRzv7dfR04XFoKhsOKbONaVlNH383HVbsTWTy8jgOXUimdU19inq8oeAsHDh6knlLVtOjbQsWTf8Wb/dCDPl2GhGR0VnGm5qY0LV1c379YQzLZk6ice1qTJ63gFPnL6ljti6ao/EY9XkvtLS0qFGxXJ5ye5W2sRHRl25wZeCEXMUburtQbtt8wg6d4mjZT7g3dxnF5n+HTb2q6hjHNo3wnzaKW9/9xNHyLYm5dJ0KOxehZ2uV7zz3nAtk+uaD9G1UlTUjeuDnbMf/fl5LWExctuuYGOhzYNIA9WPPhP7q1xKTkrke9JQ+DSuzdkR3fuzVkvvPwhk0f2O+c9x3/Ayzl6+n16dNWTZlND5urgyeNJvwqKw/85eePAtlzooNlPT3yfTaiq17Wbf7ICN7d2bh96Mw1Ndn8KTZKJPyXjEVHyapCPyLlEolAwcOxM7ODgMDA6pWrcqZM2eyjV+6dCmFChXCyMiIli1bEham+WP5ssvO/PnzcXV1xcjIiLZt2xIVFaURt3DhQvz9/TEwMKBw4cL8/PPPGq+PHDkSX19fjIyM8PT0ZMyYMSQnZ38QuHPnDp6engwYMIC0dzBoZ/vm9dRr2IQ69RrhWsidvgOGoG9gwME/dmUZ/+Xw0TRq2gIPLx9cXN3oP3A4aao0Ll38WyNOV1cXSytr9cPENP9XYQCO7FxGxdqfUr5mSxxcvGndcxy6egacPrQpy/hCXsVo1mkYpSo3RkdH752UmRs7t6yhToNm1KrXBJdCHvT6fDh6+gb8uW9HlvEDh4+jQZNWuHv64OzqRr8vRpKmUnH54lkAjIxNGP3dLCpVq4OTSyF8CxelR78h3L19g9BnIfnK8Y+tq6hevwXV6jTH2dWTrv8bhZ6+AX8d2JZlvKdPEdp1H0SFag2y3Ze3b1yiVPkalChbFRt7J8pVrkuRkhW4e+tqvnIEOLZnGWVrtKFM9VbYOXvTvPt4dPUMOHck68/n+B/L8SlWlWqNe2Ln5EXd1oNwdPfn5P7V6pj9G2fhW6I6DdsNx8ktAGv7QviXro2JWdYVnNw4uW8ppaq1oWTV1tg6edOk8wR09Qy4cDTrk47T+1fgXbQqlRv2xNbJi1otBuHoFsCZg6vUMcUqNMMzoDKWtq7YOftQv91XKBNiefboRr7zPLhjOZXrtKZSrRY4unjRvvcY9PQMOfHnlizj3byL0rLLUMpWaYSObtafe7GyNSlSuhp2jm7YO7nTvMNA9A2MuHfrUpbxuVWjlC5/nFZy5W4KT0JVrPwjEXNjLYp5Zd+QHvgglV0nkrh0JyXbGA9HbU4HJnP7cSrhMWmcuJLMk+cqCjnkrSKwZvsemtWrSZM61fFwdWZ43+4Y6Ouz4+DhLONLF/WnRsWyuLs44+xgT9umDfByc+VSYEblz9rSQuNx9MzflC7qj7ODXZ5ye9XzvUe4OW4WT7fuz1W8W5/2JNx7ROCIKcRev8uDn1cRsnEvHoO6q2M8BvcgaNE6Hi3bRGzgHS73H0dqfCKu3VvnO88Vf56mVaUStKhYHC9HG0a3a4iBni5bTmT/PdLSAhszE/XD2sxY/ZqpoQHzB7SnQWl/3O2tKe7hzKg29bkWFEJweFS2Zb7J7zv28UmdqjStVQUPFydG9u6EgZ4eO/48lu06qSoV4+Yuonfb5jjZaVam09LSWLtrPz1aNaF6uZL4uLkwbkAPQiMiOXLmfL5y/DekpaUV2OO/SCoC/6IRI0awceNGli1bxt9//423tzcNGjQgPDzzFbZTp07Rs2dPBgwYwIULF6hVqxbfffddprjbt2+zbt06tm/fzp49ezh//jz9+2dclVi1ahVjx45l0qRJBAYG8v333zNmzBiWLVumjjE1NWXp0qVcu3aN2bNns2DBAmbOnJnle7h06RJVq1alY8eOzJs37626XAAkJydz5/YNipcso16mUCgoXrIMN65fy1UZSUolqakpmL52on/l8gW6d2zBgD5dmP/Tj8RE5+/gC5CSksSje9fwKVpJI0/fohV5cOvi+1NmcjJ3b9+kWMmyGmUWK1mWW9dzd0KsVCpJSU3BxNQs25j4+Fi0tLQwMsl75SolOZn7d65TpHgFjRwDSpTn9o38n7x5+xXn2qUzhDx+AMDDeze5FXiR4qUr56u8lJQknty/ilcRzc/Hq0glgm5fyHKdoNsXNeIBfIpWVcerVCpuXDyMjYM7S6f1YvKAKvw6oR3XzuXuJCkrqSlJBD+4ikdAxvvUUijw8K/Eo7tZ5/no7gU8/DX3i2eRKjy6k3V8akoSfx9Zi76hKfYuhfOVZ0pKMkF3A/ErVlG9TKFQ4FesAvdu5u/7/jqVKpWzx3aTpEzAwzf/XZiszbQwN1Zw82GqelliEjwIScUjjyfsr7sXnEoxTx3MjdOPnd4u2thaKrjxIDWHNTMkJ6dw8859yhYvol6mUCgoWzyAqzdybglJS0vj7KWrPHwSTMmArD/P8Mgojp+7SJM61XOd17tgUbEkoQdPaCx7vu8olhVLAqClq4t56SKEHjieEZCWRujB41hULJWvbSanpBIYFEJFP3f1MoVCi4p+7ly6/zjb9eKVSTQc+zP1x/zEoN82cDv4ebaxALEJSrS00isJec8xhRt3H1KuWEYXWIVCQbli/ly+eTfb9RZv2IGVmSnNa1fN9NqTZ6GERUZTrnhGmSZGRhTx9nhjmeK/RcYI/Evi4uL45ZdfWLp0KY0aNQJgwYIF7Nu3j0WLFlGunGbT6+zZs2nYsCEjRowAwNfXl+PHj7Nnzx6NuMTERJYvX46zszMAc+fOpUmTJsyYMQMHBwfGjRvHjBkzaNWqFQAeHh5cu3aN+fPn061bNwBGjx6tLs/d3Z1hw4axZs0a9bZfOn78OE2bNuWbb75h6NCh72S/xERHoVKpsLDQbNK1sLDkcdDDXJWxfMl8LK1sNCoTpcqUp0Ll6tg7OBIS/JhVyxYycdxIJk//CW3tvP+Qx0VHolKlYmquedXWxNyaZ0/u5bm8f6rM6OgoVKpUzF/bn+YWVjx59CBXZaxa+jNWVjYalYlXJSUpWb3kFypXr4uRkXGWMW8SE5P+vs1ez9HcipBH9/Nc3kuNW3cnISGOrwd8ikKhQKVS0apTfyrVaJSv8uJf5GmSxecTGpz15xMbFYqxmU2m+JioUADiosNISoznyI6F1G09kAZth3Lz8lF+nzuQz75aikfh8nnPMzaCNFVqphYFYzMbQkPelOdr78vMhrgXeb508+KfbPptKMlJCZia29J5yGKMTC3znCNAbHRE+vfdQnO7ZhbWPM3n9/2lxw9vMuObLqQkJ6FvYETvYbNwdMm6r3xumL44SY+J17wCGBOfpn4tvzYcVtK+tgHf9jIhNTWNtDRYcyCRO09yXxGIiokhVaXCykKzsm5lYc6Dx8HZrhcbF0/L3oNISk5BW6FgSJ+ulCuZ9bic3X8excjQgBoVsz4O/FP07W1QPtX8HiqfhqJrborCQB9dS3MUOjoon4W9FhOGsZ9nvrYZERdPqipN44o+gLWpMfeeZt1tzd3emgkdG+PjbEdsgpJlB0/R7ceVbPq6J/aWmS+iKJNTmLXtTxqVCcDEMO/dACOjY7P8zC0tTLn/JOvP/ML1W2w7eJQVU8dk+XrYi25kVq+NWbAyN1O/9j5S/UcH7RYUqQj8S+7cuUNycjJVqmQMzNLV1aV8+fIEBgZmqggEBgbSsmVLjWWVKlXKVBEoVKiQuhLwMkalUnHjxg1MTU25c+cOPXv2pHfv3uqYlJQUzM3N1c/Xrl3LnDlzuHPnDrGxsaSkpGBmpnmwefjwIfXq1WPSpEkMHjz4je9VqVSiVGr2kdXXz3//5zfZtG4Vx44c5NsfZqGnl7GNqjXqqP92c/fEzd2L/r06cvXyBY0Kg9C0Zf0Kjh85wLjJczX250spKSnM+mEsaUCvz4f9+wm+wZlj+zhxeA99h3yHk6sXQfdusHrxj1hY2VK1dtOCTg9A3bTsX7o2VRp2B8DRzZ+gW+c5fXBtvioC/yT3whXoM3Yz8bERnP9rPRvnD+azr9dlqkQUNHsnD0ZNW09CfCznT+5jxU+jGTRhca4rA2X8dGhXO+Mq7fxtCf9UqlQvoYuboza/bYsnIiYNLydtPq1lQFRcAjeDcl8ZyA8jQwOWzPiOhMREzl66xrwlv+Nkb0fpopknZth58Aj1q1VCXy/rLlkfuxIezpTwyPjtLeHpTMvvFrD+2AUGNNVsRUlOTWX44i2kpcE3bRv8K/nFJSQyYe5iRvXtgoXZ23WLFf9tUhH4D4uNjQXSWx4qVKig8drLq+InTpygU6dOTJgwgQYNGmBubs6aNWuYMWOGRrytrS1OTk78/vvvfPbZZ5kqCq+aPHkyEyZoDgwbN24cbTr3zRRramaOQqEg8rVBopGREVhYvnng15aNa9i0YTXjJ83A3ePNP/gOjk6YmZkTHPw4XxUBYzMLFArtTIMaY6PCsh0IXBBlmpmZo1BoE/Xa/oyKDMcim0G2L23ftJqtG1Yx+rtZuHl4Z3o9vRIwhufPQhj7/Zx8tQYAmJqmv+/XBwZHRYVjlkOOb7J26RyatO5GhWrpP7Su7t6EPg9m58Yl+aoIGL3IMzaLz8fEPOvPx8Tchrjo0Ezxpi/ijUwtUGjrYOuk+X21dfLkwU3NMS65ztPEEi2FdqYBzHHRoTnk+dr7ig7F+LV4PX0jrOzdsLJ3w8WrJD993YDzRzdQtXHm/+WcmJhZpn/fIzW3Gx0Zlu1A4NzS0dHF1qEQAIU8A3h45wqHdq2iQ5/cTRBw5W4KD0IyBoXqaKdf9Tc10iL6lVYBUyMtHj/P/+VIXW1oWlmfRTsSuHY//aT/SagKZ1sFtUvrcTModxUQc1NTtBUKwl+7ahseGYW1hXk2a6V3JXF5MRuMj4cbDx49YeWm7ZkqAhev3eDh42AmDPk8q2L+Ucqnoejba34f9O1tSI6KQZWoJCk0AlVKCvp21q/FWKMM0fzfyy1LYyO0FVqERWsODA6LicPGLHfHOV1tbQq72BP0XHNShpeVgODwKBYM7Jiv1gAACzOTLD/ziMiYLD/zx0+fE/w8jOFTflIve3kjrirt+7F21rdYv2hdCI+KwcbSQh0XHhWNj7trvvIUHx4ZI/Av8fLyQk9Pj2PHMgb1JCcnc+bMGQICMk/r6O/vz6lTpzSWnTx5MlPcw4cPefLkiUaMQqHAz88Pe3t7nJycuHv3Lt7e3hoPD4/06caOHz+Om5sb33zzDWXLlsXHx4cHDzJ3ITE0NGTHjh0YGBjQoEEDYmJisn2vo0aNIioqSuMxatSoLGN1dXXx8vbj0oWMkyCVSsWlC+fwK5z1dJcAmzf8zoY1Kxjz7VS8fXLusxwa+oyYmGgs83miqaOjh4tHALeuZHwGKpWKW1dP4eaTv77I/0iZurp4evty+eI5jTKvXDyHT+Ei2a63dcMqNq5ZxqgJ0/HKYn++rAQEP3nEmEmzMDXL/mQjNzm6exXm2qXTGjkGXjqDt1/+p01NSkpES0vzkKZQaOd7gJeOjh5O7kW4e03z87l77SSu3iWzXMfVuwR3rmn+n96+elwdr6Ojh7NH0UxddkJD7mNhk7+pQ7V19HB0K8L9wIx+1WkqFfeun8TFM+s8XTxLci9Qsx/2vWvHcfHKOl5dbpqK1OSkfOWpo6OLq6c/N65kHNdUKhU3r5x6q/78WUlTqUjJQ57KZAiNSlM/QsJVRMWp8HXN6EaorwduDtrcC8n/VXuFdnol4/WvpCotfeBpbunq6uDr5c65SxnjflQqFecuXaOIX+ZKfHZUaSqSkjMPbN5x4DB+Xu74eBTKfVLvSOTJC1jXrqixzKZOZSJOXgAgLTmZqL+vYlP7lbE4WlpY16pE5Mn8DXDV1dHG39WBUzfvq5epVGmcuvmA4u7O2a/4ilSViltPnmNjbqJe9rIS8PB5BPMHdMDC2DBf+aXnqIOfZyHOXLn+So4qzlwJpJhv5i5Rbk4OrJo+juVTx6gf1coUp0wRP5ZPHYO9jRVOdjZYW5hx5nKger24+ASu3r6XZZnvCxks/G5Ji8C/xNjYmP/9738MHz4cKysrChUqxNSpU4mPj6dnz55cvKg5WG7gwIFUqVKF6dOn88knn7B3795M3YIADAwM6NatG9OnTyc6OpqBAwfStm1bHBwcAJgwYQIDBw7E3Nychg0bolQqOXv2LBEREQwZMgQfHx8ePnzImjVrKFeuHDt37mTz5s2ZtvPyPezcuZNGjRrRqFEj9uzZg4mJSaY4fX39PHUFatayDXN/nIy3jx8+vv5s37oBZWIiteul9+2ePeN7rK1t6Ny9DwCb1q9mzcolfDliNHZ2DkSEp19hNDA0xNDQiISEeNatXkbFKtWxtLQiJPgJyxfPx8HRmVJl8j8NXvUm3Vjzy9e4ehahkHcxjuxeQZIygfI10rtwrf55FOaWdjTp8CWQPtj06aM7AKSmJBMV8YzH9wPRNzDCxsEtV2XmR5MW7fl55iS8fArj5evPrq3rUCYmULNu+jzg82ZMxMralo7d+wGwdcNK1q1cxMDh47CzdyQy4sX+NDDEwNCIlJQUZk4ezb07NxkxdgoqlUodY2Jiho6ubp5zrP9JJxbOHo+7dwCePkX4Y/tqlIkJVK3TDIAFs8ZiYW1Hmy4D0vdlcjJPgu6q92VE+HMe3r2BvqER9o7pV65Klq3Gjg2LsbZ1wNnVkwf3brB32yqq1Wme731ZpWE3Ni4YhZNHUVw8i3F873KSlAmUqZb++WyYPxIzS3vqtx0CQOX6XVk4uStHdy/Br0QNLp3axZN7V2nRI6OFrFqjz1j781Dc/cri6V+BW5eOcuPCIT4btSzLHHKjYr3ubF38FY5uRXHyKM7p/ctIViZQokr62KAti0ZiamFHndbpY3vK1+3C8mldObF3MT7Fa3L19E6e3L9Kk67fApCkjOfozl/xLVEbEwtbEmIiOPPnaqIjnuJftmG+86zdtCsrfhpNIc8A3L2L8eeulSiVCVSs2QKA5fO+xtzKnk86DgLSBxiHvPgfSklJJjL8GY/uX0ffwEjdArB19WyKlKyCpY0jiYlxnD26m1vXztL/m1/znSfA4fPJ1C+vz/NIFWHRaTSupEdUXBqXX5kR6PNWhly6ncJfl9JnWdPTBVvzjMqotbkWzjYK4pVpRMSkoUyCW49S+KSqPskpSsJjVHg7a1POX5ctR7KfcjQr7Zs1ZNLcBRT29sDfx5N12/8gQamkSe30bikTZ8/H1tqSfp3bArBi43YKe3ng5GBHckoyJ85dYu/h4wzr002j3Lj4BP48fpoB3Ttm2mZ+aBsbYeydUaEw8nDBrERhksKjSAwKxu+7IRg423OxR/p0xA9+W4Nb/04UnjycoKUbsalVEcc2jTjTPKMV6t6sJZRYPIXIc1eIOnMJ94Hd0DE2JGhZ/mdb61KrPGNW7qBIIUeKujmy8tBZEpRJtKiYfnHim+XbsbMwZVDzmgD8uvsoxd2dKWRrSUxCIkv3nyI4IppWldIrtcmpqQxbtJnAoKfM7fspqjQVodHprfTmRobovmke2mx0aFqPiT8twd/TjQBvD9bu2k+iMokmL+4FMWHeYmytLOjfsRX6erp4FdKsxJgYGwFoLG/XuC5LN+3C1dEOJzsbfluzFRtLC6qXy9/Aa/HhkYrAv+iHH35ApVLRpUsXYmJiKFu2LHv37sXSMvPgu4oVK7JgwQLGjRvH2LFjqVu3LqNHj2bixIkacd7e3rRq1YrGjRsTHh5O06ZNNaYH7dWrF0ZGRkybNo3hw4djbGxMsWLF1P38mzdvzpdffsmAAQNQKpU0adKEMWPGMH78+Czfg4mJCbt376ZBgwY0adKEXbt2YWycvy4iL1WtXpvoqEh+X7mEyIhwPDy9GfPtVHXXoNDnT1G8crls766tpKQkM+37cRrltO3YjfadeqBQaPPg/l3+PLCX+LhYLK2sKVmqHB26fIZuNlMQ5kapSo2Iiw5n74Z5L26GVJjeX81Xd+OJDA3WmEUpOuI5P476VP380I4lHNqxBC//cvQfuzRXZeZH5ep1iI6KZN3Khek3aPP0ZtS3M9T7M+z5UxSKjJOVfbu2kJKSzI+TR2uU82mHHrTp1JPwsOecPXUUgJEDe2jEjP1+DkWKl85zjhWq1icmKoItv/9KVEQYhTx8GTJuLuYvBpKGPQ/RuLofGf6ccUM6qZ/v2bKCPVtW4FekNF9N+g2ATn2Gs3nVr6yY/wPRURFYWNpQs0ErPmnbm/wqVqExcdERHNg0h9ioUBwL+dNt2G/qLjeR4cFovbIvC/mUom2/aezfOJt9G2Zibe9Gx0FzsXfxVccElK1H8+7jOLLjN3au/B4bRw86fDEbd9/8j10pUr4x8bHhHN46l9jo59i7+tNx8AJ1ntFhTzS+m67epWnZezp/bp7Fn5tnYmXnTtvP52HnnJ6nQqFNaPA9Lh0fSHxsBIbGFjh5FKP7yFXYOWeeizy3ylRuSGx0BDvX/UxMZCjO7n58/vUvmL343MNDNT/3qPBn/DCirfr5ge3LOLB9Gd4BZRk8fjEAsVHhLP9pNNERzzEwMsHZzZf+3/yKf3HN2Zvy6sC5JPR0oV0dAwz1tbj7JJVft8ST8kqDgLW5AmPDjP1ayE6bLz41Uj9vWT193MGpa8ms3pcIwLLdiTSrok+XhgYYGWgREa1i53Flnm8oVqdqRSKjY1j4+6b0m0t5FGLGmOFYvegm8jQ0DIUiI7cEpZIZC5bxLCwcfT093JwdGTuoL3Wqal5933/0JGlpUPe15fllXqYolQ6sUD8PmP41AEHLN3Gp5yj0HW0xdHXMyPP+I84070vAjFG4f9GVxEchXO47mtB9R9Uxwet3o2drhe+4gek3FLsYyOmmvUh6lvXA3txoWMafiNh4ft75F6Excfg52/Fz/3bqAcQhEdEav0Mx8Yl8+/tuQmPiMDM0IMDVgWVfdsbLMf1/7llkDIcup8/g1HbKEo1tLRzYgXI+bnnOsV7lckRGx7Bg3TbCIqPxcXdh5tcD1V18QkLD8zyTX5dPGpCoVPLD/JXExsdTvLA3s74ehL5e3i/w/FtU/80L8wVGK+2/2tbxERg/fjxbtmzhwoULBZ1Krly9nf1sFu+LIt6O7Pg7+znA3wdNS+tw4dabp6l7H5T0seV4YPZdyN4Xlf1NWX/y/Z+Gok1FBSv/er8P152rabHvYt6ubBeEeiX0GTT7/f9uzh5kyvOrp3IOLGC2RSqwU9evoNN4oybJN0j8Y0nOgQXMoH4PIi5mfS+I94nla3dL/zeNXpq/7onvwnfd/3uD56VFQAghhBBCfBDSpEngnZLBwkIIIYQQQnyEpCLwARs/fvwH0y1ICCGEEEK8X6RrkBBCCCGE+CDIyNZ3S1oEhBBCCCGE+AhJi4AQQgghhPggqGSw8DslLQJCCCGEEEJ8hKQiIIQQQgghxEdIugYJIYQQQogPgtwH992SFgEhhBBCCCE+QtIiIIQQQgghPghpqoLO4L9FWgSEEEIIIYT4CElFQAghhBBCiI+QdA0SQgghhBAfBJUMFn6npEVACCGEEEKIj5C0CAghhBBCiA+CTB/6bkmLgBBCCCGEEB8haREQQgghhBAfBJVKWgTeJWkREEIIIYQQ4iMkFQEhhBBCCCE+QtI1SAghhBBCfBBkrPC7pZUmw6+FEEIIIcQHYPDc2ALb9qwvTAps2/8UaREQ/5rLt58WdAo5KuZtz/5LyoJO443qFtfn1p0HBZ1Gjny83Nh+LqWg08hRszI6TPz9/c9zTAcdDl1JKOg03qhmUcMP5jNv1P1SQaeRo91LixN+6a+CTiNHVsWrkfjHkoJO440M6vdgp65fQaeRoybJN0g4uKKg08iRYe0uBbbtNBks/E7JGAEhhBBCCCE+QlIREEIIIYQQ4iMkXYOEEEIIIcQHQSVDW98paREQQgghhBCigISHh9OpUyfMzMywsLCgZ8+exMZmPyg6PDycL774Aj8/PwwNDSlUqBADBw4kKioqz9uWFgEhhBBCCPFB+C8OFu7UqRPBwcHs27eP5ORkevToQZ8+fVi9enWW8U+ePOHJkydMnz6dgIAAHjx4QL9+/Xjy5AkbNmzI07alIiCEEEIIIUQBCAwMZM+ePZw5c4ayZcsCMHfuXBo3bsz06dNxcnLKtE7RokXZuHGj+rmXlxeTJk2ic+fOpKSkoKOT+9N7qQgIIYQQQogPQkG2CCiVSpRKzSnG9fX10dfXz3eZJ06cwMLCQl0JAKhbty4KhYJTp07RsmXLXJUTFRWFmZlZnioBIGMEhBBCCCGEyNHkyZMxNzfXeEyePPmtygwJCcHOzk5jmY6ODlZWVoSEhOSqjNDQUCZOnEifPn3yvH2pCAghhBBCCJGDUaNGERUVpfEYNWpUlrFfffUVWlpab3xcv379rXOKjo6mSZMmBAQEMH78+DyvL12DhBBCCCHEB6EgxwrnpRvQ0KFD6d69+xtjPD09cXBw4NmzZxrLU1JSCA8Px8HB4Y3rx8TE0LBhQ0xNTdm8eTO6urq5yu1VUhEQQgghhBDiHbK1tcXW1jbHuEqVKhEZGcm5c+coU6YMAAcPHkSlUlGhQoVs14uOjqZBgwbo6+uzbds2DAwM8pWndA0SQgghhBAfhDRVWoE9/gn+/v40bNiQ3r17c/r0aY4dO8aAAQNo3769esagx48fU7hwYU6fPg2kVwLq169PXFwcixYtIjo6mpCQEEJCQkhNTc3T9qVFQAghhBBCiAKyatUqBgwYQJ06dVAoFLRu3Zo5c+aoX09OTubGjRvEx8cD8Pfff3Pq1CkAvL29Ncq6d+8e7u7uud62VASEEEIIIYQoIFZWVtnePAzA3d2dtLSMFomaNWtqPH8bUhEQQgghhBAfhHd1AizSyRgBIYQQQgghPkJSEfiP0dLSYsuWLQWdhhBCCCHEO6dSpRXY479Iugb9g54/f87YsWPZuXMnT58+xdLSkhIlSjB27FiSk5OpVavWG9f/888/qVmz5r+TbAHbvWMT2zauITIiHDcPL3r2G4SPX0CWsfv2bOfwwb0E3b8LgKe3Hx279c42fv686ezbvY3uvQfQtEXbt8rz8J417N+2lOjIUJzdfGn72SjcfYplGfsk6DY71/7Ew7uBhD9/Quvuw6ndpItGzJG9a/nrj3WEP38CgKOLF43a9KVIqWpvleeO7dvYtHE9ERHheHh40vd/n+PnVzjL2D17dnHwwH4ePLgPgLe3D1279dCIT0hIYOmSRZw8cZyYmGjs7R1o1rwFjZs0fas8j/2xmkM7lhATFYpjIT9advuaQt7Fs42/eHIve9bPJSL0MTYObjRpPwT/UtXVr8dEhbLz9x+5eek4CfExeBYuQ4tu32Dr6PZWeQLUKKaglJcWBroQFJrG7jMqwmOzjy9kC5X8FThaamFqpMW6I6nceJz5hySv5Wbnz91r2Ld1GVGRYbi4+9K+50g8svtuPrzNtjW/8PDuNcKeB9OmxzDqNu2sEbN97S/sWDdfY5m9kzvfzt2S9+Re8SF95l1a2tOwhhXGRtpcuxXHvOWPefI0Kdv4JrWsaFLbGnsbPQAePE5k9dZnnL0co46Z8pUnxQubaKy3888w5i17nOf8Nuw5yKptewmPjMLbzZUhn3WgiI9nlrGHTp1j2aZdPAp5RkpqKq4O9nRoVp9GNSqpY8Ijo/hp5UZOX7pKTFwCJf19GNqzI66O9nnO7VVrjpxj2YFThEbH4etsx1ef1qOYu1OWsVtPXmLsql0ay/R0tDkzczgAyampzNtxhKNX7/IoLBJTA30q+Lkx6JOa2Jmb5is/q6pl8RzaE/PSRTFwsuNs6/483XbgzetUL0/A9K8wCfAhMSiY25N/4dHyzRoxbv/riOeQnug72BJ96TpXB08k6szlfOX40ppDZ1m27wRh0bH4utgzsl0Dirk7Zxm79cRFxi3frrFMT0eb03MzboB14Px11v91jsCHIUTFJbDm614Udn3z3PXiv0UqAv+g1q1bk5SUxLJly/D09OTp06ccOHCAsLAwGjZsSHBwsDp20KBBREdHs2TJEvUyKyurgkj7X3fsyAGWLfiJPgOG4uMXwM4t6/luzDDm/LYKcwvLTPFXL5+navU6+PUdhJ6eHls2rGbimGHM/HkZ1jaac/aeOn6EW9evYWVt89Z5nju2h03LptG+zxjcvYvx586VzJvUj3Gzt2Fqbp0pPlmZiLWdC6Uq1Wfj0mlZlmlpbc8nnQZj51iItLQ0Th3axvwpg/hq2jqcXL2zXCcnRw4fYuGC+Xw+YCB+hQuzdcsmxo75mvm/LcIii/15+dJFatSoib9/EXT1dNm4fh1jR4/ip18WYGOTvt8WLviVSxcvMnT4SOzt7Tn/9zl+/mku1tbWVKhYKVOZuXHhxG62rZxK68/GUci7GH/tXsGCH/oyYsaOLPfn/ZvnWTVvOI3aDSagdA3OH9vJ0h+/YPD3G3B09SEtLY2lMwai0NGh+9C5GBiacGTXMuZP7snwqdvQNzDKV54Alf21KO+rxdaTKiLj0qhZTEHHWtr8sjOVVFXW6+jqaPE0Ai7cVdG2mvY7KzcrZ47tZcPSGXTs+w0ePsU4sGMVcyb2Z8LcrZiZZz6OJCUlYmPvTJnK9Vi3ZHq25Tq5ejF4XEZlQFs76/eRWx/SZ96msS3N69kwY0EQIc+T6NrKnu+GetD3m5skJ2d9ZTA0Ipkl60N4/FSJFlC3qiVjB7kxYOwtHj5RquN2Hwpjxean6udKZR4+7Bf2HzvNnGXrGNGnM0W8PVm7cz9fTprFmtnfYWVulinezMSYbq2a4O7sgI6ODsfOXWLSz0uwNDelYsmipKWlMXLqT+joaDNlxACMDQ35fccfDPx2BqtnTsTQIHc3UXrdnnOBTN98kNHtGlDMzYlVh87wv5/XsnVMH6xNjbNcx8RAn61jequfa6Gl/jsxKZnrQU/p07Ayfs52RMcnMmXjfgbN38jvI7rnK0dtYyOiL90gaOlGym74Kcd4Q3cXym2bz8Pf1nCh6zCsa1ei2PzvSAx+Tui+owA4tmmE/7RRXPl8HJGnL+IxsBsVdi7iUJGGJD0Pz1eee89eZcbGfXzToRHFPJxZdfA0/ef8ztbx/8PKLPt9uWX8/9TPtbQ0X09ISqKUlyv1Swfw7aqd+cpLfNika9A/JDIykr/++ospU6ZQq1Yt3NzcKF++PKNGjaJ58+bo6enh4OCgfhgaGqKvr6+xTE9PL1O5SUlJDBgwAEdHRwwMDHBzc2Py5MnZ5nH58mVq166NoaEh1tbW9OnTh9jYjMuN3bt3p0WLFkyYMAFbW1vMzMzo168fSUkZV71UKhWTJ0/Gw8MDQ0NDSpQowYYNG97Zvtq+eR11Gzaldr3GuBZyp8+AoegbGHDwj6wPSoOHj6Vh05Z4ePng7OpGv4EjSFOpuHzxnEZcWOhzFv06m0HDx6Ct/fZ13gM7llO5Tmsq1WqBo6sX7fuMQU/PkBMHt2QZ7+ZdlFZdh1K2SiN0dDN/lgDFytakaOlq2Dm6Ye/kTvOOA9E3MOL+zUv5znPL5o00aNiIevUbUKiQG58PGIS+vj77/tibZfzwEaNo0rQ5nl5euLoW4otBX6JSpXHx4nl1TGDgNWrXqUvx4iWwt3egYaMmeHh6cvNG/m+PfnjXMirU+pTyNVvi4OJN657j0NU34MzhTVnG/7VnJX4lqlKr2WfYO3vRsO1AnD0COPZH+kwLoSEPeHD7Iq0/G0shr2LYOXnQ6rOxJCcpuXBiV5Zl5lZ5PwV/XVVx83EazyJh60kVpoZQ2EUr23XuBKdx6LKKG4+yb07OT7lZ2b99BVXrtqJK7RY4uXrRqe9o9PQNOH5gS5bx7t5F+bTbEMpVbfjGO1EqtLUxt7RRP0zMMlck8+JD+sxb1LdhzbannDwfzf1HiUxfEIS1pS6VS2c+yX7p1IUYzlyK4cnTJB4/TWLZxqckJqoo7K1ZIVEmqYiISlE/4hPzXhH4fcc+mtepRtNaVfFwdWJEn87o6+mx4+DRLONLFylMzQqlcXdxwsXBjnZN6uLl5sLF67cBCAp+ypVbdxneuzMB3h64OTswondnlEnJ7Dt2Ks/5vbTiz9O0qlSCFhWL4+Vow+h2DTHQ02XLieyPcVpaYGNmon5Yv3KSa2powPwB7WlQ2h93e2uKezgzqk19rgWFEBwela8cn+89ws1xs3i6dX+u4t36tCfh3iMCR0wh9vpdHvy8ipCNe/EY1F0d4zG4B0GL1vFo2SZiA+9wuf84UuMTce3eOl85Aqw4cIpWVUrRonJJvBxtGd2h8Yt9eSH7lbTAxtxE/bA202yNalqhOH2bVKeCv0e+8/q3paWlFdjjv0gqAv8QExMTTExM2LJlC0qlMucVcmnOnDls27aNdevWcePGDVatWpXtfLFxcXE0aNAAS0tLzpw5w/r169m/fz8DBgzQiDtw4ACBgYEcOnSI33//nU2bNjFhwgT165MnT2b58uX8+uuvXL16lS+//JLOnTtz+PDht34/ycnJ3L19k+Ily6qXKRQKipUsw43rV3NVRpJSSWpqCiamGT/QKpWKuTO+45PW7XF1e/sDXEpyMkF3AylcvKJGnoWLV+DuzYtvXT6AKjWVs8d2k6RMwMO3RL7KSE5O5vbtW5QsWUojz5IlS3H9emCuylC+2J+mJhnN7P7+AZw+dZLQ0FDS0tK4dPECTx4/plTpMvnKMyUlicf3ruFbNKM1QaFQ4FO0Ig9uZb0/H9y6gE/RihrL/IpX4cGtC+llJqdXXl+tdCkUCnR09Lh34+985QlgYQymhlrcC8n4EVAmw+MwcLbJ2wn7P1FuSnIyD+8E4l884w6UGd/N/FcoAZ4FP2REr3p8878mLJo1ivDnwTmvlF2eH9Bn7mCrh5WFLuevZVw0iU9QceNOPIW9sr7y+jqFFtSoYI6BvoLrt+M1XqtV0ZI1cwP45Ttfun/qgL5e3r5Hyckp3Lj7gHLFM7pDKhQKyhX358rNuzmun5aWxpnLgTx8EkIpfx8AkpJTANB7pWKoUCjQ1dXhYuDtPOWnzjMllcCgECr6ub9SphYV/dy5dD/7rlDxyiQajv2Z+mN+YtBvG7gd/PyN24lNUKKllV5J+DdYVCxJ6METGsue7zuKZcWSAGjp6mJeugihB45nBKSlEXrwOBYVS5EfySmpBD4MpkLhjN8zhUKLCoXduXQ3+32ZoEyi0TdzaPD1bAb/so7bT968L8XHR7oG/UN0dHRYunQpvXv35tdff6V06dLUqFGD9u3bU7x49v1hc/Lw4UN8fHyoWrUqWlpauLll3w929erVJCYmsnz5coyN03+85s2bR7NmzZgyZQr29un9PvX09Fi8eDFGRkYUKVKEb7/9luHDhzNx4kSSk5P5/vvv2b9/P5Uqpf+Ae3p6cvToUebPn0+NGjXy/V4AYqKjUKlSM3UBsrCw4nHQw1yVsXLJr1ha2VC8ZMZJ6ZYNq1Foa9O4+advld9LsTERqFSpmbovmJpbE/L43luV/fjBTaZ/04WU5CT0DYzoPXwWjq5e+SorOjoalUqFheXr+9OSR0FBuSpj6ZKFWFlZU7JUafWyfv/7nLlzZtG9a0e0tbXR0lLwxaDBFC2Wv+9yXEwkKlUqJlnsz2dPst6fMZGhmfa/ibk1MZFhANg5eWBh48iuNbP4tOc49AwMObJrOVHhIURH5P/Hz8TwRc6Jr72HxDRM3uK8412Vq/5uWmjuGzNza0Ie3893fh4+xeg+4FvsndyJighlx/pfmTb6M8bN2oCBYe5Ohl/1IX3mlubpP40RUSkayyOiU9SvZcfdxYAfR3uhp6sgQali4twHGt2CDp2I5GlYEuGRKXi4GvBZGwdcHPT5bt6DXOcXGRNLqkqVqQuQlbkZDx6HZLtebFw8zfsOJyklBW2FFsN6daZ8iSLpeTs74GBjxS+rNzGyTxcM9fVZs3Mfz8IiCIvM35X2iLh4UlVpGlf0AaxNjbn3NCzLddztrZnQsTE+znbEJihZdvAU3X5cyaave2Jvmbk1Rpmcwqxtf9KoTAAmhvnrvpRX+vY2KJ+GaubxNBRdc1MUBvroWpqj0NFB+SzstZgwjP2yHsORk4jYbPalmQn337Avx3dppt6Xy/efpPu0pWwc2zfLffmh+Kfu8PuxkorAP6h169Y0adKEv/76i5MnT7J7926mTp3KwoUL6d69e77K7N69O/Xq1cPPz4+GDRvStGlT6tevn2VsYGAgJUqUUFcCAKpUqYJKpeLGjRvqikCJEiUwMspouq5UqRKxsbEEBQURGxtLfHw89erV0yg7KSmJUqWyvrKhVCoztYLo6/8zB+jN61Zy7MgBxv8wBz299G3cuXWDXVs3MHXOQrRe7xD5HrJ38mDUtPUkxsdy/uQ+VswbzeAJi/NdGXgb69et4cjhw0yeMk2ja9r2bVu5cf06Y8ZNwM7OnitXLvPrz/Owfq3CUJC0dXTpPng26xaMYWyfyigU2vgUrUjhEtVII/c/HEXdtGhSLqOx9PfDebtd+39F0dJV1X+7uPvi4VuUUf0ac/bYH1St27IAM8vwrj7zWpUs+KJbxoDLcTPv5zunR8FKPh97C2NDbaqWM2doL1dG/HBHXRnYfTijf/j9R4mERybzw0gvHG31CH6e/UDkd8HI0IBl08aSkKjk7JVA5ixbi7O9DaWLFEZHR4fJw/rz/S/LaNBjENoKBWWL+VOpVFH+zR4RJTycKeGR8VmU8HSm5XcLWH/sAgOaVteITU5NZfjiLaSlwTdtG/x7SX4gSni6UMLTJeO5lwutJvzKhr/+5vPmNQsuMfFekYrAP8zAwIB69epRr149xowZQ69evRg3bly+KwKlS5fm3r177N69m/3799O2bVvq1q37Tvvsv+rleIKdO3fi7Kw5M0F2J/eTJ0/W6FoEMG7cOFp3/l+mWFMzcxQKbaIiIzSWR0aGY2H55sHSWzf+zuYNqxk76UfcPTJOmgOvXiQqKoJ+3duol6lUqSxf9DM7t27glyXr3lhuVkxMLVEotImJ0rzyEhMVhpnF2w1E1tHVxc6xEACFvAJ4cOcKf+5aRce+Y/NclpmZGQqFgsiI1/dnBJY5DD7ftHE9G9av5btJU/DwyLhqpVQqWb5sCd+MHke58undTzw8PLl35w6bNm3IV0XA2NQChUKb2DzsT1MLm0z7PzYqTONKuItnEYZM3kRCfAypKcmYmFkxe0x7XD2L5Dq3m4/TeByWcfKv86JOYGwAsa9cvTc20CIkIv9nSLEJ76Zc9XczUnPfREeFYf6W381XGRmbYe9YiOchuWtZet37/JmfPB/N9TsZ3Xd0ddIvIFia62i0Clia6XDnYWKm9V+VkppG8LP0E/rbDxLw9TDkk3o2zM1mVqCX23W0z31FwMLUBG2FgvCoaI3l4VHRWFuYZ7ueQqFQzwDk61GI+4+CWb55N6WLpM8QVtjLneXTxxEbF09ySiqW5qb0HDWJwl7uucrrdZbGRmgrtAiLjtNYHhYTh002g1tfp6utTWEXe4Keax7TXlYCgsOjWDCw47/WGgDpV//17TW/s/r2NiRHxaBKVJIUGoEqJQV9O+vXYqxRhmi2JOSWpUk2+zI6FpvX+v1nR1dbGz9XB4LyOVj5fSEtAu+WjBH4lwUEBBAXF5dz4BuYmZnRrl07FixYwNq1a9m4cSPh4Zn/sf39/bl48aLG9o4dO4ZCocDPz0+97OLFiyQkJKifnzx5EhMTE1xdXQkICEBfX5+HDx/i7e2t8XB1dc0yv1GjRhEVFaXxGDVqVJaxurq6eHr7cvlCxkBflUrF5Qt/41c4+x/yLRtWs3HNckZ/Ow1vH81pMWvUbsCMeUuYPneR+mFlbUPzVu0ZPTH7GVLeREdXF1dPf25czhg0p1KpuHH5FJ757M+fnTSVSt33Oa90dXXx9vbh4sUL6mUqlYqLFy5QuLB/tuttWL+ONb+vYsLE7/Hx9dV4LTU1hZSUlEytKwptBWmqvA9yBNDR0cPZI4BbV09q5Hn76incfLLen24+Jbl15aTGspuXT+DmUzJTrKGRKSZmVjwPfsCju1cpUqZ2rnNLSoGI2IzH82iISUjDwyHj/evpgLM1PA7N/w9SZNy7KVdHV5dCXv4EXj6tXqZSqbh+6TSevvnvhvi6xIR4nj99hLll/ioX7/NnnpCoIvhZkvrx8ImS8MhkSgZknGAZGSjw8zLi+p28Hb+1tLTQ1c2+ZdKrUHofsfDIlGxjXqerq4OfpxtnL2eM+1GpVJy9fJ2ivrnveqJKSyMpOTnTchNjIyzNTQkKfsr1O/epXq5krsvUyFNHG39XB07dvP9KnmmcuvmA4tlMefm6VJWKW0+eY2Oe8Vm8rAQ8fB7B/AEdsDA2zFd++RV58gLWtTXHrtjUqUzEyQsApCUnE/X3VWxqvzKjmpYW1rUqEXnyPPmhq6ONfyFHTt/I6EanUqVx+sZ9invmfl/efvwMm3xOsyr+m6RF4B8SFhZGmzZt+OyzzyhevDimpqacPXuWqVOn8sknn+S73B9//BFHR0dKlSqFQqFg/fr1ODg4YGFhkSm2U6dOjBs3jm7dujF+/HieP3/OF198QZcuXdTdgiC9m0/Pnj0ZPXo09+/fZ9y4cQwYMACFQoGpqSnDhg3jyy+/RKVSUbVqVaKiojh27BhmZmZ069Yt03b19fXz1BWoWcu2zPtxMl4+fnj7+rNz63qUiQnUqtcYgDkzJmFtbUOn7n0B2Lx+FWtXLmbwiDHY2jkQEZ5+xdDA0BBDQyNMzcwxNdO8KqatrYOFpRXOLoVyndfr6jTtyvKfRlPIKwB372Ic3LkSpTKBirVaALBs7tdYWNnzSadBQPogzuBHdwBITUkmMuwZQfeuo29gpG4B2LpqNgGlqmBl40hiQhxnj+7m1rWzfP7Nr/nOs0XL1sz8cRo+Pj74+hZm69ZNJCoTqVsvvel8xvSpWFtb071HTwA2rF/LyhXLGT7iK+zt7Il4UalM35+GGBkZU7RYcRYvXoCevj52dnZcuXyZgwf206t333znWaNxN9b8+jUunkUo5JU+lWRSYgLlaqR3O/n951GYW9nRuP2XAFRr2JmfJ3bn0M6lBJSszvkTu3l09wqf9hqvLvPiyb0Ym1liae1IcNAtti6fTNGytfErXiXfeQKcvqGiahEF4TEqImPTqFlcQUwCXH9lRqDOtRRcf5TG2Vvpy3R1wOqVC3UWJmBvAQlJEB2f+3Jzo26zLiydOwZ3rwDcfYpyYMcqkpQJVK6dfqxZMmc0FlZ2tOw8END8bqakpGT53dyw7EeKl62Ola0jUeHP2b72FxQKbcpVbZifXQh8WJ/5lj9Cad/MjschSTwNTaJLK3vCIpI5/nfGVfjJIzw4fi6a7QfSj0HdP3Xg7KUYnoUnYWSgTc2KFhQvbMzoGc8AcLTVo2YlC85cjCE6LgUPF0P6dnTk8vVY7j96c0vD6zo0rcfEnxZT2MuNIt4erNm5n0Slkqa10t/3hLmLsLWyoH+n9Flqlm3ehb+nG84OdiQnJ3P8/GX2HDnJiN6d1GUeOHEWSzMT7G2sufPwETOXrKF6+VJUKJH71pXXdalVnjErd1CkkCNF3RxZeegsCcokWlRMr6R+s3w7dhamDHrRVeXX3Ucp7u5MIVtLYhISWbr/FMER0bSqlF5ZTE5NZdiizQQGPWVu309RpakIjU5vuTY3MkRXJ+9T3GobG2HsnfHbYOThglmJwiSFR5EYFIzfd0MwcLbnYo+RADz4bQ1u/TtRePJwgpZuxKZWRRzbNOJM84zj4b1ZSyixeAqR564QdeYS7gO7oWNsSNCyrGfIyo0udSowZtk2Ago5UtTdmVUHT5GgTOaTF/tm9NKt2FmYMrBFeiV4/s4jFPNwppCtFTEJiSzbd4Lg8ChaVimpLjMqLoHg8CieR6XvwwcvxhvYmJloVL7Ef5dUBP4hJiYmVKhQgZkzZ3Lnzh2Sk5NxdXWld+/efP311/ku19TUlKlTp3Lr1i20tbUpV64cu3btQqHI3LhjZGTE3r17GTRoEOXKlcPIyIjWrVvz448/asTVqVMHHx8fqlevjlKppEOHDowfP179+sSJE7G1tWXy5MncvXsXCwsLSpcu/Vbv41VVqtchOiqSNSsXExkRjrunN998O13dNSj0+VMUr1yN/mPXVlJSkpn+vWbXmTYdu9Ou02fvJKeslKnSkJjoCHas/ZmYyFCc3f34/JtfMHvRTSEiNAQtrYzPISriGT+MyLiB2YHtyziwfRk+AWUZPGExADFR4SyfN5roiOcYGJng7ObL59/8in+J/M3ND1C9Rk2ioqNYuWI5EREReHp68u23k7B8MYD4+fNnKBQZ+3PXzh2kpCQz+fuJGuV06NiZTp27AjBy5NcsW7qY6dN+IDYmBjs7O7p07U6jxvm/oVjJSo2IjQ5n74Z5xESG4uRWmF5fzcfUPP2Kc0RYMFqv5OnuW4pOn09lz/o57F47CxsHN7oPmYujq486JjryOdtWTiU2KhRTS1vKVm1O3Vb98p3jS8cD09DVSaNJOQUGevDweRqrD2nO9W9pooWRPvCib7qTlRZd62SclNQvnf73xbsqtp1S5brc3ChXpQGxURFsW/ML0ZGhuHj4MXD0z+rvZnhosEaLTmTEM74b1l79fN+25ezbthzfImUY+u0iACLCnrJw5ijiYiIxMbPE278UX01ejmkW9yXIrQ/pM1+/6zkG+goG9nDGxEibqzfjGDPjnsY9BBzt9DEzzfgZtTDTYVgfV6zMdYhLUHEvKIHRM+5x/mr6SVZyahqlAkxoUd8GA30Fz8OSOXo2ijXbnuU5v7pVyhMRHcvCtVsJi4zGx92Vmd8MxupF16CnoWEax83ERCXTFq7iWVgE+nq6uDk7Mv6LntStUl4dExYRyZxlawmPjMbG0pyGNSrzWeu3u2lgwzL+RMTG8/POvwiNicPP2Y6f+7dTD3oNiYjWyDMmPpFvf99NaEwcZoYGBLg6sOzLzng5pn9HnkXGcOhy+ixGbacs0djWwoEdKOeT9xvJmZcpSqUDK9TPA6an/7YFLd/EpZ6j0He0xdDVUf16wv1HnGnel4AZo3D/oiuJj0K43He0+h4CAMHrd6Nna4XvuIHpNxS7GMjppr1Iepb1wN7caFC2CBGx8fyy4zCh0XH4udjz8xcd1FOCBodHafyfR8cnMnHVTkKj4zAzMsC/kCPLhnfHyzHjfjuHLt3UuOnYyEXpN0Xr26Qa/2v6dpOB/FNU/9FpPAuKVtp/dWJUkSvdu3cnMjKSLVu2/OPbunz7ac5BBayYtz37L7276V7/CXWL63PrTu5nGCkoPl5ubD+X++4OBaVZGR0m/v7+5zmmgw6HriTkHFiAahY1/GA+80bd325q1X/D7qXFCb/0V0GnkSOr4tVI/GNJzoEFyKB+D3bq+uUcWMCaJN8g4eCKnAMLmGHtLgW27e7jC+5cYun4t7vL9vtIWgSEEEIIIcQHQQYLv1syWFgIIYQQQoiPkLQIfOSWLl1a0CkIIYQQQogCIBUBIYQQQgjxQZChre+WdA0SQgghhBDiIyQtAkIIIYQQ4oOgksHC75S0CAghhBBCCPERkoqAEEIIIYQQHyHpGiSEEEIIIT4Ich+Bd0taBIQQQgghhPgISYuAEEIIIYT4IMj0oe+WtAgIIYQQQgjxEZIWASGEEEII8UFIU6kKOoX/FGkREEIIIYQQ4iMkFQEhhBBCCCE+QtI1SAghhBBCfBDkzsLvlrQICCGEEEII8RGSFgEhhBBCCPFBkOlD3y1pERBCCCGEEOIjJBUBIYQQQgghPkLSNUgIIYQQQnwQ0mSw8DullSadrYQQQgghxAegzZf3Cmzb62d6FNi2/ynSIiD+NdfvPCroFHJU2MuFB7dvFHQab+Tm7UfciS0FnUaOjCu14I+LSQWdRo7ql9Dj8+mRBZ1Gjn4aZsH5W6EFncYblfKxYfPp1IJOI0cty2tTtdnhgk4jR0e31yD+yLqCTiNHRtXbEnHx/d6fliVqkHBwRUGnkSPD2l3+3959R0V1tHEc/y5KU0SKDZUOggUVY6+xxF4xNuwtMRo1dmNiiRpbNPaY2Es09qhRY2+xiyJ2EVAhChYQEJB+3z94XV3BGuPdDc/nHM5xZ+8uP4HdvXNnnhl2GHuoHeO1mqSo9zkpIwLvl9QICCGEEEIIkQ1JR0AIIYQQQohsSKYGCSGEEEIIg5CupKsd4T9FRgSEEEIIIYTIhmREQAghhBBCGAQpFn6/ZERACCGEEEKIbEhGBIQQQgghhEGQEYH3S0YEhBBCCCGEyIakIyCEEEIIIUQ2JFODhBBCCCGEQVAUmRr0PsmIgBBCCCGEENmQjAgIIYQQQgiDkJ4uG4q9TzIiIIQQQgghRDYkHQEhhBBCCCGyIZkaJIQQQgghDILsI/B+yYiAEEIIIYQQ2ZCMCAghhBBCCIOgKFIs/D7JiIAQQgghhBDZkIwI6LGPP/6YsmXLMmvWLLWjAP9unh1/bGHLpvU8ehSFk7Mrn33Rn2Ienlkeu2fXDg7u38Pt27cAcHUrRueuPTMdHxZ6mxXLFnH54gXS0tKwd3Bk5DdjyV+g4Dvn3LZ9Bxs2/U7Uo0e4ODvTr89neHoUy/LYo8eO89v6jdwNDyc1NZUihQvzqU9L6tWprXPM9j93cSMomMePH7NgzixcXV3eOd9T6/YdZ+WfR4iMeUwxBzuGd2pBKRf71z5u98nzfP3zb3zsXYIfB3bVuS/k7j3mrP+Tc9dDSE1Lx6VIQX74shN2ttbvnPPIrt/Y/8dyYqMfUsTRg097fI2Tm1eWx4aHBbFj3XzCbl4h6sFdfLoOp3aTzjrH7Pl9MQGn93Hvzk2MTcxwLlaGFp0GUbCw8ztnfKpJNTOqeZlgbqoh5G4qa/c+4UH0y69M1a9oStlixhS0yUFKqkLInTS2HHnC/UfPHpMzB/h8bM5HnsYY59Bw5VYK6/Y94XHC28+B3b19E39sXkPMoygcnN3o/vkg3DxKZHns/l3bOHLgT/6+fRMAZzcP2nf5XHt8amoq61Yt5LzfCe5H3CVX7tyUKlOBDt36YGOb/62zPe/E3jUc3rmUuJiH2Nl70LzLN9i7ln7p8RdO7WLvprk8engH24KONGo3GM+ytbT3j+yc9f+xUfsh1GrS8x9l7dnRiWb1C5End04uXo1l+k83+Dv8yRs9ttOn9vTp6sL6rX8zZ3Gwtr15Azs+qVWAYq4W5M6Vk4btjxIXn/ZO+dYdPMWK3UeJjImjmH0hRnRoQinnoq993K7TF/h60QY+LuvJzH4dte0JiUnM2byXg/5XiYlPoHA+azrUqUybjyu+U76nNu46yK9/7CEqOgY3x6IM6dGBkm6vf03uPXaa0bMXU7N8GaYN76dtVxSFReu3sXX/X8TFP8HL05XhvTriYPfu7+0Aaw/5sWLvCSJj4yhWtCAj2jXAy6lIlsduPRHA2JV/6LSZ5MzB6blfa2/v97/Ghr/OcjU0gpj4J6wd1QtP+0LvnM+menlchvQkb7lSmBUugF/rvtzbtv/Vj6lZkRLTR2JRwp3EsHCCJi/g75W/6xzj+IUvLoN7YlooP7EXrnH5qwnEnLn4zjmFYZERgf+45ORktSO81l+HD7J00c+08+3Cj3N/xtnFlXGjRxAd/SjL4y9eCKBGrTpMnDyDaTPmki9ffsZ9O5zIhw+0x4SH3+XrYQMpWtSe76fOYPZPi2jboRPGJibvnPPQkb/4ZdESOvm256c5M3FxdmLU6LE8io7O8vg8efLQoV0bZk+fxi/z59Dgk7pMnzkbv7PntMckJiVRqkQJenXvmuVzvIvdpwL4ce12PmtZlzXfDcDd3o5+05cQFRv3ysfdfRDFzHU78C6W+QM67H4kPb//GSe7Aiwc+TnrJg6id/O6mBobv3POs8d38fvKH2j0aR+GT11PEcdi/PT95zyOiczy+OSkRPIVLEpz36+wtMqX5TFBV/yo0aA9Q75fTb9vF5KWlsr8iZ+TlJjwzjkBPqloysfepqzdm8APqx+TnAJffpqbnDle/hh3+5wc8U9m+urHzN0QR44c0L+NBSbP/cg+rW2Ol6sxS7YlMHNdHHktjOjdIvdb5zt+ZB+rFs/l0w49mDx7KY7ObkweM5iYl7yGrlw8R7VanzB68hzGT/8F2/wFmDRmEFH/fw0lJyVyK/g6Pu27MXn2UgaPmsTdO6FMnzDirbM9L+Dkn2xfM5V6rfrSf8JG7Bw8WTLtM+Je8ju/HejP2p+GUb6WDwMmbKLkR3VZNas/EWE3tMd8M/ewztenvSei0WgoVaH+P8rasbU9nzYtwvSfbvDZUH+eJKbx43gvTIw1r32sp3semje0I+hm5tecqakRp85FsWpD6D/Kt/vMRWas/5PPm9VmzegvKFa0EH1nrXj96/zhI2Zu2I23u2Om+2as38XxSzf4vtenbB4/gI71qjD1tx0cOn/1nXPuPX6G2Ss30OvTpqyY+i3ujvZ89f1somJiX53z/kPmrNpI2eLume5btXU36/88wIjenVg86WvMTU356vvZJCWnvHPO3X6XmbFpL583qcFvo3pRrGhB+s75jajY+Jc+xsLMlH1TvtJ+/fl9f537nyQn4+1qz8CWdd451/Ny5M5F7IXrXBrw3Rsdb+5UlArbfiHy0CmOlm/Bzbkr8PplIvk+qa49xq5NI4r/8DU3Js7naMVWPL5wjUo7lmCS3+a9ZP43KOmKal//RdIR0FPdunXj8OHDzJ49G41Gg0ajITg4mJ49e+Ls7Iy5uTkeHh7Mnj070+NatmzJ999/T+HChfHw8ADg+PHjlC1bFjMzM8qXL8+WLVvQaDScP39e+9hLly7RqFEjLCwsKFiwIJ07d+bhw4cvzXPr1q338n/d+vtG6jdsTL36DXFwcOKLL7/C1NSUfXt2ZXn8kOGjaNy0BS6ubhS1d+DLgUNIT1cICPDXHvPriiV8VL4S3Xp+jourO3Z2halUuSpWVu9+9XrT71tp1LA+DT6ph6ODAwO/7IupmSm79+zL8vgypb2oXrUKDg72FLazo1WL5rg4O3HpyhXtMfXq1KaTb3u8y5Z551wvWr37L1rVqkiLGhVwKVKQb7q2wszEmK1Hzrz0MWnp6Xzzy1r6tPyEoll8AMzfuItqpT34ql1jPB2LYF/AllreJbCxtHjnnAe3r6RK3dZUrt0Ku6KutOs9BhMTc04c/D3L4x3dStGy8xA+qtaInMZZd+j6fvMzlT9uiZ29G0WdPOjUbyKPHoYTFnIly+PfVO1ypuw6mciF4FTuPkxnxc548loYUcbt5R2h+ZviOXk5mfDIdO48SGfVnwnYWBrhUDCj92BmAlW8TNh86AmBYamE3Uvj110JuBbJiZPdK3oYWdixZR11GjTj40+aUNTBmV79hmFiasqhvduzPL7/sHHUb+KDk0sxitg78nn/kSjp6VwK8AMgV24Lvpk4myo16lK4qCPunqXo0WcwIUHXeXg/4q2yPe/on8up+HEbytf0oWARN1p2H4uJqRl+RzZnefyxPasoVro6tZr0pEARV+p/OoDCTiU4sW+19pg8Vvl1vq6cPYBL8YrYFnj9CNirtGlehJXrb3P0VCTBt+KZOPMatjam1KicdSf0KXMzI8YO8WTa3EAex6Vmun/Dtjv8ujGMy9defSL8Or/uPY5PjfK0qFYO18IF+KZTM8xMjNly7NxLH5OWns6oxRvp07wORfNlfp0HBIfStGpZyns4UzifNa1rVqBY0UJcvnnnnXP+tn0vLepWp2ntajgXLcyI3h0xMzFh+8Fjr8w5du4SerdtTuECuj9vRVFYt3Mf3X2aULNCWdwdizL2y+48fBTNkTP+L3nG11u1/xQ+1bxpWbUsrnb5+bZD44yf54nzL3+QBvLltdB+2b7wfti0Umk+b1KTSsX/+YgkwIPdRwgcO4t7W7P+zHmR42fteXLzb64On0rctRBu/7SaiE27cR7YTXuM81fdCVuynr9XbCbuajAX+44lLSER+26t30tmof+kI6CnZs+eTZUqVejduzfh4eGEh4dTtGhRihYtyoYNG7hy5Qpjxoxh1KhRrF+/Xuex+/fv5/r16+zdu5ft27cTGxtLs2bN8PLy4ty5c0yYMIERI3Sv7EVHR1OnTh28vb3x8/Nj165d3Lt3j7Zt2740j739P/ugBUhJSSE4KJAyZctp24yMjChTthzXr73ZyVtSUhJpaankscgDZOw66HfmFIWLFGXstyPo0qE1Q7/qx8njR/9RzhtBQXiXLauT07tsGa5eu/baxyuKgv/5AML+voNXqZLvnOO1OVNTuXrrDpVKPLuKZmRkRKWSblwIfvkVyIVb92FjaUHLWpmnAKSnp3P0wjUcC+Wj7/TF1O0/ni7j53Hw7OV3zpmamkJYyBU8vCrr5PTwqsytwIB3ft4XJSZkXB3NZZH3nZ/DNq8ReS2MuH772UldYjLcCk/DufCbz640N824khyfmHFVyaFgTnLm0HDtuee9F5VOVGz6Wz1vakoKN4Ou41W2grbNyMgIr7LlCbx26Y2eIykpkdS0VHLnsXzpMQkJcWg0GnL9/3X2tlJTk7lz6wpuJXV/524lq3A76HyWj7kddB63klV02op5VeP2jaz/Rh7HPORawBEq1PpnJzGFC5qRz8aUM+efjajEJ6RxJTCWUp4v/xkBDO7jznG/KPwCov9RhldJSU3l6u27VCr+bBqhkZERlYq7ciE47KWPW/jHQWzy5KZVjY+yvL+MqwOHz1/n/qNYFEXhzLUQbt97SOWSbu+c83pIKBW8iuvkrOBVnIuBIS993NKN27GxzEPzOtUz3Xf3/kMio2OpUPrZc1rkykVJN+dXPuerc6ZxNTScSp7PTtiNjDRU8nTiQsjLO0FPkpJp9M0cGoyazVcL1hN098FLj1WDVeWyPDxwQqftwd6jWFcuC4DG2Ji85UrycP/xZwcoCg8PHMeqsvcHTPp2ZETg/ZIaAT2VN29eTExMyJUrF4UKPZtT+N13z4YEnZ2dOXHiBOvXr9eesAPkzp2bxYsXY/L/aTA///wzGo2GRYsWYWZmRokSJbhz5w69e/fWPmbevHl4e3szadIkbdvSpUuxt7cnMDCQYsWKZZnnn4qNjSE9PR0ra90r9VZW1vwd9vIPtOetXLYIGxtbynhnfLjFREeT+OQJmzaspWOX7nTt3ptzZ88w5ftxTJwyg1Jeb3/1PTY2lvT0dKytrHTara2sCAt7+QdFfHw8Hbp0JyUlBSMjI/r37cNH3v/eG2z04wTS0tOxyat7ZcrGMg+3wrP+kPIPvMnWI2f4bfxXWd4fFRtPQmIyy3Ycom/rBgxs05jjF68zdN4qFo74jI88376mIT72EenpaVha2eq057Gy5d7dm2/9fFlJT09n0/KpuHh4U9gh8/SCN2WZO+MEPjZBtx7gcUK69r7X0QCta5sT/Hcq4Q/Ttc+bkqrwJEn3wyU2/s2fFyA2Npr09DTyWule4c1rZcOdv99s+sma5QuwtsmHV9nyWd6fnJzEmmULqFqzHrlyvf3UJYCExxk5LfLqXuG1sLTlwd2sT+Dioh9ikVf3b8Qibz7iYh5mefy5v7ZiapaLkuU/eaeMT9lYZ7x3PorWnWryKDpZe19W6tbITzFXC3oPfvlV+ffhUdz/X+cvXIG2tbTgVkTWPxv/G7fZcvQca8f0fenzjujQhAmrttJg+A/kzGGERqNhdOcWfFTM6Z1yRsfGZeS00u08WVvl4dbd8Cwfc/7aDbYdOMqqaaOzvD8yOmMkxSavbofUJq+l9r63lfHzVLC11P3btrW04Na9rKetORW0ZVznZrgXKUDckyRW7jtJtx+Ws2nM5xS0fnVn8UMxLZiPpHu6fw9J9x5inDcPRmamGFvnxShnTpLuR75wTCS5Pf55rZowDNIRMDDz589n6dKlhIaG8uTJE5KTkyn73FVqAC8vL20nAOD69euULl0aMzMzbVvFirpXfgMCAjh48CAWFpmnegQHB1OsWNYFsVlJSkoiKSlJp83U1PSNH/82Nq7/jb8OH+T7qTO0/+f0/y8tVqlyVVq0+hQAF1c3rl29zK6df7xTR+BdmZubs2DuLBKfJOIfEMAvi5diV6gQZUpnXRD7ocU/SWL0wnWM7t4a6zxZn+ApSsaJ6sflStKpQQ0APBwLExB0m40HT75TR+BD2LDke8LDgvhq/Iq3elyF4sZ0+CSX9vZPm1895/pNtKtnTuF8Ofjxt8f/+Lnet60bVnH8yD7GTJ6HiUnm12lqaiqzp4xGQaFnv2EqJHxzfkc2U7ZqU4yz+H+8yie1CjCs37P3uOHj375QskA+Uwb2dmPQmAskp+jXlcP4xCS+XbKR0V1avPR1DrD2wEkuhoQx68uO2NlacS7wFlPWbCe/lSWVS7j++zmfJPLd3KV8/XlnrCzfbeTpQynjUpQyLs8Ks8u4FsXnu5/Z+Nc5+jX/WL1g2UC6LB/6XklHwICsXbuWoUOHMmPGDKpUqUKePHn44YcfOHXqlM5xuXO//RW7uLg4mjVrxtSpUzPdZ2dn91bPNXnyZJ2RC4CxY8fSvnOvTMdaWubFyMiI6Ee6RY3R0Y+wtnl1sdLvm9azecNvfPf9Dzg5P/uQsrTMS44cObB30C2Gs7d34MrlN5sqkTmnJUZGRpkKgx9FR2NjbfXSxxkZGVGkcGEAXF1dCA37m7UbNv5rHQGrPLnIYWREVIzuyWtU7GNs82b+YP37fiR3Hz7iq1nPTpbT/3/iX6HH12yeMpRCNnnJmcMIl8IFdB7rXLgA5wNvvVPO3JbWGBnlIDZa90rU4+jITKME72L9ku+5dO4wA79bjrXt241gXQhK4Vb4sxP2pwXBlrmMiH1udZc8uYz4+/7rV3tpW9ecUi7GzFwXR3TcsxPE2HgF45wazE01OqMClrmNiI1/8xNJS0srjIxyEBMdpdMeEx2FlfWrX0N/bF7D1o2/8s3EWTg6Z57+8bQT8OD+PUZPmvPOowEAufJk5Hzxan5cbCQWLyn+trDKl6mQOC7mYaZRBYCb1/14EH6TDv1mvHW2o6cjuRLop71tYpwxa9baypjIR88WXLC2MiEoJOuOoYebBTbWJiyZ9WzaTc4cGsqUzItP0yLU8TlC+ns6f7G2+P/r/IXC4MjYuEzz1AH+vh/F3chovpr3rLbi6eu8/Odj+X3CQPJb5WHu7/v4sW8HapTOqC0rVrQQ18MiWLXn6Dt1BKwsLTJyvnCl/lH0Y2ytMk/Xu3PvAeEPIhk2dX6mnNXa92HdrPHY/n90ISrmMfmee++NionF3endpqtm/Dw1RL5QGBwZG0e+N6yDMs6RAw/7QoQ9iHr9wR9I0r2HmBbUfa2YFsxHSsxj0hOTSH74iPTUVEwL2L5wjC1JLxlZEv890hHQYyYmJqSlPTvROHbsGFWrVqVv32dDu8HBwVk9VIeHhwe//vorSUlJ2ivzZ87oFo6WK1eOTZs24eTkRM6cWf9ZvJjnZb7++msGDx6s02ZqasrNvzNPTTE2NsbVrRgXAvypXDVjPmh6ejoXzvvTuFnLl36PzRvWsmHdGsZNnIJ7MY9Mz+lWzIM7f+tOLbpz528KvOPSocbGxri7uXH+fADVqlTW5jx//gLNmzZ54+dRlHRSUt59ZYvXMc6Zk+JORTh9JYjaH2XUIqSnp3P6ShDt6lbNdLyTXX7WTxyk0/bTpt3EJyYxrGNzCtnkxThnTko4F800tSg04iF2+d6t+DpnTmPsXUoQeOkUZSrW1eYMvHSSGg07vNNzQsboxYalk7hw+gADxi0lX4HXL6X4oqQUMi0LGhOXjodjTv5+kPH3b2YCTnY5+Ot8UlZPodW2rjll3IyZtS6OyBjd5wy9l0pqmoKHQ07O38j4myhgbYSNpRE372YuMn2ZnMbGOLt5cCnAjwpVagIZP8tLAWdp0PTlc+W3bVzN7+tXMGr8j7i6F890/9NOQPjdMMZMnksey3evswDImdOEIk4lCLpykpLl62lzBl0+SdVPfLN8jKNbWYIun6R6wy7athuXTuDonnlU78yhzRRxLklhx6yXHX6VJ0/SuPNE973tYVQS5ctYE3Qz4+Qwl3kOShSzZMvOu1k+h19ANJ376b6vjvrKg9t/P2H1xtD31gmA/7/OHQtz6moItb0zlk9NT0/n9NUQ2tWplOl4J7t8bBj3pU7b/C37SEhMZlj7xhSysSQpJZXUtDQ0Gt1paTmMNNqT8XfJ6eHiwJlL16hV0Vub88ylq7RpWDvT8Y6FC7F6+lidtl/WbiEhMYlB3dpRMJ8NOXPkwNbKkjMXr1Ls/yf+8QlPuBx0E5/6tTI955vlzEFxBztOX79JnbIe/8+pcPr6Ldp/nPV0uRelpacTdOc+1Uu9Wz3FvyH65HnyN6qp05avblUenTwPgJKSQsy5y+SrU+XZMqQaDba1q3D7p18/cFqhFukI6DEnJydOnTrFrVu3sLCwwN3dnZUrV7J7926cnZ1ZtWoVZ86cwdn51SsS+Pr68s033/DZZ58xcuRIQkNDmT59OoD2Tb9fv34sWrSIDh06MHz4cGxsbAgKCmLt2rUsXryYHDlyZMpjY2ODkVHmenNTU9O3mgrUotWnzP5xKm7uxXAv5skfWzeRmJRIvU8aADBz+hRsbfPRpXvGiMKmDb+xZtUKhgwfRYEChXgUlXEFxszcHHNzcwBatW7H9CkTKOlVGq/SZTl39gxnTp3g+6k/vnGuF7Vu1YIffpyFu7sbnsWKsXnrNhITE2nwScaJ7LQZM7G1taFnt4ylQH9bv4Fi7m4ULmRHSkoKp/382HfgEAP6faF9ztjHj3lw/wGR//8/hN3JqDewtrbGxubdTrI7NqjB2EXrKeFclJIuRVmz5yhPklJoXiPjA230wnUUsLakf5tGmJoY41ZU94p5nlwZP8Pn27s0qsXIn9ZQzsOZ8sVdOX4xkCPnr7Jw5GfvlBGgdtMu/Dr/GxxcSuLo5sWhnatISnpC5Y9bArBy3iisbArQ3PcrIKPAOOLvYO2/Y6Lu8/eta5ia5SJ/IQcgYyTg7NGd9B4+GzPz3MRGZ1zVMstlgYmJWaYMb+rguSQaVjbl/qM0ImPSaVrNnJi4dAKCnnXqBrTJTUBQCof9M64gt6tnTnlPE37ZEkdSsoJlrozX2pNkhZTUjILjExeTaV3bnPhEhcRkhbZ1zAm5k8qt8LdbV75Jy3YsmPk9Lu6euBUrwc6t60lKTKRWvYxO6vwZE7CxzUeHbhl/e1s3/sqGXxfTf9hY8he0I/pRxlV3MzNzzMxzkZqayszJ33AzOJARY6aRnp6uPcbCwpKc77hsbPVG3diw8GuKOpfC3sWLo7tXkpz0hI9qtgJg3c8jyWtdgIbtMi4kVKvfmV8mdeXIzmV4lq1FwMmd3Ll5CZ8euiOOiU/iuHh6N01839/UpQ3b7tC1nQNhd58Qfi+RXp2ciIxK4q+Tz66UzppYmiMnHrJ5x12ePEnjZqjuMrWJienExqbotNtYGWNjbUKRwhmvMxdHCxKepHLvQVKWqwy9TKdPqjJm6WZKOBWhlHMR1uw7wZPkZFpUy1h44dslGylgbckAn/qYGhvjVkT3Ikie/79XPm03zpmTj4o5MWvjbsxMjLGzseJs4E22nzjP4LaN3uInp6tD00+YMH8ZxV0cKeHmzLqd+0hMSqbJx9UA+G7eUvLbWNHX1wdTE2NcHXTX7bfInTFN7/n2do3rsXzzTuztClC4QD4Wrt1KPmsralZ49/qrznUrMXrFNko42FHKqQirD5ziSVIKLapkdDq/Xb6VAlZ5GPD/pUB/2XEEL+ciOOS34fGTRFbsPUF4VAytqpXVPmdM/BPCo2J48P8R2tv/rzfIZ5mxytDbypE7F7ndHLS3czkXxbKMJ8lRMSSGheMxcTBmRQoS0D1jMZDbC9fi2LcjnpOHEbZ8E/lqV8auTSPONP9c+xw3Zy2jzNKpRJ+9RMyZCzgN6ErO3OaErch6JS998F8t2lWLdAT02NChQ+natSslSpTgyZMnXLt2DX9/f9q1a4dGo6FDhw707duXP//885XPY2lpyR9//MEXX3xB2bJl8fLyYsyYMfj6+mrrBgoXLsyxY8cYMWIE9evXJykpCUdHRxo2bKg92X8xz82bN3FycvrH/88atWoTGxvDmlXLefToEc4urowdP0U7reHhg/sYGT27SrVrxx+kpqYwdZLuyUB73y506JRxEl6lanW++PIrNq7/jUU/z6NIUXtGfjOOEiXffUrOxzVrEBMTw8pf1/Do0SNcXFz4fvw4rP9f6Hz/wQOdq2mJiUnM/elnHj6MxNTEBPuiRRkxdDAf16yhPebkydNMn/VsCdhJU38AoJNve7p0zPoq6es0qFSGR4/jWfD7HiJjHuPhUJh5Q3popwZFREZjpHnzYlSAOh+VYlTXVizbcZAfVm/DsVB+fviyU5Z7Drypj6o2JC42ih3r5/M4+iFFnDzpO+pn7R4Bjx6G6/w8Y6LuM3V4G+3t/X8sZ/8fy3ErUZ6B45YBcHTPOgDmjOuh87069p2g7WC8i72nkzAx1uBbPxfmphqC76Qyf1M8qc+dr+ezykFu82cNNctmdIYHtdedkrXqzwROXs7oLGw8+IR0BXo3z0XOnBqu3szYUOxtVa1Zj9iYaDb8upjoR1E4urgzcvyM515D99A89xrau/N3UlNTmDn5W53nad2hB2069iQq8gFnT2WssjViQDedY0ZPmkvJ0uV4F2UqNyL+cRR7N83lccxDCjt40mPYL+T5/1Sf6MhwNJpnFxcci3nT/otp7Nk4h90bZpGvoCOdv5pLIXvd4u+AEzsBhbJV3nx07nVWbwrDzCwHw78shkXunFy8EsOQsRd15v8XKWSOleXbdYpaNipMD18n7e2fppYF4PtZ1/hz/703fp4GFbwyXudb9xMZG4eHvR3zB3bRTg2KiIrBSPN2CwNO+awtczfvZdTiDcTGP8HO1op+LevRplaF1z/4JT6pWoHo2McsWr+NyOhY3J2KMnPUAO0Un4iHUZlGIV6nc4sGJCYlMeWXX4lLSKC0pxuzRg3E1OTd9zVpUL4kj+ISWLD9MA9j4/EoWpCf+nfQ/jzDo2J0csYmJDJh9Q4exsZjmcuM4g52rBjWDVe7ZxvuHboQqLPp2IglGUsjf96kBl80ffvRi7wflaLK/lXa2yWmjwIgbOVmLvT8GlO7/JjbP5vK++TW35xp/jklZnyNU/8uJP4dwcXPv+Xh3mcr6IVv+BOT/DYUGzsgY0OxgKucbtqL5PtZF0mL/x6NorzjmJ8waKtXr6Z79+7ExMRor6L/264F//1Bvs8/4elalNtB19WO8UqObh7En9iidozXyl2lJXsC9H9Du/plTOg3PVrtGK81f6gV/jf0e96ut3s+fj/9brvkfkitKuagerPDasd4raN/1CLhyPrXH6iyXDXb8ihAv3+e1mVq8eTAqtcfqDLzOp3ZYezx+gNV1iRFvc/JTzqeVe17712d9dK7hkxGBLKJlStX4uLiQpEiRQgICGDEiBG0bdv2g3UChBBCCCGEfpGOQDYRERHBmDFjiIiIwM7OjjZt2vD999+rHUsIIYQQQqhEOgLZxPDhwxk+fLjaMYQQQggh3pkUC79fb1dJJIQQQgghhPhPkBEBIYQQQghhEBTZWfi9khEBIYQQQgghsiEZERBCCCGEEAYhXWoE3isZERBCCCGEECIbko6AEEIIIYQQ2ZBMDRJCCCGEEAZBSZdi4fdJRgSEEEIIIYTIhqQjIIQQQgghDIKSrqj29W+JioqiY8eOWFpaYmVlRc+ePYmLi3uzn4ei0KhRIzQaDVu2bHnr7y0dASGEEEIIIVTSsWNHLl++zN69e9m+fTtHjhzhs88+e6PHzpo1C41G887fW2oEhBBCCCGEUMHVq1fZtWsXZ86coXz58gDMnTuXxo0bM336dAoXLvzSx54/f54ZM2bg5+eHnZ3dO31/6QgIIYQQQgiDoObOwklJSSQlJem0mZqaYmpq+s7PeeLECaysrLSdAIB69ephZGTEqVOnaNWqVZaPS0hIwNfXl/nz51OoUKF3/v4yNUgIIYQQQojXmDx5Mnnz5tX5mjx58j96zoiICAoUKKDTljNnTmxsbIiIiHjp4wYNGkTVqlVp0aLFP/r+MiIghBBCCCEMwr9ZtPs6X3/9NYMHD9Zpe9lowMiRI5k6deorn+/q1avvlGPbtm0cOHAAf3//d3r886QjIIQQQgghxGu8zTSgIUOG0K1bt1ce4+LiQqFChbh//75Oe2pqKlFRUS+d8nPgwAGCg4OxsrLSaW/dujU1atTg0KFDb5QRpCMghBBCCCHEe5U/f37y58//2uOqVKlCdHQ0Z8+e5aOPPgIyTvTT09OpVKlSlo8ZOXIkvXr10mnz8vJi5syZNGvW7K1ySkdACCGEEEIYhP/azsLFixenYcOG9O7dm59//pmUlBS+/PJL2rdvr10x6M6dO9StW5eVK1dSsWJFChUqlOVogYODA87Ozm/1/aVYWAghhBBCCJWsXr0aT09P6tatS+PGjalevToLFy7U3p+SksL169dJSEh4/99cEcIAJSYmKmPHjlUSExPVjvJKhpDTEDIqiuR83wwhpyFkVBTJ+T4ZQkZFkZziv0OjKIp65ddCvKPY2Fjy5s1LTEwMlpaWasd5KUPIaQgZQXK+b4aQ0xAyguR8nwwhI0hO8d8hU4OEEEIIIYTIhqQjIIQQQgghRDYkHQEhhBBCCCGyIekICINkamrK2LFj33hjD7UYQk5DyAiS830zhJyGkBEk5/tkCBlBcor/DikWFkIIIYQQIhuSEQEhhBBCCCGyIekICCGEEEIIkQ1JR0AIIYQQQohsSDoCQgjxH5eWlsaRI0eIjo5WO4oQQgg9Ih0BIT4QfarL/+uvv+jUqRNVqlThzp07AKxatYqjR4+qnOzV5ET23eTIkYP69evz6NEjtaO80oULF7L8unjxIjdu3CApKUntiFqrVq2iWrVqFC5cmNu3bwMwa9Ystm7dqnKy/5YnT56oHUHLEN43Q0NDs/ysURSF0NBQFRIJfScdAWFwgoKC2L17t/YDQp9OsLt160Z8fHym9lu3blGzZk0VEmW2adMmGjRogLm5Of7+/tqTq5iYGCZNmqRyumemTp3KunXrtLfbtm2Lra0tRYoUISAgQMVkhqlUqVKEhISoHeOVypYti7e3d6avsmXL4unpSd68eenatSuJiYmq5lywYAGDBw+mcePGREdHk5aWBoCVlRWzZs1SNduLNm7cSNu2balcuTLlypXT+dIXAwYMyLI9Pj6exo0bf+A0WTOU901nZ2cePHiQqT0qKgpnZ2cVEgl9Jx0BYTAiIyOpV68exYoVo3HjxoSHhwPQs2dPhgwZonK6DAEBAZQuXZoTJ05o21asWEGZMmXIly+fismemThxIj///DOLFi3C2NhY216tWjXOnTunYjJdP//8M/b29gDs3buXvXv38ueff9KoUSOGDRumcjqwtrbGxsbmjb70wcSJExk6dCjbt28nPDyc2NhYnS998Pvvv+Pu7s7ChQs5f/4858+fZ+HChXh4eLBmzRqWLFnCgQMH+Pbbb1XNOXfuXBYtWsQ333xDjhw5tO3ly5fn4sWLKibTNWfOHLp3707BggXx9/enYsWK2NraEhISQqNGjdSOp7Vjxw7Gjh2r0xYfH0/Dhg1JTU1VKZUuQ3nfVBQFjUaTqT0uLg4zMzMVEgl9l1PtAEK8qUGDBpEzZ05CQ0MpXry4tr1du3YMHjyYGTNmqJguw+nTpxk1ahQff/wxQ4YMISgoiD///JMff/yR3r17qx0PgOvXr2c5OpE3b169mnoTERGh7Qhs376dtm3bUr9+fZycnKhUqZLK6dC7K7+v8/TKavPmzXVOFJ6eODy9qq2m77//ntmzZ9OgQQNtm5eXF0WLFmX06NGcPn2a3LlzM2TIEKZPn65azps3b+Lt7Z2p3dTUNMsRQbX89NNPLFy4kA4dOrB8+XKGDx+Oi4sLY8aMISoqSu14Wnv27KFGjRpYW1vz1Vdf8fjxYxo0aEDOnDn5888/1Y4H6P/75uDBgwHQaDSMHj2aXLlyae9LS0vj1KlTlC1bVqV0Qp9JR0AYjD179rB7926KFi2q0+7u7q6do6s2Y2NjfvjhB3LlysWECRPImTMnhw8fpkqVKmpH0ypUqBBBQUE4OTnptB89ehQXFxd1QmXB2tqasLAw7O3t2bVrFxMnTgQyTlz14aS1a9euakd4KwcPHlQ7wmtdvHgRR0fHTO2Ojo7aK+1ly5bVjgaqxdnZmfPnz2fKumvXLp2LFGoLDQ2latWqAJibm/P48WMAOnfuTOXKlZk3b56a8bRcXV3ZtWsXtWvXxsjIiN9++w1TU1N27NhB7ty51Y4H6P/7pr+/P5Dx/njx4kVMTEy095mYmFCmTBmGDh2qVjyhx6QjIAxGfHy8zlWOp6KiovRm+/SUlBRGjhzJ/Pnz+frrrzl69Cg+Pj4sWbJEb+a69u7dm4EDB7J06VI0Gg13797lxIkTDB06lNGjR6sdT8vHxwdfX1/c3d2JjIzUTmXw9/fHzc1N5XSZpaWlsWXLFq5evQpAyZIlad68uc7UETXVqlVL7Qiv5enpyZQpU1i4cKH2RCYlJYUpU6bg6ekJwJ07dyhYsKCaMRk8eDD9+vUjMTERRVE4ffo0v/32G5MnT2bx4sWqZnteoUKFiIqKwtHREQcHB06ePEmZMmW4efOmXtVWAZQuXZrt27fzySefUKlSJbZv3465ubnasbT0/X3zaUe/e/fuzJ49G0tLS5UTCUMhHQFhMGrUqMHKlSuZMGECkDEEmp6ezrRp06hdu7bK6TKUL1+ehIQEDh06ROXKlVEUhWnTpuHj40OPHj346aef1I7IyJEjSU9Pp27duiQkJFCzZk1MTU0ZOnQo/fv3Vzue1syZM3FyciIsLIxp06ZhYWEBQHh4OH379lU5na6goCAaN27MnTt38PDwAGDy5MnY29uzY8cOXF1dVU74TEJCAqGhoSQnJ+u0ly5dWqVEz8yfP5/mzZtTtGhRbZ6LFy+SlpbG9u3bAQgJCVH999+rVy/Mzc359ttvSUhIwNfXl8KFCzN79mzat2+varbn1alTh23btuHt7U337t0ZNGgQGzduxM/PDx8fH1WzeXt7ZzmX3dTUlLt371KtWjVtmz7MwTeU981ly5YBGe9JwcHB1KxZE3Nz85fWDgihUfTtsoAQL3Hp0iXq1q1LuXLlOHDgAM2bN+fy5ctERUVx7NgxvTjZ6tmzJ3PmzMk0nO3v70/nzp25dOmSSskyS05OJigoiLi4OEqUKKE90RZvr3HjxiiKwurVq7XFwZGRkXTq1AkjIyN27NihckJ48OAB3bt3f+mca32YbgXw+PFjVq9eTWBgIAAeHh74+vqSJ08elZNlLSEhgbi4OAoUKKB2lEzS09NJT08nZ86Ma35r167l+PHjuLu78/nnn+tMH/nQvvvuuzc+9sVCYjXp+/tmVFQUbdq04eDBg2g0Gm7cuIGLiws9evTA2tpaL2rphH6RjoAwKDExMcybN4+AgADi4uIoV64c/fr1w87OTu1or5WUlKQ3U5ieCgsLA9AW5Yp3kzt3bk6ePImXl5dOe0BAANWqVSMuLk6lZM907NiR27dvM2vWLD7++GN+//137t27x8SJE5kxYwZNmjRRO6LBqFOnDps3b8bKykqnPTY2lpYtW3LgwAF1gokPQp/fN7t06cL9+/dZvHgxxYsXJyAgABcXF3bv3s3gwYO5fPmy2hGFnpGpQcKg5M2bl2+++UbtGK+0atUqfv75Z27evMmJEydwdHRk1qxZODs706JFC7XjkZqaynfffcecOXO0J6gWFhb079+fsWPH6iyNJ96MqampthDzeXFxcapedX3egQMH2Lp1K+XLl8fIyAhHR0c++eQTLC0tmTx5sl50BCZPnkzBggXp0aOHTvvSpUt58OABI0aMUCmZrkOHDmWaWgWQmJjIX3/9pUKil4uOjub06dPcv3+f9PR0nfu6dOmiUirDYyjvm4awqIbQL9IREAbjwoULWbZrNBrMzMxwcHBQ/Yr7ggULGDNmDF999RXff/99po2G9KEj0L9/fzZv3sy0adO0qxmdOHGCcePGERkZyYIFC1ROaHiaNm3KZ599xpIlS6hYsSIAp06dok+fPjRv3lzldBni4+O101esra158OABxYoVw8vLSy/mYAP88ssvrFmzJlN7yZIlad++veodgeffg65cuUJERIT2dlpaGrt27aJIkSJqRMvSH3/8QceOHYmLi8PS0lJnjrhGo9GbjkBaWhozZ85k/fr1Wdav6MNSp4byvmkIi2oIPaMIYSA0Go1iZGSkGBkZKRqNRue2kZGRYmpqqnTp0kV58uSJahmLFy+u/P7774qiKIqFhYUSHBysKIqiXLx4UbG1tVUt1/MsLS2VnTt3ZmrfsWOHYmlpqUIiw/fo0SOlefPmikajUUxMTBQTExPFyMhIadmypRIdHa12PEVRFKV8+fLKrl27FEVRlGbNmimdO3dW/v77b2X48OGKi4uLyukymJqaKiEhIZnag4ODFVNTUxUS6crqPej5r1y5cilLlixRO6aWu7u7MnDgQCU+Pl7tKK80evRoxc7OTpk+fbpiZmamTJgwQenZs6dia2urzJ49W+14iqIYzvtmo0aNlG+//VZRlIzPoJCQECUtLU1p06aN0rp1a5XTCX0kIwLCYPz++++MGDGCYcOGaa+6nj59mhkzZjB27FhSU1MZOXIk3377rWqbDRnCRkOmpqaZ1sKGjLXR9WUay/P8/Py0S3IWL16c8uXLq5xIl6IoxMbGsnbtWu7cuaOTVZ+WOR04cKB2/f2xY8fSsGFDVq9ejYmJCcuXL1c33P/Z29tz7NgxnJ2dddqPHTtG4cKFVUr1zNNlN11cXDh9+jT58+fX3mdiYkKBAgX0ZrlYyFhqdcCAAVleIdYnq1evZtGiRTRp0oRx48bRoUMHXF1dKV26NCdPnmTAgAFqRzSY981p06ZRt25d/Pz8SE5OZvjw4TqLagiRido9ESHeVIUKFbRXNJ+3a9cupUKFCoqiKMrvv/+u6tXN4sWLK1u2bFEURXdEYM6cOYq3t7dquZ733XffKR06dFASExO1bYmJiUrHjh2VcePGqZhMV1hYmFK9enVFo9Eo1tbWirW1taLRaJRq1aopYWFhasfTSktLU4yNjZXAwEC1o7yV+Ph45ezZs8qDBw/UjqI1depUxdbWVlm6dKly69Yt5datW8qSJUsUW1tbZdKkSWrHMzitWrVS1q1bp3aM18qVK5dy+/ZtRVEUpVChQsrZs2cVRckYCdKXq+2G8r6pKIoSHR2tTJw4UWnTpo3SqFEj5ZtvvlHu3r2rdiyhp2REQBgMQ9h11BA2GvL392f//v0ULVqUMmXKABmr2yQnJ1O3bl2d9cU3b96sVkx69epFSkoKV69e1a7Nf/36dbp3706vXr3YtWuXatmeZ2RkpN30zN3dXe04byxXrlyUK1dO7Rg6hg0bRmRkJH379tXOEzczM2PEiBF8/fXXKqfL7MqVK1nOadeXupAmTZowbNgwrly5gpeXV6aCVn3JWbRoUcLDw3FwcMDV1ZU9e/ZQrlw5zpw5ozfz2g3lfRMMY1ENoT9k+VBhMLy9vSlTpkymXUd79+5NQEAA/v7+HDt2jE6dOnHz5k3Vcq5evZpx48YRHBwMQJEiRRg3bhw9e/ZULdPzunfv/sbHPt2cRg3m5uYcP34801Srs2fPUqNGDRISElRKltkff/zBtGnTWLBgAaVKlVI7TpYGDx6cZfvTYns3NzdatGih3QdBTXFxcVy9ehVzc3Pc3d315mTwqZCQEFq1asXFixfRaDTaXXqfFuPqy54MRkZGL71Po9HoTc6RI0diaWnJqFGjWLduHZ06dcLJyYnQ0FAGDRrElClT1I5oMO+bhrCohtAv0hEQBuP48eM0b94cIyOjLHcdrVy5MqtWrSIiIoJhw4apkvHJkycoikKuXLlISEjg0qVLHDt2jBIlStCgQQNVMhmqYsWK8euvv2rrQZ46ffo0vr6+BAUFqZQsM2traxISEkhNTcXExARzc3Od+/Vh1ZPatWtz7tw50tLStCMsgYGB5MiRA09PT65fv45Go+Ho0aOUKFFC1az6vitqs2bNyJEjB4sXL8bZ2ZnTp08TGRnJkCFDmD59OjVq1FA7okE7ceIEJ06cwN3dnWbNmqkdx6AYGRlpXysvdlABjI2NadeuHb/88gtmZmaqZBT6RToCwqDo+66j9evXx8fHhz59+hAdHY2npyfGxsY8fPiQH3/8kS+++ELtiIwdO5YePXpkOc1Kn2zdupVJkyYxf/58bYGwn58f/fv3Z8SIEbRs2VLdgM9ZsWLFK+/v2rXrB0rycrNmzeKvv/5i2bJlWFpaAhkb9PXq1Yvq1avTu3dvfH19efLkCbt371YlY2RkJG3bttX7XVHz5cvHgQMHKF26NHnz5uX06dN4eHhw4MABhgwZgr+/v9oRxb/kwYMHXL9+Hcj4/Hm+YFwfbN269Y0W1WjXrp1qi2oI/SIdAWFw9Hlebr58+Th8+DAlS5Zk8eLFzJ07F39/fzZt2sSYMWO0K8qoqWzZsly6dIlatWrRs2dPWrdurZdDxc9fZc+ZM6Oc6em/c+fOrXOsPlxx13dFihRh7969ma72X758mfr163Pnzh3OnTtH/fr1efjwoSoZDWVXVGtra86dO4ezszOurq4sXryY2rVrExwcjJeXl6rT1ubMmcNnn32GmZkZc+bMeeWx+rAaz1P6vhFjfHw8/fv3Z+XKldqN2XLkyEGXLl2YO3eu3qzMVLFiRSZMmJBpBHr37t2MHj2a06dPs2XLFoYMGaKdviqyNykWFgYjq3m5zw956sN814SEBO3oxJ49e/Dx8cHIyIjKlSvrza6O58+fx9/fn2XLljFw4ED69etH+/bt6dGjBxUqVFA7ntasWbPUjvBWgoODWbZsGcHBwcyePZsCBQrw559/4uDgQMmSJdWOR0xMDPfv38/UEXjw4AGxsbFAxsZ3We2Y+6EYyq6opUqVIiAgAGdnZypVqsS0adMwMTFh4cKFuLi4qJpt5syZdOzYETMzM2bOnPnS4zQajd50BAxhI8bBgwdz+PBh/vjjD6pVqwbA0aNHGTBgAEOGDNGbDcUMYVENoWfUWKpIiHfRtGlTpUWLFsqDBw8UCwsL5fLly8pff/2lVKxYUTly5Ija8RRFURQvLy9l9uzZSmhoqGJpaakcP35cURRF8fPzUwoWLKhyusySk5OVTZs2KU2bNlWMjY0VLy8vZdasWXqzCZahOHTokGJubq7Uq1dPMTEx0S4bO3nyZL3ZxMfX11dxdnZWNm/erISFhSlhYWHK5s2bFRcXF6VTp06KoijKb7/9pnz00UeqZbSwsNAuw/r88rtnzpxRbGxsVMv1ol27dimbNm1SFEVRbty4oXh4eCgajUbJly+fsn//fpXTGR5D2IjR1tZWOXjwYKb2AwcOKPny5fvwgV6ibNmySteuXZWkpCRtW3JystK1a1elbNmyiqIoytGjRxUnJye1Igo9Ix0BYTBsbW2VgIAARVEydnm8du2aoiiKsn//fu0bnNo2bNigGBsbK0ZGRsonn3yibZ80aZLSsGFDFZNlLSkpSVm7dq1Sv359JWfOnErNmjUVNzc3JU+ePMratWvVjqekpqYqGzZsUMaPH6+MHz9e2bhxo5KSkqJ2rEwqV66szJgxQ1EU3ROZU6dOKUWKFFEzmtbjx4+VXr16aXc9NjIyUkxMTJTevXsrcXFxiqIoir+/v+Lv769aRkPeFTUyMlJJT09XO4ZBMjMzU27duqUoiu7rJzAwUDEzM1Mzmpa5ubly5cqVTO2XLl1ScuXKpUKirB07dkyxtbVV8ufPr9StW1epW7euUqBAAcXW1lY5ceKEoiiKsnLlSmXatGkqJxX6QmoEhMHQ53m5z4uIiCA8PJwyZcpol+87ffo0lpaWeHp6qpwuw9mzZ1m2bBm//fYbpqamdOnShV69eml3wp07dy4TJ07k3r17qmW8fPkyzZs3JyIiQmeVm/z58/PHH3/o1TKdFhYWXLx4EWdnZ/LkyaOd237r1i08PT1JTExUO6JWXFwcISEhALi4uGBhYaFyomcuXbpE3bp1KVeuHAcOHKB58+Y6u6K6urqqHZGUlBTMzc05f/68Xv0NZsVQlowtUaIEkydPpkWLFjqvn7lz57Js2TLOnTunaj6AunXrYmtry8qVK7Wr7Tx58oSuXbsSFRXFvn37VE74jL4vqiH0i9QICIOhz/Nyn1eoUCEKFSqk0/biEphq8vLy4tq1a9SvX58lS5Zol0J8XocOHRg4cKBKCTP06tWLkiVL4ufnh7W1NQCPHj2iW7dufPbZZxw/flzVfM+zsrIiPDwcZ2dnnXZ/f3+KFCmiUqqsWVhYaJff1TelSpUiMDCQefPmkSdPHuLi4vDx8aFfv37Y2dmpHQ/IWH7RwcFBL2qSXsff3/+VS8b+9NNPDBkyRPUlYw1hI8bZs2fToEGDTBuKmZmZqbbK1otSUlLw9PRk+/bt9OnTR+04wkDIiIAwGLt37yY+Ph4fHx+CgoJo2rQpgYGB2Nrasm7dOurUqaN2RIMwYcIEevTooXcnqC8yNzfHz88vU6HtpUuXqFChAk+ePFEpWWZDhw7l1KlTbNiwgWLFinHu3Dnu3btHly5d6NKlC2PHjlU7IrVr137lWvwHDhz4gGkM25IlS9i8eTOrVq1S/Wr6qxjCkrFPvbgRY+HChfnuu+/0ZiNGyFgMYvXq1Vy7dg2A4sWL07Fjx0z7hqipSJEi7Nu3j+LFi6sdRRgI6QgIgxYVFYW1tbVebTak78aPH8/QoUMzLXf35MkTfvjhB8aMGaNSMl1lypRh5syZmTp4Bw4cYODAgdoVMPRBcnIy/fr1Y/ny5aSlpZEzZ07S0tLw9fVl+fLlmUZc1DBo0CCd2ykpKZw/f55Lly7RtWtXZs+erVIyw+Pt7U1QUBApKSk4OjpmWs5WH6aygGEsGfuihIQE4uLiKFCggNpRDNKkSZMIDAxk8eLF2mWXhXgV6QgIkc3kyJGD8PDwTB+0kZGRFChQQNUpD0+XsYSMpfmGDx/OuHHjqFy5MgAnT55k/PjxTJkyhcaNG6sV86XCwsK4ePEicXFxeHt74+7urnak1xo3bhxxcXGyudBb+O677155vz6MAEHGNLDt27fz8ccf67QfOnSIZs2a8fjxY0JCQihbtqzOa08t9+/f127W5enpqfpmXdu2bXvjY/VhHxuAVq1asX//fiwsLPDy8srUSd28ebNKyYS+ko6AENmMkZER9+7dy/Qhe+DAAdq1a8eDBw9USpaR7fnRnadvT0/bnr+tT3O0DWWUJStBQUFUrFhRNmX7D+rYsSMnTpxgxowZ2j1Czpw5w9ChQ6latSqrVq1i7dq1TJ8+HT8/P9VyPn78mL59+/Lbb7/pbNbVrl075s+fT968eVXJ9XSxh9fRp/ej7t27v/L+ZcuWfaAkwlBIR0CIbOLpFKqYmBgsLS0zbcYWFxdHnz59mD9/vmoZDx8+/MbH1qpV619M8nb0eZTldVatWsWIESO4e/eu2lHEexYXF8egQYNYuXIlqampAOTMmZOuXbsyc+ZMcufOzfnz54GMTabU0q5dO/z9/Zk7dy5VqlQB4MSJEwwcOJCyZcuydu1a1bIJ8V8nHQEhsokVK1agKAo9evRg1qxZOlfZTExMcHJy0n4Ii7ejz6MsT/n4+OjcVhSF8PBw/Pz8GD16tN5MZxHvnz4vGQuQO3dudu/eTfXq1XXa//rrLxo2bEh8fLxKyV4tOjoaKysrtWMI8Y9IJYkQ2UTXrl0BcHZ2plq1alJI9h48HWXRaDQUK1bspaMs+uDF6RVGRkZ4eHgwfvx46tevr1IqXd7e3lkW/j+/7n23bt2oXbu2CukMlz4vGQtga2ub5fSfvHnzapcOVtvUqVNxcnKiXbt2ALRp04ZNmzZhZ2fHzp07tUuK6oONGzeyfv16QkNDSU5O1rlPXwrZhf6QEQEhhHhHMsryfn399dcsWLAALy8v7d4bZ86c4cKFC3Tr1o0rV66wf/9+Nm/eTIsWLVROq//i4+OZMmUK+/fv5/79+9r59089HSVQ28KFC9mwYQOrVq3S7sESERFB165d8fHx4fPPP1c5YcYFlNWrV1O1alX27t1L27ZtWbdunfaEe8+ePWpHBGDOnDl88803dOvWjYULF9K9e3eCg4M5c+YM/fr14/vvv1c7otAz0hEQQoh/6PDhw3o/yhIWFoZGo6Fo0aJAxm7Xa9asoUSJEnz22Wcqp8vQu3dvHBwcGD16tE77xIkTuX37NosWLWLs2LHs2LFD1eJWQykO79ChA4cPH6Zz587Y2dllGm1Re9PAp54ux5qUlISDgwMAoaGhmJqaZlp5S60r2ubm5gQGBmJvb8/AgQNJTEzkl19+ITAwkEqVKvHo0SNVcr3I09OTsWPH0qFDB51dmseMGUNUVBTz5s1TO6LQM9IREEKIf+jcuXMYGxvj5eUFwNatW1m2bBklSpRg3LhxmJiYqJwQatSowWeffUbnzp2JiIigWLFilCpVihs3btC/f3+9OHnNmzcvZ8+exc3NTac9KCiIjz76iJiYGK5du0aFChV4/PixSikNpzjcysqKHTt2UK1aNbWjvNLrlmN9nlq1LIULF2bjxo1UrVoVDw8PJk6cSJs2bbh+/ToVKlTQi+VXAXLlysXVq1dxdHSkQIEC7N27lzJlynDjxg0qV65MZGSk2hGFntHfy1dCCGEgPv/8c0aOHImXlxchISG0a9cOHx8fNmzYQEJCArNmzVI7IpcuXdJOt1m/fj1eXl4cO3aMPXv20KdPH73oCJiZmXH8+PFMHYHjx49jZmYGQHp6uvbfalEUJctahoCAAL3aadja2lqv8ryMIRSq+/j44Ovri7u7O5GRkTRq1AgAf3//TH+vaipUqBBRUVE4Ojri4ODAyZMnKVOmDDdv3kSu+4qsSEdACKGXDKlwNDAwULv84oYNG6hVqxZr1qzh2LFjtG/fXi86AikpKZiamgKwb98+7QZInp6ehIeHqxlNq3///vTp04ezZ8/qrHu/ePFiRo0aBcDu3btVW+rSkIrDASZMmMCYMWNYsWJFpmlM4u3MnDkTJycnwsLCmDZtmnblpfDwcPr27atyumfq1KnDtm3b8Pb2pnv37gwaNIiNGzfi5+eXaeUwIUCmBgmR7RhKAaEhFY5aWlpy9uxZ3N3d+eSTT2jatCkDBw4kNDQUDw8Pnjx5omo+gEqVKlG7dm2aNGlC/fr1tVcKT548yaeffsrff/+tdkQAVq9ezbx587Q7zHp4eNC/f398fX2BjHn4TzuDH5qhFYd7e3sTHByMoig4OTlhbGysc7+aK8g87VS9Cdns7s3dvHmTIkWKaKcjrl27luPHj+Pu7k7Dhg0NYrdz8WFJR0CIbMZQCggNpXAUMq7C2dvbU69ePXr27MmVK1dwc3Pj8OHDdO3alVu3bqmaD+DQoUO0atWK2NhYunbtytKlSwEYNWoU165dY/PmzSonNAypqamsXr1a+zvXZ6+be6/mlJwVK1Zo/x0ZGcnEiRNp0KCBzoZiu3fvZvTo0QwaNEitmAbHUOpXhP6QjoAQ2YyhFBAaSuEowIULF+jYsSOhoaEMHjxYe4LVv39/IiMjWbNmjar5nkpLSyM2NlZnbfZbt26RK1euTCcOakpOTs5ytOrpijJqe74gU/xzrVu3pnbt2nz55Zc67fPmzWPfvn1s2bJFnWAGyMjIiIiIiEyv59u3b1OiRAm93ZxNqEdqBITIZgylgNBQCkcBSpcuzcWLFzO1//DDD+TIkUOFRFnLkSNHpg2anJyc1AmThRs3btCjRw+OHz+u0/60OFdfrmZWrFgRf39/6Qi8J7t372bq1KmZ2hs2bMjIkSNVSGR4Bg8eDGTUUI0ZM0anJiQtLY1Tp06pVlsj9Jt0BITIZgylgFDfC0dfFB0dzcaNGwkODmbYsGHY2Nhw5coVChYsSJEiRdSOZxC6detGzpw52b59e5bT1vRF3759GTJkCH///TcfffQRuXPn1rlfX3bxTUtLY+bMmS/dZVZf5t7b2tqydetWhgwZotO+detWbG1tVUqV4ciRI1StWlWv9wiBjNWLIKPTfPHiRZ0li01MTChTpgxDhw5VK57QYzI1SIhsRp8LCF+kz4Wjz7tw4QJ169bFysqKW7ducf36dVxcXPj2228JDQ1l5cqVquYzFLlz5+bs2bN4enqqHeWVjIyMMrVpNBq9G7kYM2YMixcvZsiQIXz77bd888033Lp1iy1btjBmzBgGDBigdkQAli9fTq9evWjUqBGVKlUC4NSpU+zatYtFixbRrVs31bK9bM69vurevTuzZ8/G0tJS7SjCQEhHQIhsRp8LCA1VvXr1KFeuHNOmTdPZzfP48eP4+vrqRbGwIahQoQIzZ86kevXqakd5pdu3b7/yfn2ZMuTq6sqcOXNo0qQJefLk4fz589q2kydP6k3tCmSc+M+ZM4erV68CULx4cQYMGKDtGKjlZXPuhfivkI6AEEKv6XvhKGQUNp87dw5XV1edjsDt27fx8PAgMTFR7YgG4cCBA3z77bdMmjQJLy+vTKNVcpXz7eTOnZurV6/i4OCAnZ0dO3bsoFy5coSEhODt7U1MTIzaEfWekZER9+7dI3/+/GpHEeJfod+T3oQQ2ZahFI4CmJqaEhsbm6k9MDBQ1ROIOXPmvPGx+jBNpF69egDUrVtXp10ff+cAV65cyXLu/dPN2tRWtGhRwsPDcXBwwNXVlT179lCuXDnOnDmj3VxOLVm9Xl5G7Q5gt27dXvvzkuV3haGSjoAQ2YCNjQ2BgYHky5fvtRv56EsBoaEUjkLGid/48eNZv349kDFfPDQ0lBEjRtC6dWvVcs2cOVPn9oMHD0hISMDKygrIKHB+unSoPnQEDh48qHaENxISEkKrVq24ePGitjYA0P6N6kuHpVWrVuzfv59KlSrRv39/OnXqxJIlSwgNDVV9bX4rK6vXvqb1pQOYJ08ezM3NVc0gxL9FpgYJkQ2sWLGC9u3bY2pqqrORT1a6du36gVK9mqEUjgLExMTw6aef4ufnx+PHjylcuDARERFUqVKFnTt3ZlpVRg1r1qzhp59+YsmSJXh4eABw/fp1evfuzeeff07Hjh1VTmg4mjVrRo4cOVi8eDHOzs6cPn2ayMhIhgwZwvTp06lRo4baEbN08uRJ7S6zzZo1UzXL4cOH3/jYWrVq/YtJXk1qBMR/nXQEhBB6yVAKR5939OhRLly4QFxcHOXKldNOddEHrq6ubNy4EW9vb532s2fP8umnn3Lz5k1Vcl24cIFSpUphZGTEhQsXXnmsvizLmS9fPg4cOEDp0qXJmzcvp0+fxsPDgwMHDjBkyBDtUo7izUVHR7NkyRJtsXCJEiXo2bMnefPmVTWXoa0aJMTbkqlBQmRjiYmJmeY3qz0f96mpU6cyfPhwgyocrV69ut52XMLDw0lNTc3UnpaWxr1791RIlKFs2bLaK65ly5bVmWrzPH2YIvJUWloaefLkATI6BXfv3sXDwwNHR0ftUrfizfn5+dGwYUPMzMyoWLEikDGtbdKkSdq6BrXItVLxXycjAkJkM/Hx8YwYMYL169cTGRmZ6X59Odl6ulb7i/OI9WXesKEV4jZr1ow7d+6wePFi7YnV2bNn+eyzzyhSpAjbtm1TJdft27dxcHBAo9EYzLKcNWrUYMiQIbRs2RJfX18ePXrEt99+y8KFCzl79iyXLl1SO6JBqVGjBm5ubixatEi7cVdqaiq9evUiJCSEI0eOqJbt8OHDVKtWTe83FBPiXUlHQIhspl+/fhw8eJAJEybQuXNn5s+fz507d/jll1+YMmWK3swVf90cYjXnDQM4Ozu/0XEajYaQkJB/Oc3rPXjwgK5du7Jr1y7t6EpqaioNGjRg+fLlejH14WW7uKampnL8+HFq1qypUjJdu3fvJj4+Hh8fH4KCgmjatCmBgYHY2tqybt066tSpo3ZEg2Jubo6/v3+meqArV65Qvnx5EhISVEoGJ06cIDIykqZNm2rbVq5cydixY4mPj6dly5bMnTtX9VWYhHhX0hEQIptxcHBg5cqVfPzxx1haWnLu3Dnc3NxYtWoVv/32Gzt37lQ7ovgXBQYGcvXqVTQaDZ6enhQrVkztSFovm48dGRlJgQIFVB8FepWoqKjXrsglslawYEFWrVpF/fr1ddp3795Nly5dVJ261qhRIz7++GNGjBgBwMWLFylXrhzdunWjePHi/PDDD3z++eeMGzdOtYxC/BMy1iVENhMVFYWLiwuQMc/+6XKh1atX54svvlAzmkEWjr7oxaUk9U2xYsVwd3cH9C/j02lfL4qMjNSLlZdeFBQURHBwMDVr1sTGxkYv55NHR0ezceNGgoODGTZsGDY2Npw7d46CBQtSpEgRteMB0K5dO3r27Mn06dOpWrUqAMeOHWPYsGF06NBB1Wznz59nwoQJ2ttr166lUqVKLFq0CAB7e3vGjh0rHQFhsKQjIEQ24+Liws2bN3FwcMDT05P169dTsWJF/vjjD+368moxxMLRp1auXMkPP/zAjRs3gIwT7mHDhtG5c2eVkz2jrxl9fHyAjN/ri5s3paWlceHCBe0Joj6IjIykbdu2HDx4EI1Gw40bN3BxcaFnz55YW1szY8YMtSMCGR3revXqkTdvXm7dukXv3r2xsbFh8+bNhIaGsnLlSrUjAjB9+nQ0Gg1dunTRFrQbGxvzxRdfMGXKFFWzPXr0iIIFC2pvHz58mEaNGmlvV6hQgbCwMDWiCfFeSEdAiGyme/fuBAQEUKtWLUaOHEmzZs2YN28eKSkp/Pjjj6pmu3nzpnYnXrWWs3wXP/74I6NHj+bLL7+kWrVqQMZSon369OHhw4eqb94E+p3x6RKRiqJk2rzJxMSEypUr07t3b7XiZTJo0CCMjY0JDQ2lePHi2vZ27doxePBgvekIDB48mG7dujFt2jTtKkcAjRs3xtfXV8VkukxMTJg9ezaTJ08mODgYyFjuNleuXCony5i2dPPmTezt7UlOTubcuXN899132vsfP36caUUzIQyKIoTI1m7evKls2rRJCQgIUDuKjsOHDyspKSmZ2lNSUpTDhw+rkOjlnJyclBUrVmRqX758ueLk5KRCoswMIeO4ceOUuLg4tWO8VsGCBZXz588riqIoFhYWSnBwsKIoihIcHKzkzp1bzWg6LC0tlaCgIEVRdHPeunVLMTU1VTOawejTp49SpUoV5ciRI8rgwYMVW1tbJSkpSXv/r7/+qpQvX17FhEL8MzIiIEQ25+TkhJOTk9oxMqldu3aWhaMxMTHUrl1br6YGhYeHZzl1pWrVqoSHh6uQKDNDyDh8+HCdqWC3b9/m999/p0SJEpkKSdUUHx+f5dXqqKgovVo9xtTUlNjY2EztgYGB2pE38WoTJkzAx8eHWrVqYWFhwYoVKzAxMdHev3TpUr362xTibRmpHUAI8eHt37+fpk2b4urqiqurK02bNmXfvn1qx9KhGFDhqJubG+vXr8/Uvm7dOm1hrtoMIWOLFi2089ajo6OpWLEiM2bMoEWLFixYsEDldM/UqFFDZ369RqMhPT2dadOmUbt2bRWT6WrevDnjx48nJSUFyMgZGhrKiBEjaN26tcrpDEO+fPk4cuQIjx494tGjR7Rq1Urn/g0bNjB27FiV0gnxz8nyoUJkMz/99BMDBw7k008/pUqVKgCcPHmSjRs3MnPmTPr166dqvqeFo1u3bqVhw4ZZFo56eHiwa9cutSJmsmnTJtq1a0e9evW08++PHTvG/v37Wb9+faaTBzUYQsZ8+fJx+PBhSpYsyeLFi5k7dy7+/v5s2rSJMWPGcPXqVbUjAnDp0iXq1q1LuXLlOHDgAM2bN+fy5ctERUVx7NgxXF1d1Y4IZIyeffrpp/j5+fH48WMKFy5MREQEVapUYefOnXrXoRZCfHjSERAimylatCgjR47kyy+/1GmfP38+kyZN4s6dOyoly9C9e3cAVqxYQdu2bTMVjjo5OdG7d2/y5cunVsQsnT17lpkzZ2pPVosXL86QIUPw9vZWOdkz+p4xV65cXLt2DQcHB9q2bUvJkiUZO3YsYWFheHh4qLqx1ItiYmKYN28eAQEBxMXFUa5cOfr164ednZ3a0TI5duyYTs569eqpHUkIoSekIyBENmNhYcH58+dxc3PTab9x4wbe3t7ExcWplEzXd999x9ChQ+WqZTZSunRpevXqRatWrShVqhS7du2iSpUqnD17liZNmhAREaF2RIMXHR2t+jLBQgj9IR0BIbIZX19fvL29GTZsmE779OnT8fPzY+3atSol0/XkyRMURdEWZepr4ehT6enpBAUFcf/+fdLT03Xuq1mzpkqpdKWlpbFlyxbtiEDJkiVp3rw5OXLkUDlZho0bN+Lr60taWhp16tRh7969AEyePJkjR47w559/qpzwmejoaE6fPp3l77tLly4qpdI1depUnJycaNeuHQBt27Zl06ZNFCpUiJ07d1KmTBmVEwoh1CYdASGymYkTJzJ9+nSqVaumUyNw7NgxhgwZgqWlpfbYAQMGqBWT+vXr4+PjQ58+fYiOjsbDwwMTExMePnzIjz/+qPouyM87efIkvr6+3L59O9MGaPqy+VlQUBBNmjTh77//xsPDA4Dr169jb2/Pjh079GZee0REBOHh4ZQpUwYjo4z1LE6fPo2lpSWenp4qp8vwxx9/0LFjR+Li4rC0tNQpatdoNNrdutXm7OzM6tWrqVq1Knv37qVt27asW7eO9evXExoayp49e9SOKIRQmXQEhMhmnJ2d3+g4jUZDSEjIv5zm5QylcBQydkQuVqwY3333HXZ2dplWO3q6YZaaGjdujKIorF69GhsbGyBjBaZOnTphZGTEjh07VE74TFBQEMHBwdSsWRNzc/OXriCllmLFitG4cWMmTZqkF5tevYy5uTmBgYHY29szcOBAEhMT+eWXXwgMDKRSpUo8evRI7YhCCJXJPgJCZDOGsmNvQkKCdjfUPXv24OPjg5GREZUrV+b27dsqp9N148YNNm7cmKnuQp8cPnyYkydPajsBALa2tkyZMkW7ipDaIiMjadu2LQcPHkSj0XDjxg1cXFzo2bMn1tbWerNj7507dxgwYIBedwIArK2tCQsLw97enl27djFx4kQgY2lefRilEkKoT/YREELoJTc3N7Zs2UJYWBi7d+/W1gXcv39fZ/qSPqhUqRJBQUFqx3glU1NTHj9+nKk9Li5OZ4MkNQ0aNAhjY2NCQ0N1TrLbtWunV8vFNmjQAD8/P7VjvJaPjw++vr588sknREZG0qhRIwD8/f31utMqhPhwZERACKGXxowZg6+vL4MGDaJOnTraeoY9e/boxXKXFy5c0P67f//+DBkyhIiICLy8vDA2NtY5tnTp0h86XiZNmzbls88+Y8mSJVSsWBGAU6dO0adPH5o3b65yugx79uxh9+7dFC1aVKfd3d1d9VGgbdu2af/dpEkThg0bxpUrV7L8fevLz3PmzJk4OTkRFhbGtGnTsLCwADJ2me7bt6/K6YQQ+kBqBIQQekufC0eNjIzQaDSZioOfenqfvhQLR0dH07VrV/744w/tiWtqairNmzdn+fLlelHHkCdPHs6dO4e7uzt58uQhICAAFxcX/Pz8aNCgAZGRkaple/r39zr68vsWQog3IR0BIYRe09fC0be5Qu3o6PgvJnk7N27c4Nq1a0DGhmL6NEWkcePGfPTRR0yYMIE8efJw4cIFHB0dad++Penp6WzcuFHtiHpv27ZtNGrUCGNjY51RjKzoy8iFEEI90hEQIpsJDQ3F3t4+08m0oiiEhYXh4OCgUjJdLysc7dGjh14VjkLGOvcFCxakR48eOu1Lly7lwYMHjBgxQqVkhuXSpUvUrVuXcuXKceDAAZo3b87ly5eJiori2LFjerPEqT4zMjIiIiKCAgUKvHIUQ0YuhBAgHQEhsp0cOXIQHh5OgQIFdNojIyMpUKCA3pwcdOnShfv377N48WKKFy+unSaye/duBg8ezOXLl9WOqOXk5MSaNWuoWrWqTvupU6do3769ais1DR48+I2P/fHHH//FJG8uJiaGefPmERAQQFxcHOXKlaNfv37Y2dmpHU0IIf5zpFhYiGzmZVNr4uLiMDMzUyFR1vS5cPRFERERWZ6o5s+fn/DwcBUSZfD393+j4/RhqlVKSgoNGzbk559/5ptvvlE7jhBCZAvSERAim3h6dVij0TB69Gid5RnT0tI4deoUZcuWVSldZvHx8Vmu0x4VFYWpqakKiV7O3t6eY8eOZdqs7dixYxQuXFilVHDw4EHVvvfbMjY21lmJSfxzAwYMwM3NLdMO4fPmzSMoKIhZs2apE0wIoTdkHwEhsgl/f3/8/f1RFIWLFy9qb/v7+3Pt2jXKlCnD8uXL1Y6pVaNGDVauXKm9rdFoSE9PZ9q0adSuXVvFZJn17t2br776imXLlnH79m1u377N0qVLGTRoEL1791Y7XiZhYWGEhYWpHSOTTp06sWTJErVj/Gds2rQpy83iqlatKoXXQghARgSEyDaeXh3u3r07s2fP1rtNuV40bdo06tati5+fH8nJyQwfPlyncFSfDBs2jMjISPr27UtycjIAZmZmjBgxgq+//lrldBlSU1P57rvvmDNnDnFxcQBYWFjQv39/xo4dm2ktfDWkpqaydOlS9u3bx0cffUTu3Ll17teXOobly5fTrVu3TO2pqamMHj2ayZMnf/hQWYiMjMxyWVhLS0sePnyoQiIhhL6RYmEhsrnY2FgOHDiAp6en6mvzv8jQCkfj4uK4evUq5ubmuLu769UUpi+++ILNmzczfvx47eZsJ06cYNy4cbRs2ZIFCxaonJBXjvRoNBoOHDjwAdO8nKWlJQ0aNGDhwoVYW1sDcP36dXx9fYmMjOTWrVvqBvy/UqVK0adPH7788kud9rlz57JgwQKuXLmiUjIhhL6QjoAQ2Uzbtm2pWbMmX375JU+ePKFMmTLcunULRVFYu3YtrVu3VjuiTuGou7u72nH+E/LmzcvatWtp1KiRTvvOnTvp0KEDMTExKiUzPMHBwXTq1ImwsDCWLVtGYGAgw4cPp2XLlvz00096sTkbZCxf++WXXzJs2DDq1KkDwP79+5kxYwazZs3Sy2lrQogPS6YGCZHNHDlyRLsqy++//46iKERHR7NixQomTpyoFx0BKRx9/0xNTXFycsrU7uzsjImJyYcPZMBcXV05duwYX331FQ0bNiRHjhysWLGCDh06qB1NR48ePUhKSuL7779nwoQJQMZStwsWLKBLly4qpxNC6AMpFhYim4mJicHGxgaAXbt20bp1a3LlykWTJk24ceOGyumekcLR9+vLL79kwoQJJCUladueniS+OHVEvN6OHTtYu3YtVapUwcrKiiVLlnD37l21Y2mlpqaycuVKfHx8+Pvvv7l37x6xsbGEhIRIJ0AIoSUjAkJkM/b29pw4cQIbGxt27drF2rVrAXj06JFe7SNgKIWj+szHx0fn9r59+yhatChlypQBICAggOTkZOrWratGPIP1+eefs2LFCr7//nsGDx7MvXv36NGjB15eXixYsIC2bduqHZGcOXPSp08frl69CmTsaSGEEC+SjoAQ2cxXX31Fx44dsbCwwMHBgY8//hjImDLk5eWlbrjnXLp0iXLlygEQGBioc58+bIBlCF6cq/7itC97e/sPGec/49ixY5w6dUrboSpUqBA7d+5k/vz59OjRQy86AgAVK1bE398fR0dHtaMIIfSUFAsLkQ35+fkRFhbGJ598goWFBZAx1cHKyirLdceFEM8kJSW9dEWo69ev4+Hh8YETZW39+vV8/fXXDBo0KMtRtdKlS6uUTAihL6QjIEQ2lZyczM2bN3F1dSVnThkcFOK/xsgocxmgRqNBURQ0Gg1paWkqpBJC6BP59Bcim0lISKB///6sWLECyJh24+LiQv/+/SlSpAgjR45UOaEQ+m/jxo2sX7+e0NBQ7SZyT507d06lVLpu3rypdgQhhJ6TVYOEyGa+/vprAgICOHTokE5xcL169Vi3bp2KyYQwDHPmzKF79+4ULFgQf39/KlasiK2tLSEhIZn2aVCTo6PjK7+EEEI6AkJkM1u2bGHevHlUr15dp+i2ZMmSBAcHq5hMCMPw008/sXDhQubOnYuJiQnDhw9n7969DBgwQO82Zlu1ahXVqlWjcOHC3L59G4BZs2axdetWlZMJIfSBdASEyGYePHhAgQIFMrXHx8fLajxCvIHQ0FCqVq0KgLm5OY8fPwagc+fO/Pbbb2pG07FgwQIGDx5M48aNiY6O1tYEWFlZMWvWLHXDCSH0gtQICJHNlC9fnh07dtC/f3/g2VKcixcvpkqVKmpGE/+iOXPmZNmu0WgwMzPDzc2NmjVrkiNHjg+czPAUKlSIqKgoHB0dcXBw4OTJk5QpU4abN2+iT+tvzJ07l0WLFtGyZUumTJmibS9fvjxDhw5VMZkQQl9IR0CIbGbSpEk0atSIK1eukJqayuzZs7ly5QrHjx/n8OHDascT/5KZM2fy4MEDEhISsLa2BjI2kcuVKxcWFhbcv38fFxcXDh48KPsLvEadOnXYtm0b3t7edO/enUGDBrFx40b8/PwybeKmpps3b+Lt7Z2p3dTUlPj4eBUSCSH0jUwNEiKbqV69OufPnyc1NRUvLy/27NlDgQIFOHHiBB999JHa8cS/ZNKkSVSoUIEbN24QGRlJZGQkgYGBVKpUidmzZxMaGkqhQoUYNGiQ2lH13sKFC/nmm28A6NevH0uXLqV48eKMHz+eBQsWqJzuGWdnZ86fP5+pfdeuXRQvXvzDBxJC6B3ZR0AIIbIBV1dXNm3aRNmyZXXa/f39ad26NSEhIRw/fpzWrVsTHh6uTkjxXi1evJhx48YxY8YMevbsyeLFiwkODmby5MksXryY9u3bqx1RCKEymRokRDaUnp5OUFAQ9+/fJz09Xee+mjVrqpRK/JvCw8NJTU3N1J6amkpERAQAhQsX1ha+ildLTEzkwoULWb6GmjdvrlIqXb169cLc3Jxvv/2WhIQEfH19KVy4MLNnz5ZOgBACkBEBIbKdkydP4uvry+3btzMVNspuo/9dTZo0ISIigsWLF2vnjfv7+9O7d28KFSrE9u3b+eOPPxg1ahQXL15UOa1+27VrF126dOHhw4eZ7tPX11BCQgJxcXFZrhgmhMi+pEZAiGymT58+lC9fnkuXLhEVFcWjR4+0X1FRUWrHE/+SJUuWYGNjw0cffYSpqSmmpqaUL18eGxsblixZAoCFhQUzZsxQOan+69+/P23atCE8PJz09HSdL33qBIwbN047WpErVy5tJyAmJoYOHTqoGU0IoSdkRECIbCZ37twEBATg5uamdhShgmvXrhEYGAiAh4cHHh4eKicyPJaWlvj7++Pq6qp2lFeyt7fH3t6eX3/9FRcXFwAOHTpEly5dKFSoEKdPn1Y5oRBCbTIiIEQ2U6lSJYKCgtSOIVTi6elJ8+bNad68uXQC3tGnn37KoUOH1I7xWhcuXKBo0aKULVuWRYsWMWzYMOrXr0/nzp05fvy42vGEEHpARgSEyAYuXLig/XdwcDDffvstw4YNw8vLC2NjY51jS5cu/aHjiQ8gLS2N5cuXs3///iwLXA8cOKBSMsOTkJBAmzZtyJ8/f5avoQEDBqiULGujRo1iypQp5MyZkz///JO6deuqHUkIoSekIyBENmBkZIRGo3nprqdP79PXQkfxz3355ZcsX76cJk2aYGdnp91R+qmZM2eqlMzwLFmyhD59+mBmZoatra3Oz1Kj0RASEqJiOl1z585l5MiRtGzZkrNnz5IjRw7WrFlDmTJl1I4mhNAD0hEQIhu4ffv2Gx/r6Oj4LyYRasmXLx8rV66kcePGakcxeIUKFWLAgAGMHDkSIyP9nWHbsGFD/Pz8+Pnnn/n000958uQJgwcPZvny5Xz33XcMHz5c7YhCCJVJR0AIIbKBwoULc+jQIYoVK6Z2FINnY2PDmTNn9L5Y+JNPPmHFihUULlxYp33Hjh306tVLNo4TQkhHQIjsZvLkyRQsWJAePXrotC9dupQHDx4wYsQIlZKJf9OMGTMICQlh3rx5maYFibczaNAg8ufPz6hRo9SO8s4ePnxIvnz51I4hhFCZdASEyGacnJxYs2YNVatW1Wk/deoU7du35+bNmyolE/+mVq1acfDgQWxsbChZsmSmAtfNmzerlMzwDBgwgJUrV1KmTBlKly6d6Wf5448/qpQss7/++otffvmF4OBgNm7cSJEiRVi1ahXOzs5Ur15d7XhCCJXlVDuAEOLDioiIwM7OLlN7/vz5ZarAf5iVlRWtWrVSO8Z/wsWLF7W7M1+6dEnnPn0abdm0aROdO3emY8eO+Pv7k5SUBGRsKDZp0iR27typckIhhNqkIyBENmNvb8+xY8dwdnbWaT927FimucTiv2PZsmVqR/jPOHjwoNoR3sjEiRP5+eef6dKlC2vXrtW2V6tWjYkTJ6qYTAihL6QjIEQ207t3b7766itSUlKoU6cOAPv372f48OEMGTJE5XRCiPfl+vXr1KxZM1N73rx5iY6O/vCBhBB6RzoCQmQzw4YNIzIykr59+5KcnAyAmZkZI0aM4Ouvv1Y5nXifypUrx/79+7G2tsbb2/uV01bOnTv3AZOJD6FQoUIEBQXh5OSk03706FFcXFzUCSWE0CvSERAim9FoNEydOpXRo0dz9epVzM3NcXd3x9TUVO1o4j1r0aKF9vfaokULvZq/Lv59vXv3ZuDAgSxduhSNRsPdu3c5ceIEQ4cOZfTo0WrHE0LoAVk1SAghhPgPUhSFSZMmMXnyZBISEgAwNTVl6NChTJgwQeV0Qgh9IB0BIYTIBlxcXDhz5gy2trY67dHR0ZQrV46QkBCVkol/W3JyMkFBQcTFxVGiRAksLCzUjiSE0BPSERBCiGzAyMiIiIgIChQooNN+79497O3ttfUiQgghsg+pERBCiP+wbdu2af+9e/du8ubNq72dlpbG/v37My0lK4QQInuQEQEhhPgPMzIyAjKKxF98uzc2NsbJyYkZM2bQtGlTNeIJIYRQkXQEhBAiG3B2dubMmTPky5dP7ShCCCH0hHQEhBAim4qOjsbKykrtGEIIIVRipHYAIYQQ/76pU6eybt067e02bdpgY2NDkSJFCAgIUDGZEEIItUhHQAghsoGff/4Ze3t7APbu3cu+ffvYtWsXjRo1YtiwYSqnE0IIoQZZNUgIIbKBiIgIbUdg+/bttG3blvr16+Pk5ESlSpVUTieEEEINMiIghBDZgLW1NWFhYQDs2rWLevXqARm7z6alpakZTQghhEpkREAIIbIBHx8ffH19cXd3JzIykkaNGgHg7++Pm5ubyumEEEKoQToCQgiRDcycORMnJyfCwsKYNm0aFhYWAISHh9O3b1+V0wkhhFCDLB8qhBBCCCFENiQjAkIIkY1cuXKF0NBQkpOTddqbN2+uUiIhhBBqkY6AEEJkAyEhIbRq1YqLFy+i0Wh4Ohis0WgApGBYCCGyIVk1SAghsoGBAwfi7OzM/fv3yZUrF5cvX+bIkSOUL1+eQ4cOqR1PCCGECqRGQAghsoF8+fJx4MABSpcuTd68eTl9+jQeHh4cOHCAIUOG4O/vr3ZEIYQQH5iMCAghRDaQlpZGnjx5gIxOwd27dwFwdHTk+vXrakYTQgihEqkREEKIbKBUqVIEBATg7OxMpUqVmDZtGiYmJixcuBAXFxe14wkhhFCBTA0SQohsYPfu3cTHx+Pj40NQUBBNmzYlMDAQW1tb1q1bR506ddSOKIQQ4gOTjoAQQvxHXbhwgVKlSmFklPUs0KioKKytrbUrBwkhhMhepEZACCH+o7y9vXn48CEALi4uREZG6txvY2MjnQAhhMjGpCMghBD/UVZWVty8eROAW7dukZ6ernIiIYQQ+kSKhYUQ4j+qdevW1KpVCzs7OzQaDeXLlydHjhxZHhsSEvKB0wkhhFCbdASEEOI/auHChdri4AEDBtC7d2/tEqJCCCGEFAsLIUQ20L17d+bMmSMdASGEEFrSERBCCCGEECIbkmJhIYQQQgghsiHpCAghhBBCCJENSUdACCGEEEKIbEg6AkIIIYQQQmRD0hEQQgghhBAiG5KOgBBCCCGEENmQdASEEEIIIYTIhqQjIIQQQgghRDb0P8KN0fZS7cNUAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"target\"].value_counts()"
      ],
      "metadata": {
        "id": "SxKcyPrlTWkS",
        "outputId": "3f5ad19e-701d-457d-b0e4-3719d581b603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "1    629\n",
              "0    561\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle, np conversion, and splitting"
      ],
      "metadata": {
        "id": "Wvs0FzPmVhcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set numpy print options\n",
        "np.set_printoptions(formatter={'float': '{: 0.2f}'.format})"
      ],
      "metadata": {
        "id": "kCG6ZQEeXW3l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert DF to numpy array for normalizing\n",
        "np_data = df.to_numpy()\n",
        "np_data[:5, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yLqqUu9LREU",
        "outputId": "673d4877-f7a9-4195-febf-6ab7389980bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 40.00,  1.00,  2.00,  140.00,  289.00,  0.00,  0.00,  172.00,\n",
              "         0.00,  0.00,  1.00,  0.00],\n",
              "       [ 49.00,  0.00,  3.00,  160.00,  180.00,  0.00,  0.00,  156.00,\n",
              "         0.00,  1.00,  2.00,  1.00],\n",
              "       [ 37.00,  1.00,  2.00,  130.00,  283.00,  0.00,  1.00,  98.00,\n",
              "         0.00,  0.00,  1.00,  0.00],\n",
              "       [ 48.00,  0.00,  4.00,  138.00,  214.00,  0.00,  0.00,  108.00,\n",
              "         1.00,  1.50,  2.00,  1.00],\n",
              "       [ 54.00,  1.00,  3.00,  150.00,  195.00,  0.00,  0.00,  122.00,\n",
              "         0.00,  0.00,  1.00,  0.00]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "np.random.shuffle(np_data)"
      ],
      "metadata": {
        "id": "2etAXAJUUoDD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just checking shuffle\n",
        "np_data[:5, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U57gwf7ZYux1",
        "outputId": "b9070924-2378-4327-ff5a-757c1c123391"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 44.00,  1.00,  4.00,  135.00,  491.00,  0.00,  0.00,  135.00,\n",
              "         0.00,  0.00,  2.00,  1.00],\n",
              "       [ 61.00,  0.00,  4.00,  130.00,  330.00,  0.00,  2.00,  169.00,\n",
              "         0.00,  0.00,  1.00,  1.00],\n",
              "       [ 38.00,  1.00,  3.00,  115.00,  0.00,  0.00,  0.00,  128.00,\n",
              "         1.00,  0.00,  2.00,  1.00],\n",
              "       [ 57.00,  1.00,  4.00,  95.00,  0.00,  1.00,  0.00,  182.00,\n",
              "         0.00,  0.70,  3.00,  1.00],\n",
              "       [ 64.00,  1.00,  4.00,  144.00,  0.00,  0.00,  1.00,  122.00,\n",
              "         1.00,  1.00,  2.00,  1.00]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# integer for splitting the data in the next steps\n",
        "index_20percent = int(0.2 * len(np_data[:, 0]))\n",
        "index_20percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK74h2dQZAO1",
        "outputId": "15627d0f-a353-4997-b83a-c531ebe84f53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove last column (target)\n",
        "XVALID = np_data[:index_20percent, :-1]\n",
        "YVALID = np_data[:index_20percent, -1]\n",
        "\n",
        "XTRAIN = np_data[index_20percent:, :-1]\n",
        "YTRAIN = np_data[index_20percent:, -1]"
      ],
      "metadata": {
        "id": "F5MtiY3eZT0d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(XTRAIN[:, 0])\n",
        "plt.ylabel('age')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ul3SRLBFZs2z",
        "outputId": "0b4a1d7c-dfee-4fa7-b636-4ffb5c45a755"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(XTRAIN[:, 3])\n",
        "plt.ylabel('resting bp')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "y42SjwAgX68t",
        "outputId": "f02fd429-5776-4f08-91b2-1f71ab8c6559"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(YTRAIN)\n",
        "plt.ylabel('train output')\n",
        "plt.show()\n",
        "\n",
        "plt.hist(YVALID)\n",
        "plt.ylabel('Validation output')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "ioo98VAUaHtG",
        "outputId": "cf9814dc-1720-4655-ba5d-e9569f180e4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = XTRAIN.mean(axis=0)\n",
        "std = XTRAIN.std(axis=0)\n",
        "\n",
        "XTRAIN -= mean\n",
        "XTRAIN /= std\n",
        "\n",
        "XVALID -= mean\n",
        "XVALID /= std"
      ],
      "metadata": {
        "id": "gKwfwrgWaboq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean)\n",
        "print(std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMd2JRPayB9",
        "outputId": "3ac8931a-de98-40e8-a72e-66cab84609af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 53.70  0.77  3.22  132.28  208.96  0.21  0.68  139.93  0.39  0.93  1.62]\n",
            "[ 9.24  0.42  0.94  18.53  99.32  0.41  0.86  25.45  0.49  1.10  0.62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the values are normalize between 0 and 1\n",
        "plt.hist(XTRAIN[:, 0])\n",
        "plt.ylabel('age column')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "cc2dAKq8a3qd",
        "outputId": "0af7ea1d-ab0e-44aa-e0ac-405aab6dc3cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase - 2: Intentional Overfit\n",
        "- For this phase **do not** split data into training and validation\n"
      ],
      "metadata": {
        "id": "PhWsOzk9kh58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input"
      ],
      "metadata": {
        "id": "I8K2qH2qlZ_S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np_data[:, :-1]\n",
        "Y = np_data[:, -1]"
      ],
      "metadata": {
        "id": "1saxsGE-ma_3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, Y.shape)\n",
        "print(X.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATGse2tsmivI",
        "outputId": "9cc9a3cc-5595-4134-e8a1-2a28879fca42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1190, 11) (1190,)\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit with 1 neuron (No overfit ~83% acc)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "fzXL3a5Am7I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up network architecture\n",
        "model = Sequential(name=\"overfit_one_neuron\")\n",
        "model.add(Input(shape=(X.shape[1],)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZXUBMDv1ll-s"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b73oEGvpqeZG",
        "outputId": "4ef499cb-8d5f-4727-eb86-6e8883576a05",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 1s 1ms/step - loss: 1.1680 - accuracy: 0.3101\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 1.0771 - accuracy: 0.3361\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.9970 - accuracy: 0.3672\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.9226 - accuracy: 0.4143\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.8534 - accuracy: 0.4622\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.7927 - accuracy: 0.5210\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.7378 - accuracy: 0.5874\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.6345\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6484 - accuracy: 0.6706\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6117 - accuracy: 0.7151\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.7513\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5521 - accuracy: 0.7622\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7697\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5100 - accuracy: 0.7832\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7933\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.7891\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.7992\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.8050\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8092\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.8118\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8134\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8151\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8176\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8193\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8168\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4150 - accuracy: 0.8193\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8218\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.8252\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8261\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4050 - accuracy: 0.8269\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8286\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4019 - accuracy: 0.8294\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4005 - accuracy: 0.8286\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8277\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8261\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8269\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3959 - accuracy: 0.8277\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8311\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3943 - accuracy: 0.8319\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8336\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3932 - accuracy: 0.8328\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8353\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8353\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8345\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8336\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3909 - accuracy: 0.8336\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8336\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3903 - accuracy: 0.8353\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8336\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8353\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8328\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8319\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.8345\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8345\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.8345\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8345\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8345\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.8345\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8345\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8345\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8345\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8336\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8353\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8345\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8336\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8303\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8319\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8303\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8286\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8311\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8311\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8294\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8328\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8294\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8345\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8311\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8336\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8353\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8345\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8336\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8336\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8345\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8345\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8345\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8336\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8353\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8345\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8361\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8345\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8353\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8311\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8319\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8345\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8336\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8336\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8303\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8294\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8303\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8311\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8311\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8319\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8328\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e785a31c6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit with a 2x1 network (No overfit ~85% acc)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "9Cnj7q4CnAVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons\n",
        "overfit_model = Sequential(name=\"overfit_2x1\")\n",
        "overfit_model.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model.add(Dense(2, activation='relu'))\n",
        "overfit_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dak-Q8j1slgY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_model.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC-ZjzgEdsI2",
        "outputId": "a19917f2-174b-4742-c524-9e8aaf73e515",
        "collapsed": true
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.7482 - accuracy: 0.4571\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.6487\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6484 - accuracy: 0.6983\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.7496\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5928 - accuracy: 0.7681\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5721 - accuracy: 0.7832\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5547 - accuracy: 0.7924\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7950\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.8000\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.8042\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5017 - accuracy: 0.8084\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.8126\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4830 - accuracy: 0.8143\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.8143\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.8168\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.8227\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.8227\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8277\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.8294\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.8303\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.8311\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8311\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8319\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.8311\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8328\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8345\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8361\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8361\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4222 - accuracy: 0.8361\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4204 - accuracy: 0.8378\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4187 - accuracy: 0.8378\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4169 - accuracy: 0.8387\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4153 - accuracy: 0.8378\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.8353\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4122 - accuracy: 0.8336\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8336\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4098 - accuracy: 0.8319\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8353\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8345\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4063 - accuracy: 0.8345\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8345\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4040 - accuracy: 0.8361\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8361\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4017 - accuracy: 0.8353\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8345\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8336\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8328\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3970 - accuracy: 0.8319\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3961 - accuracy: 0.8328\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8319\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8336\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3938 - accuracy: 0.8303\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3929 - accuracy: 0.8336\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8353\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8328\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8311\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8328\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.8353\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8319\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8345\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8336\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8336\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8319\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8370\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8370\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8395\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3827 - accuracy: 0.8370\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8387\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3817 - accuracy: 0.8403\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8370\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8403\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8403\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8412\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8412\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8412\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8437\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8429\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8420\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8412\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3780 - accuracy: 0.8420\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8429\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8429\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3771 - accuracy: 0.8437\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3768 - accuracy: 0.8429\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3764 - accuracy: 0.8445\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8429\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8412\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3757 - accuracy: 0.8412\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3755 - accuracy: 0.8437\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8403\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8420\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8429\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3746 - accuracy: 0.8412\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8429\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8437\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8412\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8420\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8420\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3738 - accuracy: 0.8429\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8420\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8437\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3734 - accuracy: 0.8370\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8395\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8387\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8403\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8395\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8387\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8395\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8361\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.8395\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8378\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8353\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8361\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8378\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8378\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8378\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8353\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8378\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8370\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3717 - accuracy: 0.8378\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3717 - accuracy: 0.8370\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8378\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8345\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8361\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8361\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3712 - accuracy: 0.8345\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8370\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8370\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8387\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8378\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8395\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8403\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8378\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8412\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8387\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8387\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8387\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8403\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8395\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8420\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8420\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8420\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8395\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8412\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8420\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8412\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8395\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8395\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8395\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8395\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8403\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3689 - accuracy: 0.8395\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8395\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8395\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8395\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8395\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8378\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8395\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8403\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8395\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8395\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8395\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8395\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8395\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8387\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8395\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8387\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8378\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8403\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8387\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8395\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8412\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8412\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8403\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8412\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8395\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8420\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3668 - accuracy: 0.8420\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8420\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8420\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3666 - accuracy: 0.8412\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3665 - accuracy: 0.8429\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8412\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8420\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8420\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8429\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3662 - accuracy: 0.8429\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8429\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8429\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8420\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8420\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8412\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8412\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8429\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3657 - accuracy: 0.8412\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3657 - accuracy: 0.8412\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8412\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8412\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3655 - accuracy: 0.8412\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8412\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3652 - accuracy: 0.8387\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8412\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8387\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8412\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8420\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8420\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8412\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8412\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8403\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8403\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8403\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8412\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8403\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8403\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8429\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8429\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8437\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8420\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8429\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8445\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8445\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8420\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8420\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8429\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8429\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8437\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.8471\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8445\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8437\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8471\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8471\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8479\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8462\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3623 - accuracy: 0.8479\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8479\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8479\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8462\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8479\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8479\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8479\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8479\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8479\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8471\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8479\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.8479\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8479\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.8471\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8479\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8471\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.8479\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8471\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8479\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8479\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8479\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e70545df6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit with 8x1 network (No overfit ~90% acc)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "1xY9JRJpn1V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons\n",
        "overfit_model_2 = Sequential(name=\"overfit_8x1\")\n",
        "overfit_model_2.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_2.add(Dense(8, activation='relu'))\n",
        "overfit_model_2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "overfit_model_2.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "id": "ioKNvJYIhlAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3019e7e9-4540-47d7-d2b7-130c9f71c5c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4980 - accuracy: 0.7782\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4659 - accuracy: 0.8008\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4462 - accuracy: 0.8042\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8126\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4216 - accuracy: 0.8118\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8151\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8193\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4023 - accuracy: 0.8261\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8294\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8294\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3918 - accuracy: 0.8319\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8319\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8319\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8336\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8336\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8311\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8336\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8345\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8353\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8361\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8345\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8361\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8353\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8370\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8378\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8412\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8403\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8403\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8403\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8429\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3598 - accuracy: 0.8437\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.8429\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8445\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8445\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8437\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8462\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8454\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8454\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8462\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3529 - accuracy: 0.8454\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8471\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.8471\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8479\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3500 - accuracy: 0.8479\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.8462\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8462\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8462\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8454\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8471\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8462\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8462\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8454\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8462\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8445\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8471\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3427 - accuracy: 0.8462\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8496\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3415 - accuracy: 0.8496\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8487\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8496\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8496\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8487\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8504\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3378 - accuracy: 0.8504\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8496\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8496\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8538\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8546\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8538\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8571\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8571\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8588\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8571\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8580\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8580\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8622\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8613\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8622\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8613\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8630\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8630\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8639\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3266 - accuracy: 0.8605\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8605\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8630\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8613\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8639\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8613\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8630\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8647\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3229 - accuracy: 0.8630\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8639\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8630\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8622\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8630\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.8639\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8647\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8647\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3194 - accuracy: 0.8630\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8630\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8639\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8639\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8647\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3173 - accuracy: 0.8655\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8647\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8655\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8655\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8655\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3154 - accuracy: 0.8647\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8664\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3146 - accuracy: 0.8655\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8655\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.8630\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8647\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8664\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.8672\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8672\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.8647\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8647\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8655\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8639\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8639\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8672\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8647\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8672\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8681\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8697\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8697\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8706\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3087 - accuracy: 0.8664\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8681\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8697\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8731\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8731\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8714\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8731\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8731\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8723\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8739\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8731\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.8748\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8773\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3054 - accuracy: 0.8773\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8756\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8748\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8773\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3044 - accuracy: 0.8790\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8765\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8782\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8773\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8807\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.8790\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8773\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8773\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8782\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3027 - accuracy: 0.8773\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3025 - accuracy: 0.8773\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.8773\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8782\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8782\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8798\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8765\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8782\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8773\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8756\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8782\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8765\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8790\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8773\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8765\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.8773\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8790\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8798\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8798\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8832\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8807\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8782\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8807\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8815\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8815\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8807\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8849\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8824\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8832\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8798\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8832\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2990 - accuracy: 0.8832\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2990 - accuracy: 0.8824\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8824\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8840\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8849\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8866\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8849\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8849\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8849\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8857\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8857\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8874\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8857\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8891\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8866\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8866\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8866\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8840\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.8849\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2978 - accuracy: 0.8866\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8857\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8866\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8857\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2978 - accuracy: 0.8857\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8866\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8866\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.8849\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8849\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8874\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8882\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8874\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8857\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8849\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8840\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8874\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8882\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8840\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8866\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8874\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8857\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8857\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8874\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8882\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8891\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8866\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8882\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8916\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8916\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8916\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8866\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8891\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8857\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8874\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8874\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8916\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8908\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8908\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8899\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8916\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8891\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8882\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8908\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8916\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8899\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8899\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8899\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8899\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.8924\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8899\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e70544d5ea0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit 8x4x1 (No overfit ~90% acc)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "0Pfy6CaEoLqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons and another layer\n",
        "overfit_model_3 = Sequential(name=\"overfit_8x4x1\")\n",
        "overfit_model_3.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_3.add(Dense(8, activation='relu'))\n",
        "overfit_model_3.add(Dense(4, activation='relu'))\n",
        "overfit_model_3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_3.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "overfit_model_3.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "id": "-rb88mRkiNOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "43e2e26a-d2e1-480d-b2a1-aedc69a790e6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5891\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6508 - accuracy: 0.6966\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6265 - accuracy: 0.7521\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.6040 - accuracy: 0.7706\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.7908\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5682 - accuracy: 0.7983\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.8050\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.8092\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.8126\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5206 - accuracy: 0.8143\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5117 - accuracy: 0.8227\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.8269\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4959 - accuracy: 0.8319\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4889 - accuracy: 0.8437\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4819 - accuracy: 0.8445\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.8429\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4694 - accuracy: 0.8403\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.8462\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.8429\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4536 - accuracy: 0.8462\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4488 - accuracy: 0.8487\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.8479\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.8504\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.8521\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8538\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8521\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8538\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8546\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.8571\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4144 - accuracy: 0.8555\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.8538\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8571\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4058 - accuracy: 0.8555\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.8563\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4005 - accuracy: 0.8597\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3979 - accuracy: 0.8571\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3957 - accuracy: 0.8597\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8580\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8597\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8597\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8571\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3860 - accuracy: 0.8580\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8613\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3827 - accuracy: 0.8613\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3810 - accuracy: 0.8597\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8605\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8597\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.8613\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3746 - accuracy: 0.8639\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8622\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8605\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8605\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8613\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8622\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3666 - accuracy: 0.8622\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8622\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8605\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8630\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8622\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8639\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3593 - accuracy: 0.8630\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8647\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8639\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8639\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8655\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3543 - accuracy: 0.8655\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8664\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8681\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8681\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8689\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8706\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8697\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8697\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8697\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8697\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8706\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3443 - accuracy: 0.8706\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8731\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8748\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8748\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8782\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8765\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8790\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8773\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3390 - accuracy: 0.8790\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8815\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8824\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8824\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8824\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8824\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8849\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8832\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8832\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8832\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8798\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8798\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3317 - accuracy: 0.8815\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8832\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8840\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8857\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8815\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8815\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.8866\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8782\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8824\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8840\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8832\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8807\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8832\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8824\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8840\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8832\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8857\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8832\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.8849\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8832\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8815\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3233 - accuracy: 0.8840\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8832\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8832\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8824\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8849\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8866\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8824\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8849\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8849\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8849\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.8840\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.8866\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8857\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8866\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8840\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3194 - accuracy: 0.8874\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8849\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8857\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8866\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.8866\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8866\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8866\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8866\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8866\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3173 - accuracy: 0.8866\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3172 - accuracy: 0.8874\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8874\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8874\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3164 - accuracy: 0.8874\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.8891\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8874\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8891\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8882\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8874\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8882\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3146 - accuracy: 0.8882\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8882\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3141 - accuracy: 0.8891\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8891\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3138 - accuracy: 0.8899\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3138 - accuracy: 0.8899\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8891\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.8908\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.8882\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8891\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8908\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3125 - accuracy: 0.8908\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.8916\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8916\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3120 - accuracy: 0.8916\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3116 - accuracy: 0.8891\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8916\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8924\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8916\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8916\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8908\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3107 - accuracy: 0.8924\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.8916\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8908\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8933\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8916\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8908\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8916\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8924\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8916\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8924\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8933\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.8950\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.8924\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.8899\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8908\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3085 - accuracy: 0.8916\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8924\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8924\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8899\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8916\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3076 - accuracy: 0.8908\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8924\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8916\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8933\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8916\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8899\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8908\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8924\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8916\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8941\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8924\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8933\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8916\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8966\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8933\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8941\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8950\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8933\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3044 - accuracy: 0.8933\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3041 - accuracy: 0.8950\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8950\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8933\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8950\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8941\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8950\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8950\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8916\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8941\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8924\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.8966\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.8950\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8941\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8966\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3024 - accuracy: 0.8966\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8958\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.8950\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8941\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8966\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8975\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8958\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8992\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8975\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8983\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8950\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8975\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8983\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8966\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8958\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8975\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8983\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8958\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.9000\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.9000\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8992\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8992\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8992\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8975\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2998 - accuracy: 0.8992\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.9017\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8975\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.9008\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8992\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8983\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e70544d5000>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit 64x4x1 (success)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "RIBWuzTZo0rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons to first layer\n",
        "overfit_model_4 = Sequential(name=\"overfit_64x4x1\")\n",
        "overfit_model_4.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_4.add(Dense(64, activation='relu'))\n",
        "overfit_model_4.add(Dense(4, activation='relu'))\n",
        "overfit_model_4.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_4.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "overfit_model_4.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlkoFTcoivIr",
        "outputId": "8bb4bce3-9aed-4f8e-d19c-94819b3ae04e",
        "collapsed": true
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6782\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.7866\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8151\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8311\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8395\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8395\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8487\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8521\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8571\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3500 - accuracy: 0.8605\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8605\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8639\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8639\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8647\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8664\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8681\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8681\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8706\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8723\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3125 - accuracy: 0.8731\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8723\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8714\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8739\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8773\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8773\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8773\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.8798\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2923 - accuracy: 0.8798\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8765\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.8840\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8782\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.8798\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.8866\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2782 - accuracy: 0.8832\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8857\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8866\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8849\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8857\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.8874\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8882\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8916\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8882\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.8874\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2564 - accuracy: 0.8924\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2538 - accuracy: 0.8966\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8975\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9034\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.8975\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.8983\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.8975\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.8992\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9034\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.9000\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9092\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2340 - accuracy: 0.9050\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.9076\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9067\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9109\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9101\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9126\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9134\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9151\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9101\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9160\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9143\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9160\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9185\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9176\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9218\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9202\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9210\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9227\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9235\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9244\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9218\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9252\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9252\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9277\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9269\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9294\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9269\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9328\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9269\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9353\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9311\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9328\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9378\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9361\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9353\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9336\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9361\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9353\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9361\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9378\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9395\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9420\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9403\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9403\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9420\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9437\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9403\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9412\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9437\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9454\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9445\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.9471\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9487\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9471\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9471\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1506 - accuracy: 0.9479\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9504\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9496\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9521\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9487\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1446 - accuracy: 0.9487\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9496\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9521\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9487\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9555\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9529\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9563\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9555\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9538\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9513\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9538\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9588\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9538\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9563\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9571\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9580\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9580\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9571\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9580\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9588\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9605\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9563\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9571\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9613\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9630\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9630\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9605\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9613\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9630\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9647\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9639\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9622\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9664\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9647\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9681\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9630\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9706\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9655\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9664\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9697\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9681\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9714\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9672\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9689\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9664\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9689\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9655\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9697\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9689\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9714\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9689\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9706\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9714\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9714\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9714\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9689\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9723\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9723\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.9706\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9714\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9756\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9706\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9731\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9739\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9689\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9765\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0863 - accuracy: 0.9731\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9731\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9739\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9748\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9748\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9731\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9756\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9756\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9739\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0803 - accuracy: 0.9765\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9782\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9748\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9739\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9756\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9739\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9790\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9756\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9748\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9756\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9798\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9790\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9773\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9815\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9815\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9782\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9815\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9790\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9798\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9832\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9782\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9807\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9798\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9815\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9807\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9807\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9840\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9815\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9815\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9815\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9832\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9824\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9824\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9824\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.9815\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9824\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9849\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9832\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0545 - accuracy: 0.9832\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9832\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9824\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9824\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9849\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9832\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9832\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9849\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9832\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9832\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9857\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9840\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0484 - accuracy: 0.9849\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9840\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9840\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 0.9857\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9849\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9832\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0458 - accuracy: 0.9849\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9849\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9857\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9849\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0435 - accuracy: 0.9866\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9874\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9849\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9840\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9849\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9857\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9866\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6f943b29b0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit 64x32x1 (success)\n",
        "- 100 epochs"
      ],
      "metadata": {
        "id": "0ppaXn7PpT5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons\n",
        "overfit_model_5 = Sequential(name=\"overfit_64x32x1\")\n",
        "overfit_model_5.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_5.add(Dense(64, activation='relu'))\n",
        "overfit_model_5.add(Dense(32, activation='relu'))\n",
        "overfit_model_5.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_5.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "overfit_model_5.fit(X,Y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi94E1BAjKnl",
        "outputId": "f252f7e2-28b7-4a5e-e9c7-e208420ea3a3",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 1s 1ms/step - loss: 0.5720 - accuracy: 0.7563\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8176\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8395\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8429\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3492 - accuracy: 0.8479\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8546\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8588\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8622\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8655\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8655\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8714\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8748\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8832\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.8866\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8874\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.8849\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8958\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.8933\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8941\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8924\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8933\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8958\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9000\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.9025\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9067\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9000\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9143\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9126\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9143\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9126\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9151\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9168\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9202\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9202\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9210\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9252\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9311\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9336\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9345\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9319\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9395\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9319\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9420\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9387\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9462\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9403\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9471\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1660 - accuracy: 0.9462\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9504\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9538\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9513\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9521\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9538\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9597\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9605\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9588\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9605\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9605\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9630\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9655\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9647\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9622\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9630\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9681\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9689\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1180 - accuracy: 0.9706\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9672\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9697\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9731\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9706\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9714\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9689\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9723\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9723\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9739\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9765\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9731\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9756\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9756\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9765\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9773\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9765\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9773\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9798\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9790\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9807\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9824\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9790\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9849\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9824\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9849\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9832\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9824\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9824\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9857\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9815\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9874\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9857\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9857\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6f942beef0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit 500x1 (success)\n",
        "- 256 epochs"
      ],
      "metadata": {
        "id": "7QTJMZU7pmKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons\n",
        "overfit_model_6 = Sequential(name=\"overfit_mode_6\")\n",
        "overfit_model_6.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_6.add(Dense(500, activation='relu'))\n",
        "overfit_model_6.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_6.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "overfit_model_6.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFADBToejg_O",
        "outputId": "5366d7ce-bdd0-40be-e770-bb9f4a161f44",
        "collapsed": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.8025\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8395\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3593 - accuracy: 0.8412\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8496\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8546\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8622\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3240 - accuracy: 0.8630\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3173 - accuracy: 0.8689\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.8756\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8790\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.8798\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8874\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8874\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8882\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8866\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2829 - accuracy: 0.8874\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8882\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8933\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8891\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.8866\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.8916\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.8950\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2634 - accuracy: 0.9008\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.8983\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.8933\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.8958\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8958\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.8992\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.8983\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.8966\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.9034\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9050\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9042\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9042\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9118\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.9067\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9109\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9134\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9143\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9143\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9126\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9118\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9210\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9202\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9176\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9160\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9185\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9176\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9261\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9252\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9227\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9210\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9261\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9370\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9261\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9311\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9328\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9311\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9336\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9345\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9370\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9336\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9395\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9319\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9420\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9403\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9412\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9487\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9445\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9513\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9429\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9462\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9462\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9487\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9437\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9504\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1577 - accuracy: 0.9504\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9521\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9563\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9563\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9580\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9538\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9513\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9546\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9529\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9597\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9588\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9588\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9613\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9613\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9630\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9655\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9605\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9605\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9613\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9639\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9664\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9655\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9681\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9655\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9639\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9689\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9689\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9681\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9664\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9714\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9723\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9672\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9739\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9681\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9697\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9706\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9773\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9756\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9723\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9756\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9756\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9756\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9765\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9756\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9748\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9790\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9748\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9714\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9807\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9765\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9773\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9798\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9790\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9807\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9807\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9773\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9824\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9798\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9782\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9807\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9815\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9815\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9815\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9798\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9798\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9832\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9824\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9840\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9832\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9866\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9857\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9840\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9832\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9882\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9874\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9840\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9849\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9874\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9840\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9866\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9882\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9849\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9857\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9908\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9874\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9891\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9849\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9899\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9874\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9899\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9874\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9916\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9891\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9924\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9916\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9882\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9924\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 0.9916\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9891\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9908\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9908\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9924\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9891\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0435 - accuracy: 0.9924\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.9924\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9899\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9908\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9916\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9933\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9908\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9950\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9958\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9958\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9933\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.9933\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0357 - accuracy: 0.9958\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9950\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9924\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9933\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9975\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9941\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9958\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9958\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0311 - accuracy: 0.9950\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.9966\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9966\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9958\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9966\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9983\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9966\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9983\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9983\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0285 - accuracy: 0.9950\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9983\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9958\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9958\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9983\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9975\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9992\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9958\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9966\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9975\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9992\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9992\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9975\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9983\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9983\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9992\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9975\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9992\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9983\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9992\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9992\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9992\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9975\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9992\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 1.0000\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9983\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.9992\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9992\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9992\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9992\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9992\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 0.9992\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9992\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 0.9992\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 0.9992\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.9983\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6f941ce380>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add more neurons\n",
        "overfit_model_6 = Sequential(name=\"overfit_mode_6\")\n",
        "overfit_model_6.add(Input(shape=(X.shape[1],)))\n",
        "overfit_model_6.add(Dense(8, activation='relu'))\n",
        "overfit_model_6.add(Dense(8, activation='relu'))\n",
        "overfit_model_6.add(Dense(8, activation='relu'))\n",
        "overfit_model_6.add(Dense(8, activation='relu'))\n",
        "overfit_model_6.add(Dense(8, activation='relu'))\n",
        "overfit_model_6.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile new overfit model\n",
        "overfit_model_6.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "overfit_model_6.fit(X,Y, epochs=256, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STN94KXpkpPT",
        "outputId": "bdfa8d3d-d958-4888-9def-f6139939728c",
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.7622\n",
            "Epoch 2/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7723\n",
            "Epoch 3/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7832\n",
            "Epoch 4/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7924\n",
            "Epoch 5/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8017\n",
            "Epoch 6/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8101\n",
            "Epoch 7/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8176\n",
            "Epoch 8/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8235\n",
            "Epoch 9/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8286\n",
            "Epoch 10/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8277\n",
            "Epoch 11/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8336\n",
            "Epoch 12/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8303\n",
            "Epoch 13/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8345\n",
            "Epoch 14/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.8387\n",
            "Epoch 15/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8370\n",
            "Epoch 16/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8420\n",
            "Epoch 17/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8429\n",
            "Epoch 18/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8445\n",
            "Epoch 19/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8471\n",
            "Epoch 20/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8529\n",
            "Epoch 21/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8521\n",
            "Epoch 22/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8521\n",
            "Epoch 23/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8529\n",
            "Epoch 24/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8555\n",
            "Epoch 25/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8546\n",
            "Epoch 26/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8563\n",
            "Epoch 27/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8580\n",
            "Epoch 28/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8571\n",
            "Epoch 29/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8563\n",
            "Epoch 30/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8605\n",
            "Epoch 31/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8588\n",
            "Epoch 32/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8605\n",
            "Epoch 33/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8622\n",
            "Epoch 34/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8597\n",
            "Epoch 35/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8605\n",
            "Epoch 36/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8613\n",
            "Epoch 37/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8588\n",
            "Epoch 38/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8622\n",
            "Epoch 39/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8664\n",
            "Epoch 40/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8639\n",
            "Epoch 41/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8647\n",
            "Epoch 42/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8689\n",
            "Epoch 43/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8664\n",
            "Epoch 44/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8714\n",
            "Epoch 45/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8714\n",
            "Epoch 46/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8706\n",
            "Epoch 47/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8773\n",
            "Epoch 48/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8706\n",
            "Epoch 49/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8706\n",
            "Epoch 50/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8756\n",
            "Epoch 51/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8807\n",
            "Epoch 52/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8739\n",
            "Epoch 53/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8798\n",
            "Epoch 54/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8815\n",
            "Epoch 55/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8807\n",
            "Epoch 56/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8874\n",
            "Epoch 57/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8866\n",
            "Epoch 58/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8849\n",
            "Epoch 59/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.8891\n",
            "Epoch 60/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8899\n",
            "Epoch 61/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8891\n",
            "Epoch 62/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8891\n",
            "Epoch 63/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8874\n",
            "Epoch 64/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8882\n",
            "Epoch 65/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8891\n",
            "Epoch 66/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8966\n",
            "Epoch 67/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.8908\n",
            "Epoch 68/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8916\n",
            "Epoch 69/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8983\n",
            "Epoch 70/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8983\n",
            "Epoch 71/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8941\n",
            "Epoch 72/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8992\n",
            "Epoch 73/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9034\n",
            "Epoch 74/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.8950\n",
            "Epoch 75/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2702 - accuracy: 0.9017\n",
            "Epoch 76/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2688 - accuracy: 0.9017\n",
            "Epoch 77/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.9008\n",
            "Epoch 78/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.9000\n",
            "Epoch 79/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.9034\n",
            "Epoch 80/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2643 - accuracy: 0.9025\n",
            "Epoch 81/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.9042\n",
            "Epoch 82/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9101\n",
            "Epoch 83/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9025\n",
            "Epoch 84/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.9076\n",
            "Epoch 85/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9101\n",
            "Epoch 86/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9050\n",
            "Epoch 87/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9092\n",
            "Epoch 88/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9084\n",
            "Epoch 89/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.9059\n",
            "Epoch 90/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.9067\n",
            "Epoch 91/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.9076\n",
            "Epoch 92/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9084\n",
            "Epoch 93/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.9084\n",
            "Epoch 94/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2496 - accuracy: 0.9109\n",
            "Epoch 95/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.9126\n",
            "Epoch 96/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9151\n",
            "Epoch 97/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9101\n",
            "Epoch 98/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9134\n",
            "Epoch 99/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.9151\n",
            "Epoch 100/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9126\n",
            "Epoch 101/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.9143\n",
            "Epoch 102/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9185\n",
            "Epoch 103/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 0.9151\n",
            "Epoch 104/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9176\n",
            "Epoch 105/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.9151\n",
            "Epoch 106/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9176\n",
            "Epoch 107/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.9185\n",
            "Epoch 108/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9218\n",
            "Epoch 109/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9168\n",
            "Epoch 110/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9168\n",
            "Epoch 111/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9168\n",
            "Epoch 112/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2351 - accuracy: 0.9185\n",
            "Epoch 113/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2342 - accuracy: 0.9218\n",
            "Epoch 114/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9193\n",
            "Epoch 115/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.9218\n",
            "Epoch 116/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9176\n",
            "Epoch 117/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9168\n",
            "Epoch 118/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9202\n",
            "Epoch 119/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9218\n",
            "Epoch 120/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.9185\n",
            "Epoch 121/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9176\n",
            "Epoch 122/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9218\n",
            "Epoch 123/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.9193\n",
            "Epoch 124/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9235\n",
            "Epoch 125/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9143\n",
            "Epoch 126/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2232 - accuracy: 0.9193\n",
            "Epoch 127/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9202\n",
            "Epoch 128/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9252\n",
            "Epoch 129/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2216 - accuracy: 0.9244\n",
            "Epoch 130/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9185\n",
            "Epoch 131/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9261\n",
            "Epoch 132/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9227\n",
            "Epoch 133/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2167 - accuracy: 0.9261\n",
            "Epoch 134/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9193\n",
            "Epoch 135/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9311\n",
            "Epoch 136/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9261\n",
            "Epoch 137/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9252\n",
            "Epoch 138/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2164 - accuracy: 0.9252\n",
            "Epoch 139/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2137 - accuracy: 0.9244\n",
            "Epoch 140/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9227\n",
            "Epoch 141/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2145 - accuracy: 0.9210\n",
            "Epoch 142/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9227\n",
            "Epoch 143/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9210\n",
            "Epoch 144/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9252\n",
            "Epoch 145/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9252\n",
            "Epoch 146/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9252\n",
            "Epoch 147/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9218\n",
            "Epoch 148/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9218\n",
            "Epoch 149/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9252\n",
            "Epoch 150/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9277\n",
            "Epoch 151/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9235\n",
            "Epoch 152/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9252\n",
            "Epoch 153/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9269\n",
            "Epoch 154/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9218\n",
            "Epoch 155/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2055 - accuracy: 0.9269\n",
            "Epoch 156/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9269\n",
            "Epoch 157/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9261\n",
            "Epoch 158/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9269\n",
            "Epoch 159/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9227\n",
            "Epoch 160/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9303\n",
            "Epoch 161/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9294\n",
            "Epoch 162/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9261\n",
            "Epoch 163/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9261\n",
            "Epoch 164/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9244\n",
            "Epoch 165/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9294\n",
            "Epoch 166/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9311\n",
            "Epoch 167/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9294\n",
            "Epoch 168/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9336\n",
            "Epoch 169/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9286\n",
            "Epoch 170/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2021 - accuracy: 0.9269\n",
            "Epoch 171/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9294\n",
            "Epoch 172/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9286\n",
            "Epoch 173/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9294\n",
            "Epoch 174/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9319\n",
            "Epoch 175/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9277\n",
            "Epoch 176/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1989 - accuracy: 0.9303\n",
            "Epoch 177/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9319\n",
            "Epoch 178/256\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9303\n",
            "Epoch 179/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9336\n",
            "Epoch 180/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1968 - accuracy: 0.9303\n",
            "Epoch 181/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9311\n",
            "Epoch 182/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9294\n",
            "Epoch 183/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9328\n",
            "Epoch 184/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1958 - accuracy: 0.9328\n",
            "Epoch 185/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1949 - accuracy: 0.9345\n",
            "Epoch 186/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9336\n",
            "Epoch 187/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9311\n",
            "Epoch 188/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 0.9345\n",
            "Epoch 189/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9252\n",
            "Epoch 190/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9336\n",
            "Epoch 191/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9353\n",
            "Epoch 192/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9370\n",
            "Epoch 193/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9319\n",
            "Epoch 194/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9336\n",
            "Epoch 195/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.9353\n",
            "Epoch 196/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9303\n",
            "Epoch 197/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9311\n",
            "Epoch 198/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9328\n",
            "Epoch 199/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9345\n",
            "Epoch 200/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9319\n",
            "Epoch 201/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9353\n",
            "Epoch 202/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9336\n",
            "Epoch 203/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9370\n",
            "Epoch 204/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9345\n",
            "Epoch 205/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.9378\n",
            "Epoch 206/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9345\n",
            "Epoch 207/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9336\n",
            "Epoch 208/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1864 - accuracy: 0.9353\n",
            "Epoch 209/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9345\n",
            "Epoch 210/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1868 - accuracy: 0.9303\n",
            "Epoch 211/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9353\n",
            "Epoch 212/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1868 - accuracy: 0.9361\n",
            "Epoch 213/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9303\n",
            "Epoch 214/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9353\n",
            "Epoch 215/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9294\n",
            "Epoch 216/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9353\n",
            "Epoch 217/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9303\n",
            "Epoch 218/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9328\n",
            "Epoch 219/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.9387\n",
            "Epoch 220/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9336\n",
            "Epoch 221/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1823 - accuracy: 0.9361\n",
            "Epoch 222/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1824 - accuracy: 0.9353\n",
            "Epoch 223/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9361\n",
            "Epoch 224/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9345\n",
            "Epoch 225/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9328\n",
            "Epoch 226/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1810 - accuracy: 0.9361\n",
            "Epoch 227/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9370\n",
            "Epoch 228/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1802 - accuracy: 0.9378\n",
            "Epoch 229/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9311\n",
            "Epoch 230/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1800 - accuracy: 0.9336\n",
            "Epoch 231/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.9361\n",
            "Epoch 232/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9361\n",
            "Epoch 233/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1797 - accuracy: 0.9370\n",
            "Epoch 234/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1776 - accuracy: 0.9403\n",
            "Epoch 235/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9311\n",
            "Epoch 236/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9378\n",
            "Epoch 237/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1788 - accuracy: 0.9345\n",
            "Epoch 238/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9387\n",
            "Epoch 239/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1792 - accuracy: 0.9361\n",
            "Epoch 240/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1741 - accuracy: 0.9420\n",
            "Epoch 241/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9336\n",
            "Epoch 242/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9387\n",
            "Epoch 243/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9395\n",
            "Epoch 244/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9412\n",
            "Epoch 245/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9387\n",
            "Epoch 246/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9353\n",
            "Epoch 247/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9353\n",
            "Epoch 248/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9353\n",
            "Epoch 249/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9429\n",
            "Epoch 250/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.9420\n",
            "Epoch 251/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9412\n",
            "Epoch 252/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.9454\n",
            "Epoch 253/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1709 - accuracy: 0.9387\n",
            "Epoch 254/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9429\n",
            "Epoch 255/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1703 - accuracy: 0.9445\n",
            "Epoch 256/256\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.9420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6f607d6740>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 - Model selection & evaluation\n",
        "- Goal: obtain highest possible acc on validation set"
      ],
      "metadata": {
        "id": "gO0lpkPg8ji2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, F1Score"
      ],
      "metadata": {
        "id": "x28IP9r4SeFC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check train and validation sets\n",
        "print(XTRAIN.shape, YTRAIN.shape)\n",
        "print(XVALID.shape, YVALID.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZXNtTwtNZ0G",
        "outputId": "395c9f99-8b33-49c7-81f5-d7d7c5edecb7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(952, 11) (952,)\n",
            "(238, 11) (238,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create metric classes and model callbacks\n",
        "- Accuracy, Precision, Recall, F1Score"
      ],
      "metadata": {
        "id": "CgxGmTmPSrxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec = Precision()\n",
        "rec = Recall()\n",
        "f1 = F1Score()"
      ],
      "metadata": {
        "id": "jNOIco1WS5P9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(filepath = 'model.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min')"
      ],
      "metadata": {
        "id": "PEd0ULjRTpSw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a neural network logistic regression model\n",
        "- One output neuron with sigmoid\n",
        "- val_accuracy: 0.8319\n",
        "- val_loss: 0.3709\n",
        "- val_precision: 0.8382\n",
        "- val_recall: 0.8636"
      ],
      "metadata": {
        "id": "s6y3sBg3NmJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_regression_model = Sequential(name=\"baseline_regression_model\")\n",
        "baseline_regression_model.add(Input(shape=(X.shape[1],)))\n",
        "baseline_regression_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "baseline_regression_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n"
      ],
      "metadata": {
        "id": "lBdMnhlTNjja"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = baseline_regression_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x-1pDEyEUWBq",
        "outputId": "9dc9656c-edc4-4b00-db44-1ccf3b3b9f84"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 13s - loss: 1.3697 - accuracy: 0.1875 - precision: 0.2778 - recall: 0.2778\n",
            "Epoch 1: val_loss improved from inf to 1.15263, saving model to model.keras\n",
            "30/30 [==============================] - 1s 11ms/step - loss: 1.2222 - accuracy: 0.2763 - precision: 0.3028 - recall: 0.3010 - val_loss: 1.1526 - val_accuracy: 0.3193 - val_precision: 0.3772 - val_recall: 0.3209\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 1.0499 - accuracy: 0.4375 - precision: 0.4375 - recall: 0.4375\n",
            "Epoch 2: val_loss improved from 1.15263 to 1.08000, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1425 - accuracy: 0.2973 - precision: 0.3239 - recall: 0.3232 - val_loss: 1.0800 - val_accuracy: 0.3403 - val_precision: 0.3982 - val_recall: 0.3358\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 1.0779 - accuracy: 0.3125 - precision: 0.3125 - recall: 0.3125\n",
            "Epoch 3: val_loss improved from 1.08000 to 1.01063, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0716 - accuracy: 0.3183 - precision: 0.3416 - recall: 0.3354 - val_loss: 1.0106 - val_accuracy: 0.3613 - val_precision: 0.4211 - val_recall: 0.3582\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 1.0147 - accuracy: 0.3438 - precision: 0.3333 - recall: 0.4000\n",
            "Epoch 4: val_loss improved from 1.01063 to 0.94522, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0034 - accuracy: 0.3393 - precision: 0.3638 - recall: 0.3616 - val_loss: 0.9452 - val_accuracy: 0.3697 - val_precision: 0.4310 - val_recall: 0.3731\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 1.1374 - accuracy: 0.2188 - precision: 0.2941 - recall: 0.2778\n",
            "Epoch 5: val_loss improved from 0.94522 to 0.88401, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9405 - accuracy: 0.3666 - precision: 0.3916 - recall: 0.3939 - val_loss: 0.8840 - val_accuracy: 0.3950 - val_precision: 0.4576 - val_recall: 0.4030\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.9987 - accuracy: 0.2188 - precision: 0.2143 - recall: 0.1765\n",
            "Epoch 6: val_loss improved from 0.88401 to 0.82551, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8804 - accuracy: 0.3971 - precision: 0.4221 - recall: 0.4323 - val_loss: 0.8255 - val_accuracy: 0.4412 - val_precision: 0.5043 - val_recall: 0.4403\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.9916 - accuracy: 0.3125 - precision: 0.2500 - recall: 0.2857\n",
            "Epoch 7: val_loss improved from 0.82551 to 0.77188, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8249 - accuracy: 0.4422 - precision: 0.4641 - recall: 0.4707 - val_loss: 0.7719 - val_accuracy: 0.4706 - val_precision: 0.5345 - val_recall: 0.4627\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7633 - accuracy: 0.5625 - precision: 0.5556 - recall: 0.6250\n",
            "Epoch 8: val_loss improved from 0.77188 to 0.72262, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7738 - accuracy: 0.4842 - precision: 0.5039 - recall: 0.5172 - val_loss: 0.7226 - val_accuracy: 0.5336 - val_precision: 0.5983 - val_recall: 0.5224\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6806 - accuracy: 0.5312 - precision: 0.4444 - recall: 0.6154\n",
            "Epoch 9: val_loss improved from 0.72262 to 0.67925, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7282 - accuracy: 0.5263 - precision: 0.5437 - recall: 0.5535 - val_loss: 0.6792 - val_accuracy: 0.5882 - val_precision: 0.6500 - val_recall: 0.5821\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6933 - accuracy: 0.6250 - precision: 0.7333 - recall: 0.5789\n",
            "Epoch 10: val_loss improved from 0.67925 to 0.63769, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5714 - precision: 0.5858 - recall: 0.6000 - val_loss: 0.6377 - val_accuracy: 0.6345 - val_precision: 0.6911 - val_recall: 0.6343\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7608 - accuracy: 0.5312 - precision: 0.5556 - recall: 0.5882\n",
            "Epoch 11: val_loss improved from 0.63769 to 0.60015, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6469 - accuracy: 0.6187 - precision: 0.6294 - recall: 0.6485 - val_loss: 0.6002 - val_accuracy: 0.6807 - val_precision: 0.7377 - val_recall: 0.6716\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6000 - accuracy: 0.7188 - precision: 0.7857 - recall: 0.6471\n",
            "Epoch 12: val_loss improved from 0.60015 to 0.56683, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6120 - accuracy: 0.6618 - precision: 0.6673 - recall: 0.6970 - val_loss: 0.5668 - val_accuracy: 0.7101 - val_precision: 0.7600 - val_recall: 0.7090\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5598 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7059\n",
            "Epoch 13: val_loss improved from 0.56683 to 0.53688, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5809 - accuracy: 0.7069 - precision: 0.7101 - recall: 0.7374 - val_loss: 0.5369 - val_accuracy: 0.7605 - val_precision: 0.8130 - val_recall: 0.7463\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6170 - accuracy: 0.6562 - precision: 0.6429 - recall: 0.6000\n",
            "Epoch 14: val_loss improved from 0.53688 to 0.51153, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7353 - precision: 0.7378 - recall: 0.7616 - val_loss: 0.5115 - val_accuracy: 0.7983 - val_precision: 0.8413 - val_recall: 0.7910\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4709 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 15: val_loss improved from 0.51153 to 0.48902, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.7637 - precision: 0.7606 - recall: 0.7960 - val_loss: 0.4890 - val_accuracy: 0.8109 - val_precision: 0.8560 - val_recall: 0.7985\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5258 - accuracy: 0.7812 - precision: 0.7222 - recall: 0.8667\n",
            "Epoch 16: val_loss improved from 0.48902 to 0.47033, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5095 - accuracy: 0.7826 - precision: 0.7738 - recall: 0.8222 - val_loss: 0.4703 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4736 - accuracy: 0.7812 - precision: 0.7619 - recall: 0.8889\n",
            "Epoch 17: val_loss improved from 0.47033 to 0.45415, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4920 - accuracy: 0.7899 - precision: 0.7842 - recall: 0.8222 - val_loss: 0.4541 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4325 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 18: val_loss improved from 0.45415 to 0.44044, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.8015 - precision: 0.8000 - recall: 0.8242 - val_loss: 0.4404 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7188 - precision: 0.7619 - recall: 0.8000\n",
            "Epoch 19: val_loss improved from 0.44044 to 0.42901, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.8078 - precision: 0.8083 - recall: 0.8263 - val_loss: 0.4290 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4342 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 20: val_loss improved from 0.42901 to 0.41981, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.8099 - precision: 0.8127 - recall: 0.8242 - val_loss: 0.4198 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3905 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 21: val_loss improved from 0.41981 to 0.41180, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.8141 - precision: 0.8193 - recall: 0.8242 - val_loss: 0.4118 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4633 - accuracy: 0.8125 - precision: 0.8261 - recall: 0.9048\n",
            "Epoch 22: val_loss improved from 0.41180 to 0.40520, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8109 - precision: 0.8169 - recall: 0.8202 - val_loss: 0.4052 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5437 - accuracy: 0.7188 - precision: 0.7826 - recall: 0.8182\n",
            "Epoch 23: val_loss improved from 0.40520 to 0.39987, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.8099 - precision: 0.8153 - recall: 0.8202 - val_loss: 0.3999 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4721 - accuracy: 0.7500 - precision: 0.6471 - recall: 0.8462\n",
            "Epoch 24: val_loss improved from 0.39987 to 0.39538, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8130 - precision: 0.8189 - recall: 0.8222 - val_loss: 0.3954 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3414 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.9474\n",
            "Epoch 25: val_loss improved from 0.39538 to 0.39146, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8120 - precision: 0.8185 - recall: 0.8202 - val_loss: 0.3915 - val_accuracy: 0.8319 - val_precision: 0.8730 - val_recall: 0.8209\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5438 - accuracy: 0.7500 - precision: 0.7333 - recall: 0.7333\n",
            "Epoch 26: val_loss improved from 0.39146 to 0.38828, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8151 - precision: 0.8209 - recall: 0.8242 - val_loss: 0.3883 - val_accuracy: 0.8319 - val_precision: 0.8730 - val_recall: 0.8209\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3359 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 27: val_loss improved from 0.38828 to 0.38558, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8172 - precision: 0.8229 - recall: 0.8263 - val_loss: 0.3856 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4693 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 28: val_loss improved from 0.38558 to 0.38334, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4120 - accuracy: 0.8183 - precision: 0.8233 - recall: 0.8283 - val_loss: 0.3833 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2436 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 29: val_loss improved from 0.38334 to 0.38154, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8162 - precision: 0.8213 - recall: 0.8263 - val_loss: 0.3815 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4683 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 30: val_loss improved from 0.38154 to 0.38008, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8162 - precision: 0.8226 - recall: 0.8242 - val_loss: 0.3801 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3988 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 31: val_loss improved from 0.38008 to 0.37861, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8172 - precision: 0.8242 - recall: 0.8242 - val_loss: 0.3786 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4285 - accuracy: 0.8125 - precision: 0.7059 - recall: 0.9231\n",
            "Epoch 32: val_loss improved from 0.37861 to 0.37739, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8183 - precision: 0.8259 - recall: 0.8242 - val_loss: 0.3774 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4526 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 33: val_loss improved from 0.37739 to 0.37637, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4035 - accuracy: 0.8183 - precision: 0.8259 - recall: 0.8242 - val_loss: 0.3764 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5844 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.8750\n",
            "Epoch 34: val_loss improved from 0.37637 to 0.37538, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8183 - precision: 0.8272 - recall: 0.8222 - val_loss: 0.3754 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3788 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 35: val_loss improved from 0.37538 to 0.37460, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8225 - precision: 0.8327 - recall: 0.8242 - val_loss: 0.3746 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3674 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.8333\n",
            "Epoch 36: val_loss improved from 0.37460 to 0.37393, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4004 - accuracy: 0.8225 - precision: 0.8354 - recall: 0.8202 - val_loss: 0.3739 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2160 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 37: val_loss improved from 0.37393 to 0.37343, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8256 - precision: 0.8406 - recall: 0.8202 - val_loss: 0.3734 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 38: val_loss improved from 0.37343 to 0.37284, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8267 - precision: 0.8409 - recall: 0.8222 - val_loss: 0.3728 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4053 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 39: val_loss improved from 0.37284 to 0.37240, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3984 - accuracy: 0.8267 - precision: 0.8409 - recall: 0.8222 - val_loss: 0.3724 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4973 - accuracy: 0.7812 - precision: 0.9375 - recall: 0.7143\n",
            "Epoch 40: val_loss improved from 0.37240 to 0.37203, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8277 - precision: 0.8441 - recall: 0.8202 - val_loss: 0.3720 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3450 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.6923\n",
            "Epoch 41: val_loss improved from 0.37203 to 0.37164, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8277 - precision: 0.8441 - recall: 0.8202 - val_loss: 0.3716 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3524 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 42: val_loss improved from 0.37164 to 0.37139, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8267 - precision: 0.8423 - recall: 0.8202 - val_loss: 0.3714 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3659 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 43: val_loss improved from 0.37139 to 0.37111, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8288 - precision: 0.8430 - recall: 0.8242 - val_loss: 0.3711 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4875 - accuracy: 0.7188 - precision: 0.8571 - recall: 0.6316\n",
            "Epoch 44: val_loss improved from 0.37111 to 0.37091, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3709 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4012 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 45: val_loss improved from 0.37091 to 0.37076, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3960 - accuracy: 0.8298 - precision: 0.8419 - recall: 0.8283 - val_loss: 0.3708 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8125 - precision: 0.6923 - recall: 0.8182\n",
            "Epoch 46: val_loss improved from 0.37076 to 0.37066, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8298 - precision: 0.8419 - recall: 0.8283 - val_loss: 0.3707 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2946 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 47: val_loss improved from 0.37066 to 0.37046, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8298 - precision: 0.8419 - recall: 0.8283 - val_loss: 0.3705 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4306 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 48: val_loss improved from 0.37046 to 0.37034, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3703 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7593 - accuracy: 0.7188 - precision: 0.7619 - recall: 0.8000\n",
            "Epoch 49: val_loss improved from 0.37034 to 0.37022, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3702 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5522 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 50: val_loss improved from 0.37022 to 0.37009, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3701 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4311 - accuracy: 0.7812 - precision: 1.0000 - recall: 0.6818\n",
            "Epoch 51: val_loss improved from 0.37009 to 0.36986, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3699 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4486 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 52: val_loss improved from 0.36986 to 0.36970, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3697 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3179 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 53: val_loss improved from 0.36970 to 0.36962, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3696 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5226 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 54: val_loss improved from 0.36962 to 0.36957, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3696 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3700 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 55: val_loss improved from 0.36957 to 0.36940, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8309 - precision: 0.8422 - recall: 0.8303 - val_loss: 0.3694 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4350 - accuracy: 0.7500 - precision: 0.7857 - recall: 0.6875\n",
            "Epoch 56: val_loss improved from 0.36940 to 0.36929, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8288 - precision: 0.8388 - recall: 0.8303 - val_loss: 0.3693 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4231 - accuracy: 0.8438 - precision: 0.7143 - recall: 0.9091\n",
            "Epoch 57: val_loss improved from 0.36929 to 0.36919, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8298 - precision: 0.8405 - recall: 0.8303 - val_loss: 0.3692 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3311 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 58: val_loss improved from 0.36919 to 0.36914, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8298 - precision: 0.8391 - recall: 0.8323 - val_loss: 0.3691 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2620 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8947\n",
            "Epoch 59: val_loss did not improve from 0.36914\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8298 - precision: 0.8391 - recall: 0.8323 - val_loss: 0.3692 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3987 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 60: val_loss improved from 0.36914 to 0.36912, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3691 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5028 - accuracy: 0.8125 - precision: 0.7826 - recall: 0.9474\n",
            "Epoch 61: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3692 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4096 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 62: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3693 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4500 - accuracy: 0.8125 - precision: 0.8421 - recall: 0.8421\n",
            "Epoch 63: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3693 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4109 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 64: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3693 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 65: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8319 - precision: 0.8425 - recall: 0.8323 - val_loss: 0.3692 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2352 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8889\n",
            "Epoch 66: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8298 - precision: 0.8391 - recall: 0.8323 - val_loss: 0.3693 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3260 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.7727\n",
            "Epoch 67: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8309 - precision: 0.8408 - recall: 0.8323 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2624 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 68: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8319 - precision: 0.8425 - recall: 0.8323 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2760 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 69: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8340 - precision: 0.8446 - recall: 0.8343 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4187 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.8333\n",
            "Epoch 70: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3730 - accuracy: 0.8438 - precision: 0.7333 - recall: 0.9167\n",
            "Epoch 71: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8340 - precision: 0.8446 - recall: 0.8343 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4715 - accuracy: 0.7188 - precision: 0.8500 - recall: 0.7391\n",
            "Epoch 72: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3198 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 73: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5141 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 74: val_loss did not improve from 0.36912\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8361 - precision: 0.8466 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2492 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 75: val_loss improved from 0.36912 to 0.36907, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3691 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6545 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7059\n",
            "Epoch 76: val_loss improved from 0.36907 to 0.36899, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3690 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4746 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 77: val_loss improved from 0.36899 to 0.36897, saving model to model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3690 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1953 - accuracy: 0.9688 - precision: 0.9375 - recall: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8361 - precision: 0.8466 - recall: 0.8364 - val_loss: 0.3690 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5281 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 79: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3690 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2339 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 80: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8361 - precision: 0.8466 - recall: 0.8364 - val_loss: 0.3691 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3485 - accuracy: 0.8125 - precision: 1.0000 - recall: 0.7000\n",
            "Epoch 81: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3069 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 82: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2930 - accuracy: 0.9062 - precision: 0.8667 - recall: 0.9286\n",
            "Epoch 83: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3487 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.6667\n",
            "Epoch 84: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4513 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 85: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8361 - precision: 0.8466 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4766 - accuracy: 0.6875 - precision: 0.7857 - recall: 0.6111\n",
            "Epoch 86: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3486 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 87: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6194 - accuracy: 0.6875 - precision: 0.7333 - recall: 0.6471\n",
            "Epoch 88: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3690 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2992 - accuracy: 0.9375 - precision: 0.9000 - recall: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8361 - precision: 0.8466 - recall: 0.8364 - val_loss: 0.3691 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4756 - accuracy: 0.8125 - precision: 0.9474 - recall: 0.7826\n",
            "Epoch 90: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8361 - precision: 0.8452 - recall: 0.8384 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3256 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7647\n",
            "Epoch 91: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8372 - precision: 0.8469 - recall: 0.8384 - val_loss: 0.3692 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3153 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7895\n",
            "Epoch 92: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8330 - precision: 0.8415 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3012 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8636\n",
            "Epoch 93: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8361 - precision: 0.8452 - recall: 0.8384 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6289 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7059\n",
            "Epoch 94: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8340 - precision: 0.8418 - recall: 0.8384 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.9062 - precision: 0.9048 - recall: 0.9500\n",
            "Epoch 95: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3693 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2946 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 96: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8351 - precision: 0.8435 - recall: 0.8384 - val_loss: 0.3694 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3175 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 97: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8340 - precision: 0.8418 - recall: 0.8384 - val_loss: 0.3693 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 97: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 16x8x1\n",
        "- val_accuracy: 0.8908\n",
        "- val_loss: 0.3069\n",
        "- val_precision: 0.9015\n",
        "- val_recall: 0.9015"
      ],
      "metadata": {
        "id": "ksBCK1whbCFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "three_layer_model = Sequential(name=\"16x8x1_model\")\n",
        "three_layer_model.add(Input(shape=(X.shape[1],)))\n",
        "three_layer_model.add(Dense(16, activation='relu'))\n",
        "three_layer_model.add(Dense(8, activation='relu'))\n",
        "three_layer_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "three_layer_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "three_layer_history = three_layer_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ytn5UOFuauAk",
        "outputId": "37b994c6-577f-4b96-d8a6-f791e1b116d0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 17s - loss: 0.7054 - accuracy: 0.6250 - precision: 0.8289 - recall: 0.8400\n",
            "Epoch 1: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6251 - accuracy: 0.6933 - precision: 0.6965 - recall: 0.8426 - val_loss: 0.5706 - val_accuracy: 0.7563 - val_precision: 0.7794 - val_recall: 0.7910\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6167 - accuracy: 0.5938 - precision: 0.5294 - recall: 0.6429\n",
            "Epoch 2: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5484 - accuracy: 0.7647 - precision: 0.7571 - recall: 0.8061 - val_loss: 0.5113 - val_accuracy: 0.8109 - val_precision: 0.8450 - val_recall: 0.8134\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4792 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 3: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7941 - precision: 0.8008 - recall: 0.8040 - val_loss: 0.4682 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4346 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 4: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4601 - accuracy: 0.8057 - precision: 0.8088 - recall: 0.8202 - val_loss: 0.4383 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5674 - accuracy: 0.7188 - precision: 0.6500 - recall: 0.8667\n",
            "Epoch 5: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4370 - accuracy: 0.8130 - precision: 0.8241 - recall: 0.8141 - val_loss: 0.4167 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3929 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 6: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8183 - precision: 0.8246 - recall: 0.8263 - val_loss: 0.4050 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4937 - accuracy: 0.7500 - precision: 0.8462 - recall: 0.6471\n",
            "Epoch 7: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4135 - accuracy: 0.8246 - precision: 0.8320 - recall: 0.8303 - val_loss: 0.3973 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4191 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 8: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.8288 - precision: 0.8374 - recall: 0.8323 - val_loss: 0.3897 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3740 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 9: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8319 - precision: 0.8384 - recall: 0.8384 - val_loss: 0.3849 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3349 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 10: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.8330 - precision: 0.8443 - recall: 0.8323 - val_loss: 0.3798 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2813 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 11: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3894 - accuracy: 0.8340 - precision: 0.8432 - recall: 0.8364 - val_loss: 0.3745 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3916 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 12: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8393 - precision: 0.8434 - recall: 0.8485 - val_loss: 0.3721 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6480 - accuracy: 0.7188 - precision: 0.6429 - recall: 0.6923\n",
            "Epoch 13: val_loss did not improve from 0.36897\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8382 - precision: 0.8431 - recall: 0.8465 - val_loss: 0.3695 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4231 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 14: val_loss improved from 0.36897 to 0.36730, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3782 - accuracy: 0.8403 - precision: 0.8465 - recall: 0.8465 - val_loss: 0.3673 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4058 - accuracy: 0.7812 - precision: 0.8462 - recall: 0.8800\n",
            "Epoch 15: val_loss improved from 0.36730 to 0.36560, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8435 - precision: 0.8460 - recall: 0.8545 - val_loss: 0.3656 - val_accuracy: 0.8487 - val_precision: 0.9016 - val_recall: 0.8209\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3072 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 16: val_loss improved from 0.36560 to 0.36195, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.8424 - precision: 0.8499 - recall: 0.8465 - val_loss: 0.3620 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3855 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 17: val_loss improved from 0.36195 to 0.35996, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8477 - precision: 0.8514 - recall: 0.8566 - val_loss: 0.3600 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4489 - accuracy: 0.8125 - precision: 0.8947 - recall: 0.8095\n",
            "Epoch 18: val_loss improved from 0.35996 to 0.35865, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8498 - precision: 0.8534 - recall: 0.8586 - val_loss: 0.3587 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2972 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.8333\n",
            "Epoch 19: val_loss improved from 0.35865 to 0.35694, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3630 - accuracy: 0.8487 - precision: 0.8531 - recall: 0.8566 - val_loss: 0.3569 - val_accuracy: 0.8361 - val_precision: 0.8926 - val_recall: 0.8060\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3973 - accuracy: 0.8438 - precision: 0.8500 - recall: 0.8947\n",
            "Epoch 20: val_loss improved from 0.35694 to 0.35493, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8498 - precision: 0.8548 - recall: 0.8566 - val_loss: 0.3549 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2575 - accuracy: 0.9375 - precision: 0.9167 - recall: 1.0000\n",
            "Epoch 21: val_loss improved from 0.35493 to 0.35318, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3575 - accuracy: 0.8519 - precision: 0.8569 - recall: 0.8586 - val_loss: 0.3532 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5737 - accuracy: 0.7500 - precision: 0.8462 - recall: 0.6471\n",
            "Epoch 22: val_loss did not improve from 0.35318\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8529 - precision: 0.8543 - recall: 0.8646 - val_loss: 0.3532 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3601 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 23: val_loss improved from 0.35318 to 0.35246, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.8508 - precision: 0.8509 - recall: 0.8646 - val_loss: 0.3525 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3077 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 24: val_loss improved from 0.35246 to 0.35210, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3509 - accuracy: 0.8540 - precision: 0.8546 - recall: 0.8667 - val_loss: 0.3521 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 25: val_loss improved from 0.35210 to 0.34886, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3486 - accuracy: 0.8540 - precision: 0.8560 - recall: 0.8646 - val_loss: 0.3489 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3440 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 26: val_loss improved from 0.34886 to 0.34836, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.8550 - precision: 0.8535 - recall: 0.8707 - val_loss: 0.3484 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3616 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8095\n",
            "Epoch 27: val_loss improved from 0.34836 to 0.34715, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.8561 - precision: 0.8580 - recall: 0.8667 - val_loss: 0.3471 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3559 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 28: val_loss improved from 0.34715 to 0.34439, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.8592 - precision: 0.8617 - recall: 0.8687 - val_loss: 0.3444 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 29: val_loss improved from 0.34439 to 0.34407, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3407 - accuracy: 0.8550 - precision: 0.8549 - recall: 0.8687 - val_loss: 0.3441 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4693 - accuracy: 0.8438 - precision: 0.7647 - recall: 0.9286\n",
            "Epoch 30: val_loss improved from 0.34407 to 0.34336, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3388 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3434 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2168 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 31: val_loss improved from 0.34336 to 0.34327, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3433 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2708 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 32: val_loss improved from 0.34327 to 0.34174, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3355 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3417 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1987 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 33: val_loss did not improve from 0.34174\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3435 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2802 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 34: val_loss did not improve from 0.34174\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8603 - precision: 0.8635 - recall: 0.8687 - val_loss: 0.3439 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3181 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 35: val_loss did not improve from 0.34174\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8603 - precision: 0.8635 - recall: 0.8687 - val_loss: 0.3417 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2959 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 36: val_loss improved from 0.34174 to 0.34084, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8603 - precision: 0.8635 - recall: 0.8687 - val_loss: 0.3408 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6691 - accuracy: 0.6875 - precision: 0.7692 - recall: 0.5882\n",
            "Epoch 37: val_loss improved from 0.34084 to 0.33882, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.8613 - precision: 0.8652 - recall: 0.8687 - val_loss: 0.3388 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2637 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 38: val_loss did not improve from 0.33882\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8592 - precision: 0.8603 - recall: 0.8707 - val_loss: 0.3412 - val_accuracy: 0.8613 - val_precision: 0.9040 - val_recall: 0.8433\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2662 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 39: val_loss improved from 0.33882 to 0.33827, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3249 - accuracy: 0.8687 - precision: 0.8760 - recall: 0.8707 - val_loss: 0.3383 - val_accuracy: 0.8613 - val_precision: 0.9040 - val_recall: 0.8433\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2383 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 40: val_loss did not improve from 0.33827\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8697 - precision: 0.8747 - recall: 0.8747 - val_loss: 0.3386 - val_accuracy: 0.8613 - val_precision: 0.9040 - val_recall: 0.8433\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2106 - accuracy: 0.9062 - precision: 0.8667 - recall: 0.9286\n",
            "Epoch 41: val_loss improved from 0.33827 to 0.33532, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.8676 - precision: 0.8758 - recall: 0.8687 - val_loss: 0.3353 - val_accuracy: 0.8655 - val_precision: 0.9048 - val_recall: 0.8507\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3073 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 42: val_loss improved from 0.33532 to 0.33460, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8697 - precision: 0.8778 - recall: 0.8707 - val_loss: 0.3346 - val_accuracy: 0.8697 - val_precision: 0.9055 - val_recall: 0.8582\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1312 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9286\n",
            "Epoch 43: val_loss improved from 0.33460 to 0.33327, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8739 - precision: 0.8803 - recall: 0.8768 - val_loss: 0.3333 - val_accuracy: 0.8697 - val_precision: 0.9055 - val_recall: 0.8582\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3142 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 44: val_loss improved from 0.33327 to 0.33304, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3330 - val_accuracy: 0.8697 - val_precision: 0.9055 - val_recall: 0.8582\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3152 - accuracy: 0.8750 - precision: 0.8095 - recall: 1.0000\n",
            "Epoch 45: val_loss improved from 0.33304 to 0.33246, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8729 - precision: 0.8740 - recall: 0.8828 - val_loss: 0.3325 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3290 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 46: val_loss improved from 0.33246 to 0.33215, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3149 - accuracy: 0.8708 - precision: 0.8735 - recall: 0.8788 - val_loss: 0.3322 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3583 - accuracy: 0.8125 - precision: 0.7143 - recall: 1.0000\n",
            "Epoch 47: val_loss improved from 0.33215 to 0.33038, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.8739 - precision: 0.8788 - recall: 0.8788 - val_loss: 0.3304 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2706 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 48: val_loss improved from 0.33038 to 0.32965, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3127 - accuracy: 0.8771 - precision: 0.8826 - recall: 0.8808 - val_loss: 0.3296 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2218 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 49: val_loss improved from 0.32965 to 0.32926, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3113 - accuracy: 0.8739 - precision: 0.8773 - recall: 0.8808 - val_loss: 0.3293 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3366 - accuracy: 0.9062 - precision: 0.8125 - recall: 1.0000\n",
            "Epoch 50: val_loss improved from 0.32926 to 0.32797, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.8813 - precision: 0.8820 - recall: 0.8909 - val_loss: 0.3280 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3800 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 51: val_loss improved from 0.32797 to 0.32724, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3089 - accuracy: 0.8834 - precision: 0.8840 - recall: 0.8929 - val_loss: 0.3272 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3198 - accuracy: 0.8438 - precision: 0.9048 - recall: 0.8636\n",
            "Epoch 52: val_loss did not improve from 0.32724\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3079 - accuracy: 0.8866 - precision: 0.8832 - recall: 0.9010 - val_loss: 0.3279 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3037 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 53: val_loss improved from 0.32724 to 0.32563, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3067 - accuracy: 0.8834 - precision: 0.8887 - recall: 0.8869 - val_loss: 0.3256 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4188 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 54: val_loss did not improve from 0.32563\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3057 - accuracy: 0.8824 - precision: 0.8792 - recall: 0.8970 - val_loss: 0.3266 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3599 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 55: val_loss did not improve from 0.32563\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8834 - precision: 0.8825 - recall: 0.8949 - val_loss: 0.3290 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2543 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 56: val_loss improved from 0.32563 to 0.32487, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3031 - accuracy: 0.8845 - precision: 0.8889 - recall: 0.8889 - val_loss: 0.3249 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2390 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.8182\n",
            "Epoch 57: val_loss improved from 0.32487 to 0.32350, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3025 - accuracy: 0.8824 - precision: 0.8838 - recall: 0.8909 - val_loss: 0.3235 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1421 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 58: val_loss improved from 0.32350 to 0.32297, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3015 - accuracy: 0.8845 - precision: 0.8812 - recall: 0.8990 - val_loss: 0.3230 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1473 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 59: val_loss improved from 0.32297 to 0.32291, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3000 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3229 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3661 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 60: val_loss did not improve from 0.32291\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8845 - precision: 0.8842 - recall: 0.8949 - val_loss: 0.3230 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1927 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 61: val_loss improved from 0.32291 to 0.32131, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2985 - accuracy: 0.8855 - precision: 0.8860 - recall: 0.8949 - val_loss: 0.3213 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3485 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 62: val_loss improved from 0.32131 to 0.32069, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2971 - accuracy: 0.8876 - precision: 0.8849 - recall: 0.9010 - val_loss: 0.3207 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2714 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 63: val_loss improved from 0.32069 to 0.32046, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2966 - accuracy: 0.8876 - precision: 0.8880 - recall: 0.8970 - val_loss: 0.3205 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2255 - accuracy: 0.9062 - precision: 0.9524 - recall: 0.9091\n",
            "Epoch 64: val_loss improved from 0.32046 to 0.32029, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2953 - accuracy: 0.8876 - precision: 0.8834 - recall: 0.9030 - val_loss: 0.3203 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2162 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8462\n",
            "Epoch 65: val_loss did not improve from 0.32029\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8855 - precision: 0.8860 - recall: 0.8949 - val_loss: 0.3212 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1485 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 66: val_loss improved from 0.32029 to 0.31970, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2935 - accuracy: 0.8887 - precision: 0.8882 - recall: 0.8990 - val_loss: 0.3197 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3684 - accuracy: 0.7812 - precision: 0.9375 - recall: 0.7143\n",
            "Epoch 67: val_loss improved from 0.31970 to 0.31762, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2924 - accuracy: 0.8887 - precision: 0.8882 - recall: 0.8990 - val_loss: 0.3176 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2009 - accuracy: 0.9062 - precision: 0.8571 - recall: 1.0000\n",
            "Epoch 68: val_loss improved from 0.31762 to 0.31674, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2919 - accuracy: 0.8876 - precision: 0.8849 - recall: 0.9010 - val_loss: 0.3167 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2465 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 69: val_loss improved from 0.31674 to 0.31647, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2909 - accuracy: 0.8897 - precision: 0.8869 - recall: 0.9030 - val_loss: 0.3165 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2101 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 70: val_loss improved from 0.31647 to 0.31527, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2898 - accuracy: 0.8866 - precision: 0.8847 - recall: 0.8990 - val_loss: 0.3153 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2272 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 71: val_loss improved from 0.31527 to 0.31301, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2892 - accuracy: 0.8876 - precision: 0.8834 - recall: 0.9030 - val_loss: 0.3130 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3439 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 72: val_loss did not improve from 0.31301\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.8855 - precision: 0.8860 - recall: 0.8949 - val_loss: 0.3136 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3031 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 73: val_loss did not improve from 0.31301\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8866 - precision: 0.8832 - recall: 0.9010 - val_loss: 0.3140 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1797 - accuracy: 0.9062 - precision: 0.8333 - recall: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.31301\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8855 - precision: 0.8829 - recall: 0.8990 - val_loss: 0.3136 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1642 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 75: val_loss improved from 0.31301 to 0.30956, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2852 - accuracy: 0.8855 - precision: 0.8845 - recall: 0.8970 - val_loss: 0.3096 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4283 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 76: val_loss did not improve from 0.30956\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8887 - precision: 0.8836 - recall: 0.9051 - val_loss: 0.3111 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4346 - accuracy: 0.7500 - precision: 0.8000 - recall: 0.7059\n",
            "Epoch 77: val_loss did not improve from 0.30956\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8897 - precision: 0.8809 - recall: 0.9111 - val_loss: 0.3122 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2166 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 78: val_loss improved from 0.30956 to 0.30922, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.8939 - precision: 0.8940 - recall: 0.9030 - val_loss: 0.3092 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1667 - accuracy: 0.9688 - precision: 0.9333 - recall: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.30922\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8887 - precision: 0.8836 - recall: 0.9051 - val_loss: 0.3104 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1552 - accuracy: 0.9688 - precision: 0.9167 - recall: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.30922\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8929 - precision: 0.8861 - recall: 0.9111 - val_loss: 0.3102 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2639 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 81: val_loss improved from 0.30922 to 0.30870, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2804 - accuracy: 0.8918 - precision: 0.8843 - recall: 0.9111 - val_loss: 0.3087 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3655 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 82: val_loss improved from 0.30870 to 0.30723, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2797 - accuracy: 0.8929 - precision: 0.8845 - recall: 0.9131 - val_loss: 0.3072 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3231 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 83: val_loss improved from 0.30723 to 0.30413, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2793 - accuracy: 0.8950 - precision: 0.8911 - recall: 0.9091 - val_loss: 0.3041 - val_accuracy: 0.8613 - val_precision: 0.8797 - val_recall: 0.8731\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3284 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.6875\n",
            "Epoch 84: val_loss improved from 0.30413 to 0.30392, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2782 - accuracy: 0.8908 - precision: 0.8871 - recall: 0.9051 - val_loss: 0.3039 - val_accuracy: 0.8613 - val_precision: 0.8797 - val_recall: 0.8731\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2454 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 85: val_loss did not improve from 0.30392\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8887 - precision: 0.8851 - recall: 0.9030 - val_loss: 0.3041 - val_accuracy: 0.8613 - val_precision: 0.8797 - val_recall: 0.8731\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4234 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 86: val_loss did not improve from 0.30392\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8908 - precision: 0.8826 - recall: 0.9111 - val_loss: 0.3062 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1733 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9375\n",
            "Epoch 87: val_loss improved from 0.30392 to 0.30358, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.8887 - precision: 0.8836 - recall: 0.9051 - val_loss: 0.3036 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1788 - accuracy: 0.9375 - precision: 0.8667 - recall: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.30358\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.8939 - precision: 0.8893 - recall: 0.9091 - val_loss: 0.3036 - val_accuracy: 0.8529 - val_precision: 0.8722 - val_recall: 0.8657\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2063 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 89: val_loss improved from 0.30358 to 0.30308, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2740 - accuracy: 0.8960 - precision: 0.8929 - recall: 0.9091 - val_loss: 0.3031 - val_accuracy: 0.8529 - val_precision: 0.8722 - val_recall: 0.8657\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3354 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 90: val_loss did not improve from 0.30308\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8939 - precision: 0.8893 - recall: 0.9091 - val_loss: 0.3039 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1054 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 91: val_loss improved from 0.30308 to 0.30237, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2732 - accuracy: 0.8939 - precision: 0.8863 - recall: 0.9131 - val_loss: 0.3024 - val_accuracy: 0.8529 - val_precision: 0.8722 - val_recall: 0.8657\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2960 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 92: val_loss did not improve from 0.30237\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8939 - precision: 0.8878 - recall: 0.9111 - val_loss: 0.3037 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9000\n",
            "Epoch 93: val_loss did not improve from 0.30237\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.9013 - precision: 0.8986 - recall: 0.9131 - val_loss: 0.3025 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2973 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 94: val_loss did not improve from 0.30237\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8950 - precision: 0.8911 - recall: 0.9091 - val_loss: 0.3041 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2395 - accuracy: 0.8750 - precision: 0.9375 - recall: 0.8333\n",
            "Epoch 95: val_loss improved from 0.30237 to 0.30151, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2700 - accuracy: 0.8992 - precision: 0.9014 - recall: 0.9051 - val_loss: 0.3015 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1767 - accuracy: 0.9375 - precision: 0.9167 - recall: 0.9167\n",
            "Epoch 96: val_loss did not improve from 0.30151\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.8971 - precision: 0.8900 - recall: 0.9152 - val_loss: 0.3022 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1459 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 97: val_loss did not improve from 0.30151\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2687 - accuracy: 0.8971 - precision: 0.8946 - recall: 0.9091 - val_loss: 0.3041 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 98/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2916 - accuracy: 0.9062 - precision: 0.8462 - recall: 0.9167\n",
            "Epoch 98: val_loss improved from 0.30151 to 0.30038, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2683 - accuracy: 0.9013 - precision: 0.9018 - recall: 0.9091 - val_loss: 0.3004 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 99/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1817 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 99: val_loss did not improve from 0.30038\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.9002 - precision: 0.8953 - recall: 0.9152 - val_loss: 0.3044 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 100/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2377 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 100: val_loss improved from 0.30038 to 0.29674, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2669 - accuracy: 0.9013 - precision: 0.9051 - recall: 0.9051 - val_loss: 0.2967 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 101/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2319 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 101: val_loss did not improve from 0.29674\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.8981 - precision: 0.8948 - recall: 0.9111 - val_loss: 0.2982 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 102/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1450 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 102: val_loss did not improve from 0.29674\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8992 - precision: 0.8950 - recall: 0.9131 - val_loss: 0.3016 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 103/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3196 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8235\n",
            "Epoch 103: val_loss did not improve from 0.29674\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.9044 - precision: 0.9089 - recall: 0.9071 - val_loss: 0.2983 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 104/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2166 - accuracy: 0.9062 - precision: 0.9500 - recall: 0.9048\n",
            "Epoch 104: val_loss improved from 0.29674 to 0.29658, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2647 - accuracy: 0.8992 - precision: 0.8998 - recall: 0.9071 - val_loss: 0.2966 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 105/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2406 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 105: val_loss improved from 0.29658 to 0.29558, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2643 - accuracy: 0.9002 - precision: 0.9016 - recall: 0.9071 - val_loss: 0.2956 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 106/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3657 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 106: val_loss did not improve from 0.29558\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.9044 - precision: 0.9056 - recall: 0.9111 - val_loss: 0.2961 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 107/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2689 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 107: val_loss improved from 0.29558 to 0.29537, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2629 - accuracy: 0.9034 - precision: 0.9054 - recall: 0.9091 - val_loss: 0.2954 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 108/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2525 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 108: val_loss did not improve from 0.29537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.9002 - precision: 0.9000 - recall: 0.9091 - val_loss: 0.2957 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 109/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3392 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 109: val_loss improved from 0.29537 to 0.29370, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.9055 - precision: 0.9091 - recall: 0.9091 - val_loss: 0.2937 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 110/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4242 - accuracy: 0.8750 - precision: 0.7895 - recall: 1.0000\n",
            "Epoch 110: val_loss improved from 0.29370 to 0.29361, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.9002 - precision: 0.8984 - recall: 0.9111 - val_loss: 0.2936 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 111/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3852 - accuracy: 0.8750 - precision: 0.7895 - recall: 1.0000\n",
            "Epoch 111: val_loss did not improve from 0.29361\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.9055 - precision: 0.9074 - recall: 0.9111 - val_loss: 0.2947 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 112/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2471 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8889\n",
            "Epoch 112: val_loss did not improve from 0.29361\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2600 - accuracy: 0.9055 - precision: 0.9058 - recall: 0.9131 - val_loss: 0.2938 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 113/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3202 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 113: val_loss did not improve from 0.29361\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.9034 - precision: 0.9087 - recall: 0.9051 - val_loss: 0.2943 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 114/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2197 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 114: val_loss did not improve from 0.29361\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8981 - precision: 0.8933 - recall: 0.9131 - val_loss: 0.2950 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 115/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1948 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 115: val_loss improved from 0.29361 to 0.29175, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2585 - accuracy: 0.9086 - precision: 0.9163 - recall: 0.9071 - val_loss: 0.2917 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 116/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2003 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 116: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2579 - accuracy: 0.9065 - precision: 0.9076 - recall: 0.9131 - val_loss: 0.2924 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 117/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9688 - precision: 0.9333 - recall: 1.0000\n",
            "Epoch 117: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.9034 - precision: 0.9071 - recall: 0.9071 - val_loss: 0.2940 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 118/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2918 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 118: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.9097 - precision: 0.9131 - recall: 0.9131 - val_loss: 0.2958 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 119/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3326 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 119: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9065 - precision: 0.9093 - recall: 0.9111 - val_loss: 0.2945 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 120/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3388 - accuracy: 0.8438 - precision: 0.9048 - recall: 0.8636\n",
            "Epoch 120: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.9076 - precision: 0.9128 - recall: 0.9091 - val_loss: 0.2940 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 121/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1045 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 121: val_loss did not improve from 0.29175\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.9076 - precision: 0.9111 - recall: 0.9111 - val_loss: 0.2949 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 122/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2725 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 122: val_loss improved from 0.29175 to 0.28963, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2545 - accuracy: 0.9065 - precision: 0.9109 - recall: 0.9091 - val_loss: 0.2896 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 123/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1950 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 123: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.9097 - precision: 0.9082 - recall: 0.9192 - val_loss: 0.2911 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 124/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3863 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 124: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9055 - precision: 0.9074 - recall: 0.9111 - val_loss: 0.2923 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 125/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2205 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 125: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2535 - accuracy: 0.9086 - precision: 0.9130 - recall: 0.9111 - val_loss: 0.2953 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 126/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3779 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 126: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.9055 - precision: 0.9074 - recall: 0.9111 - val_loss: 0.2907 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 127/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3101 - accuracy: 0.9062 - precision: 0.8125 - recall: 1.0000\n",
            "Epoch 127: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.9086 - precision: 0.9130 - recall: 0.9111 - val_loss: 0.2898 - val_accuracy: 0.8655 - val_precision: 0.8984 - val_recall: 0.8582\n",
            "Epoch 128/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1694 - accuracy: 0.9375 - precision: 0.8824 - recall: 1.0000\n",
            "Epoch 128: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.9065 - precision: 0.9060 - recall: 0.9152 - val_loss: 0.2923 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 129/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1872 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 129: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.9097 - precision: 0.9131 - recall: 0.9131 - val_loss: 0.2896 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 130/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1894 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9167\n",
            "Epoch 130: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.9076 - precision: 0.9128 - recall: 0.9091 - val_loss: 0.2899 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 131/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2666 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 131: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.9065 - precision: 0.9093 - recall: 0.9111 - val_loss: 0.2925 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 132/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3113 - accuracy: 0.8438 - precision: 0.7143 - recall: 0.9091\n",
            "Epoch 132: val_loss did not improve from 0.28963\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.9118 - precision: 0.9152 - recall: 0.9152 - val_loss: 0.2913 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 133/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3296 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 133: val_loss improved from 0.28963 to 0.28928, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2488 - accuracy: 0.9076 - precision: 0.9095 - recall: 0.9131 - val_loss: 0.2893 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 134/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2563 - accuracy: 0.9375 - precision: 0.9000 - recall: 1.0000\n",
            "Epoch 134: val_loss did not improve from 0.28928\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9065 - precision: 0.9076 - recall: 0.9131 - val_loss: 0.2926 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 135/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2510 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 135: val_loss did not improve from 0.28928\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9097 - precision: 0.9115 - recall: 0.9152 - val_loss: 0.2919 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 136/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2412 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 136: val_loss did not improve from 0.28928\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9065 - precision: 0.9093 - recall: 0.9111 - val_loss: 0.2920 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 137/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2688 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 137: val_loss did not improve from 0.28928\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.9076 - precision: 0.9095 - recall: 0.9131 - val_loss: 0.2896 - val_accuracy: 0.8529 - val_precision: 0.8837 - val_recall: 0.8507\n",
            "Epoch 138/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1314 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9412\n",
            "Epoch 138: val_loss improved from 0.28928 to 0.28685, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2468 - accuracy: 0.9107 - precision: 0.9133 - recall: 0.9152 - val_loss: 0.2869 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 139/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2086 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 139: val_loss improved from 0.28685 to 0.28537, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2464 - accuracy: 0.9097 - precision: 0.9098 - recall: 0.9172 - val_loss: 0.2854 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 140/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3083 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 140: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9086 - precision: 0.9096 - recall: 0.9152 - val_loss: 0.2876 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 141/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1434 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 141: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2457 - accuracy: 0.9107 - precision: 0.9116 - recall: 0.9172 - val_loss: 0.2868 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 142/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1248 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9375\n",
            "Epoch 142: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.9076 - precision: 0.9078 - recall: 0.9152 - val_loss: 0.2878 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 143/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2303 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 143: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.9086 - precision: 0.9113 - recall: 0.9131 - val_loss: 0.2873 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 144/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1790 - accuracy: 0.9688 - precision: 0.9286 - recall: 1.0000\n",
            "Epoch 144: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9097 - precision: 0.9131 - recall: 0.9131 - val_loss: 0.2867 - val_accuracy: 0.8697 - val_precision: 0.8872 - val_recall: 0.8806\n",
            "Epoch 145/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2044 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 145: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.9086 - precision: 0.9096 - recall: 0.9152 - val_loss: 0.2881 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 146/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2447 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 146: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9076 - precision: 0.9095 - recall: 0.9131 - val_loss: 0.2865 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 147/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2604 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 147: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9107 - precision: 0.9133 - recall: 0.9152 - val_loss: 0.2884 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 148/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1177 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 148: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.9086 - precision: 0.9096 - recall: 0.9152 - val_loss: 0.2884 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 149/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1434 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 149: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9107 - precision: 0.9150 - recall: 0.9131 - val_loss: 0.2868 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 150/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2768 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 150: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9097 - precision: 0.9115 - recall: 0.9152 - val_loss: 0.2869 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 151/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2317 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 151: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.9107 - precision: 0.9133 - recall: 0.9152 - val_loss: 0.2883 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 152/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3890 - accuracy: 0.8125 - precision: 0.9286 - recall: 0.7222\n",
            "Epoch 152: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.9107 - precision: 0.9133 - recall: 0.9152 - val_loss: 0.2869 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 153/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1907 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 153: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.9118 - precision: 0.9135 - recall: 0.9172 - val_loss: 0.2887 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 154/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.0910 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 154: val_loss did not improve from 0.28537\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9097 - precision: 0.9131 - recall: 0.9131 - val_loss: 0.2862 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 155/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2952 - accuracy: 0.9375 - precision: 0.8462 - recall: 1.0000\n",
            "Epoch 155: val_loss improved from 0.28537 to 0.28456, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2389 - accuracy: 0.9128 - precision: 0.9153 - recall: 0.9172 - val_loss: 0.2846 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 156/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1867 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9231\n",
            "Epoch 156: val_loss did not improve from 0.28456\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9097 - precision: 0.9115 - recall: 0.9152 - val_loss: 0.2859 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 157/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2115 - accuracy: 0.9375 - precision: 0.9167 - recall: 1.0000\n",
            "Epoch 157: val_loss improved from 0.28456 to 0.28439, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 0.9107 - precision: 0.9150 - recall: 0.9131 - val_loss: 0.2844 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 158/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2718 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 158: val_loss did not improve from 0.28439\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9118 - precision: 0.9135 - recall: 0.9172 - val_loss: 0.2853 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 159/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2487 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 159: val_loss improved from 0.28439 to 0.28401, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2367 - accuracy: 0.9118 - precision: 0.9152 - recall: 0.9152 - val_loss: 0.2840 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 160/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2753 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 160: val_loss did not improve from 0.28401\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.9139 - precision: 0.9155 - recall: 0.9192 - val_loss: 0.2883 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 161/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1866 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 161: val_loss did not improve from 0.28401\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.9118 - precision: 0.9185 - recall: 0.9111 - val_loss: 0.2842 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 162/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1728 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9500\n",
            "Epoch 162: val_loss did not improve from 0.28401\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9107 - precision: 0.9133 - recall: 0.9152 - val_loss: 0.2852 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 163/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2053 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 163: val_loss improved from 0.28401 to 0.28373, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9139 - precision: 0.9172 - recall: 0.9172 - val_loss: 0.2837 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 164/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1829 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 164: val_loss did not improve from 0.28373\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 0.9149 - precision: 0.9173 - recall: 0.9192 - val_loss: 0.2848 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 165/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3360 - accuracy: 0.8125 - precision: 0.7692 - recall: 0.7692\n",
            "Epoch 165: val_loss improved from 0.28373 to 0.28255, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2336 - accuracy: 0.9118 - precision: 0.9152 - recall: 0.9152 - val_loss: 0.2825 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 166/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1924 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 166: val_loss improved from 0.28255 to 0.28055, saving model to model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2329 - accuracy: 0.9139 - precision: 0.9172 - recall: 0.9172 - val_loss: 0.2806 - val_accuracy: 0.8613 - val_precision: 0.8797 - val_recall: 0.8731\n",
            "Epoch 167/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1506 - accuracy: 0.9688 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 167: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.9128 - precision: 0.9137 - recall: 0.9192 - val_loss: 0.2846 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 168/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1915 - accuracy: 0.9062 - precision: 0.8636 - recall: 1.0000\n",
            "Epoch 168: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.9128 - precision: 0.9170 - recall: 0.9152 - val_loss: 0.2860 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 169/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2005 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 169: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9170 - precision: 0.9194 - recall: 0.9212 - val_loss: 0.2857 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 170/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1353 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8889\n",
            "Epoch 170: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.9181 - precision: 0.9195 - recall: 0.9232 - val_loss: 0.2876 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 171/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1188 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9286\n",
            "Epoch 171: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9128 - precision: 0.9153 - recall: 0.9172 - val_loss: 0.2853 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 172/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2366 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 172: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.9160 - precision: 0.9192 - recall: 0.9192 - val_loss: 0.2840 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 173/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3027 - accuracy: 0.8750 - precision: 0.9231 - recall: 0.8000\n",
            "Epoch 173: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.9181 - precision: 0.9195 - recall: 0.9232 - val_loss: 0.2873 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 174/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2695 - accuracy: 0.9062 - precision: 0.8667 - recall: 0.9286\n",
            "Epoch 174: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9191 - precision: 0.9231 - recall: 0.9212 - val_loss: 0.2857 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 175/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1773 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8667\n",
            "Epoch 175: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.9149 - precision: 0.9190 - recall: 0.9172 - val_loss: 0.2847 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 176/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3942 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 176: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.9139 - precision: 0.9189 - recall: 0.9152 - val_loss: 0.2848 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 177/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3445 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 177: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.9191 - precision: 0.9214 - recall: 0.9232 - val_loss: 0.2862 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 178/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2404 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 178: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.9160 - precision: 0.9192 - recall: 0.9192 - val_loss: 0.2847 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 179/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.0835 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 179: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9181 - precision: 0.9212 - recall: 0.9212 - val_loss: 0.2829 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 180/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 180: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9202 - precision: 0.9215 - recall: 0.9253 - val_loss: 0.2853 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 181/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 181: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9181 - precision: 0.9229 - recall: 0.9192 - val_loss: 0.2827 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 182/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1151 - accuracy: 0.9688 - precision: 0.9167 - recall: 1.0000\n",
            "Epoch 182: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9202 - precision: 0.9232 - recall: 0.9232 - val_loss: 0.2832 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 183/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2145 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 183: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9191 - precision: 0.9231 - recall: 0.9212 - val_loss: 0.2852 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 184/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2621 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 184: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9191 - precision: 0.9231 - recall: 0.9212 - val_loss: 0.2842 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 185/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1715 - accuracy: 0.9688 - precision: 0.9333 - recall: 1.0000\n",
            "Epoch 185: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9202 - precision: 0.9232 - recall: 0.9232 - val_loss: 0.2808 - val_accuracy: 0.8697 - val_precision: 0.8872 - val_recall: 0.8806\n",
            "Epoch 186/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2584 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 186: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.9191 - precision: 0.9214 - recall: 0.9232 - val_loss: 0.2844 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 186: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8x8x8x1"
      ],
      "metadata": {
        "id": "7EONJ6CEDEcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_eights_model = Sequential(name=\"all_eights_model\")\n",
        "all_eights_model.add(Input(shape=(X.shape[1],)))\n",
        "all_eights_model.add(Dense(8, activation='relu'))\n",
        "all_eights_model.add(Dense(8, activation='relu'))\n",
        "all_eights_model.add(Dense(8, activation='relu'))\n",
        "all_eights_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "all_eights_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "all_eights_history = all_eights_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UL5A-6GfDlM4",
        "outputId": "030a2b20-c508-42fd-9f00-cb6d56893691"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 19s - loss: 0.6820 - accuracy: 0.5938 - precision: 0.8355 - recall: 0.8639\n",
            "Epoch 1: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 1s 11ms/step - loss: 0.6228 - accuracy: 0.6691 - precision: 0.6836 - recall: 0.8347 - val_loss: 0.5822 - val_accuracy: 0.7437 - val_precision: 0.7325 - val_recall: 0.8582\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6490 - accuracy: 0.6875 - precision: 0.6316 - recall: 0.8000\n",
            "Epoch 2: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5754 - accuracy: 0.7311 - precision: 0.6995 - recall: 0.8465 - val_loss: 0.5403 - val_accuracy: 0.7647 - val_precision: 0.7786 - val_recall: 0.8134\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5492 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.8750\n",
            "Epoch 3: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.7521 - precision: 0.7495 - recall: 0.7859 - val_loss: 0.5059 - val_accuracy: 0.7773 - val_precision: 0.8092 - val_recall: 0.7910\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5278 - accuracy: 0.7500 - precision: 0.8000 - recall: 0.7059\n",
            "Epoch 4: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5119 - accuracy: 0.7710 - precision: 0.7832 - recall: 0.7737 - val_loss: 0.4805 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3731 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.7368\n",
            "Epoch 5: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7962 - precision: 0.8195 - recall: 0.7798 - val_loss: 0.4597 - val_accuracy: 0.7941 - val_precision: 0.8400 - val_recall: 0.7836\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5058 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 6: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4676 - accuracy: 0.8046 - precision: 0.8337 - recall: 0.7798 - val_loss: 0.4462 - val_accuracy: 0.7899 - val_precision: 0.8443 - val_recall: 0.7687\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5636 - accuracy: 0.7188 - precision: 0.8000 - recall: 0.6667\n",
            "Epoch 7: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4525 - accuracy: 0.8099 - precision: 0.8443 - recall: 0.7778 - val_loss: 0.4354 - val_accuracy: 0.7857 - val_precision: 0.8430 - val_recall: 0.7612\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4614 - accuracy: 0.7812 - precision: 0.7895 - recall: 0.8333\n",
            "Epoch 8: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8130 - precision: 0.8453 - recall: 0.7838 - val_loss: 0.4278 - val_accuracy: 0.7857 - val_precision: 0.8430 - val_recall: 0.7612\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5300 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 9: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8130 - precision: 0.8499 - recall: 0.7778 - val_loss: 0.4207 - val_accuracy: 0.7899 - val_precision: 0.8500 - val_recall: 0.7612\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4067 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 10: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8162 - precision: 0.8524 - recall: 0.7818 - val_loss: 0.4160 - val_accuracy: 0.7941 - val_precision: 0.8512 - val_recall: 0.7687\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4201 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.7143\n",
            "Epoch 11: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4215 - accuracy: 0.8193 - precision: 0.8534 - recall: 0.7879 - val_loss: 0.4113 - val_accuracy: 0.7941 - val_precision: 0.8512 - val_recall: 0.7687\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4099 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 12: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8193 - precision: 0.8519 - recall: 0.7899 - val_loss: 0.4075 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4083 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 13: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4136 - accuracy: 0.8204 - precision: 0.8553 - recall: 0.7879 - val_loss: 0.4045 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3365 - accuracy: 0.8750 - precision: 0.7778 - recall: 1.0000\n",
            "Epoch 14: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8246 - precision: 0.8581 - recall: 0.7939 - val_loss: 0.4006 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6258 - accuracy: 0.7500 - precision: 0.8235 - recall: 0.7368\n",
            "Epoch 15: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8256 - precision: 0.8600 - recall: 0.7939 - val_loss: 0.3976 - val_accuracy: 0.8151 - val_precision: 0.8750 - val_recall: 0.7836\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2604 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 16: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8235 - precision: 0.8562 - recall: 0.7939 - val_loss: 0.3939 - val_accuracy: 0.8235 - val_precision: 0.8770 - val_recall: 0.7985\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2543 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8235\n",
            "Epoch 17: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8267 - precision: 0.8571 - recall: 0.8000 - val_loss: 0.3918 - val_accuracy: 0.8235 - val_precision: 0.8770 - val_recall: 0.7985\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5225 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 18: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8298 - precision: 0.8581 - recall: 0.8061 - val_loss: 0.3884 - val_accuracy: 0.8235 - val_precision: 0.8770 - val_recall: 0.7985\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5433 - accuracy: 0.7188 - precision: 0.6923 - recall: 0.6429\n",
            "Epoch 19: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8277 - precision: 0.8559 - recall: 0.8040 - val_loss: 0.3865 - val_accuracy: 0.8193 - val_precision: 0.8699 - val_recall: 0.7985\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3582 - accuracy: 0.8438 - precision: 0.9231 - recall: 0.7500\n",
            "Epoch 20: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8267 - precision: 0.8526 - recall: 0.8061 - val_loss: 0.3854 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4207 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.7143\n",
            "Epoch 21: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8288 - precision: 0.8547 - recall: 0.8081 - val_loss: 0.3844 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3837 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 22: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8288 - precision: 0.8532 - recall: 0.8101 - val_loss: 0.3821 - val_accuracy: 0.8193 - val_precision: 0.8640 - val_recall: 0.8060\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5159 - accuracy: 0.8125 - precision: 0.9048 - recall: 0.8261\n",
            "Epoch 23: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8319 - precision: 0.8556 - recall: 0.8141 - val_loss: 0.3806 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3648 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 24: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8309 - precision: 0.8508 - recall: 0.8182 - val_loss: 0.3794 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2872 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 25: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8330 - precision: 0.8515 - recall: 0.8222 - val_loss: 0.3794 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4349 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 26: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8330 - precision: 0.8500 - recall: 0.8242 - val_loss: 0.3783 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3356 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 27: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8309 - precision: 0.8494 - recall: 0.8202 - val_loss: 0.3767 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2562 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.8462\n",
            "Epoch 28: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8372 - precision: 0.8512 - recall: 0.8323 - val_loss: 0.3762 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6589 - accuracy: 0.6562 - precision: 0.7000 - recall: 0.7368\n",
            "Epoch 29: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8382 - precision: 0.8515 - recall: 0.8343 - val_loss: 0.3753 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3258 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 30: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3737 - accuracy: 0.8382 - precision: 0.8501 - recall: 0.8364 - val_loss: 0.3734 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3640 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7647\n",
            "Epoch 31: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8424 - precision: 0.8528 - recall: 0.8424 - val_loss: 0.3740 - val_accuracy: 0.8319 - val_precision: 0.8730 - val_recall: 0.8209\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2532 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 32: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8393 - precision: 0.8533 - recall: 0.8343 - val_loss: 0.3730 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2015 - accuracy: 0.9375 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 33: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8351 - precision: 0.8449 - recall: 0.8364 - val_loss: 0.3732 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1807 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8393 - precision: 0.8504 - recall: 0.8384 - val_loss: 0.3718 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2421 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8414 - precision: 0.8496 - recall: 0.8444 - val_loss: 0.3710 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2001 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9333\n",
            "Epoch 36: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8382 - precision: 0.8487 - recall: 0.8384 - val_loss: 0.3706 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4339 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.8421\n",
            "Epoch 37: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3631 - accuracy: 0.8424 - precision: 0.8499 - recall: 0.8465 - val_loss: 0.3694 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4882 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.8333\n",
            "Epoch 38: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3617 - accuracy: 0.8414 - precision: 0.8496 - recall: 0.8444 - val_loss: 0.3686 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3718 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 39: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8445 - precision: 0.8534 - recall: 0.8465 - val_loss: 0.3668 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5142 - accuracy: 0.7500 - precision: 0.6842 - recall: 0.8667\n",
            "Epoch 40: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8456 - precision: 0.8537 - recall: 0.8485 - val_loss: 0.3653 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4321 - accuracy: 0.8125 - precision: 0.6667 - recall: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8445 - precision: 0.8519 - recall: 0.8485 - val_loss: 0.3655 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3059 - accuracy: 0.8750 - precision: 0.9231 - recall: 0.8000\n",
            "Epoch 42: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8466 - precision: 0.8540 - recall: 0.8505 - val_loss: 0.3637 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2196 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 43: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8456 - precision: 0.8551 - recall: 0.8465 - val_loss: 0.3626 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3236 - accuracy: 0.8438 - precision: 0.9444 - recall: 0.8095\n",
            "Epoch 44: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8477 - precision: 0.8571 - recall: 0.8485 - val_loss: 0.3608 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5596 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.7692\n",
            "Epoch 45: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8466 - precision: 0.8511 - recall: 0.8545 - val_loss: 0.3620 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3175 - accuracy: 0.9375 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 46: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8487 - precision: 0.8574 - recall: 0.8505 - val_loss: 0.3602 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3049 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 47: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.8508 - precision: 0.8580 - recall: 0.8545 - val_loss: 0.3594 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2572 - accuracy: 0.8750 - precision: 0.9167 - recall: 0.7857\n",
            "Epoch 48: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8529 - precision: 0.8600 - recall: 0.8566 - val_loss: 0.3592 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3310 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 49: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8529 - precision: 0.8586 - recall: 0.8586 - val_loss: 0.3578 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4335 - accuracy: 0.8125 - precision: 0.7059 - recall: 0.9231\n",
            "Epoch 50: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8550 - precision: 0.8606 - recall: 0.8606 - val_loss: 0.3567 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3518 - accuracy: 0.8750 - precision: 0.9333 - recall: 0.8235\n",
            "Epoch 51: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8561 - precision: 0.8609 - recall: 0.8626 - val_loss: 0.3566 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4962 - accuracy: 0.8125 - precision: 0.7368 - recall: 0.9333\n",
            "Epoch 52: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8550 - precision: 0.8592 - recall: 0.8626 - val_loss: 0.3552 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4360 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 53: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8561 - precision: 0.8623 - recall: 0.8606 - val_loss: 0.3538 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2759 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.8182\n",
            "Epoch 54: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3536 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4107 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 55: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8613 - precision: 0.8623 - recall: 0.8727 - val_loss: 0.3537 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2470 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 56: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8613 - precision: 0.8608 - recall: 0.8747 - val_loss: 0.3533 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3051 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 57: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8645 - precision: 0.8690 - recall: 0.8707 - val_loss: 0.3510 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2899 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 58: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8645 - precision: 0.8645 - recall: 0.8768 - val_loss: 0.3503 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2788 - accuracy: 0.9062 - precision: 0.8235 - recall: 1.0000\n",
            "Epoch 59: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8645 - precision: 0.8631 - recall: 0.8788 - val_loss: 0.3493 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2697 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 60: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8634 - precision: 0.8614 - recall: 0.8788 - val_loss: 0.3489 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3657 - accuracy: 0.8438 - precision: 0.7368 - recall: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8666 - precision: 0.8651 - recall: 0.8808 - val_loss: 0.3477 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4323 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 62: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8697 - precision: 0.8688 - recall: 0.8828 - val_loss: 0.3469 - val_accuracy: 0.8445 - val_precision: 0.8647 - val_recall: 0.8582\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2795 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8000\n",
            "Epoch 63: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8676 - precision: 0.8625 - recall: 0.8869 - val_loss: 0.3483 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3884 - accuracy: 0.8750 - precision: 0.9091 - recall: 0.7692\n",
            "Epoch 64: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8687 - precision: 0.8671 - recall: 0.8828 - val_loss: 0.3466 - val_accuracy: 0.8445 - val_precision: 0.8647 - val_recall: 0.8582\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2353 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 65: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8729 - precision: 0.8696 - recall: 0.8889 - val_loss: 0.3462 - val_accuracy: 0.8487 - val_precision: 0.8657 - val_recall: 0.8657\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4850 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 66: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8697 - precision: 0.8630 - recall: 0.8909 - val_loss: 0.3479 - val_accuracy: 0.8445 - val_precision: 0.8647 - val_recall: 0.8582\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1181 - accuracy: 0.9688 - precision: 0.9474 - recall: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8697 - precision: 0.8644 - recall: 0.8889 - val_loss: 0.3456 - val_accuracy: 0.8487 - val_precision: 0.8657 - val_recall: 0.8657\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1555 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 68: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8729 - precision: 0.8652 - recall: 0.8949 - val_loss: 0.3459 - val_accuracy: 0.8529 - val_precision: 0.8722 - val_recall: 0.8657\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3496 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 69: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8739 - precision: 0.8684 - recall: 0.8929 - val_loss: 0.3458 - val_accuracy: 0.8487 - val_precision: 0.8657 - val_recall: 0.8657\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3580 - accuracy: 0.8438 - precision: 0.8500 - recall: 0.8947\n",
            "Epoch 70: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8761 - precision: 0.8660 - recall: 0.9010 - val_loss: 0.3464 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4247 - accuracy: 0.8438 - precision: 0.7647 - recall: 0.9286\n",
            "Epoch 71: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8782 - precision: 0.8665 - recall: 0.9051 - val_loss: 0.3463 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3255 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 72: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8803 - precision: 0.8699 - recall: 0.9051 - val_loss: 0.3451 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2196 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 73: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8792 - precision: 0.8682 - recall: 0.9051 - val_loss: 0.3456 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4479 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 74: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8792 - precision: 0.8682 - recall: 0.9051 - val_loss: 0.3438 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3649 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 75: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.8803 - precision: 0.8699 - recall: 0.9051 - val_loss: 0.3422 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3670 - accuracy: 0.9062 - precision: 0.7273 - recall: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3187 - accuracy: 0.8792 - precision: 0.8668 - recall: 0.9071 - val_loss: 0.3412 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4020 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.9474\n",
            "Epoch 77: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8813 - precision: 0.8702 - recall: 0.9071 - val_loss: 0.3415 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3318 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 78: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8813 - precision: 0.8687 - recall: 0.9091 - val_loss: 0.3423 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3389 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 79: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3156 - accuracy: 0.8803 - precision: 0.8671 - recall: 0.9091 - val_loss: 0.3422 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2291 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 80: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3148 - accuracy: 0.8855 - precision: 0.8712 - recall: 0.9152 - val_loss: 0.3419 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3739 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 81: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.8855 - precision: 0.8726 - recall: 0.9131 - val_loss: 0.3412 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2597 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 82: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8834 - precision: 0.8692 - recall: 0.9131 - val_loss: 0.3418 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2291 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 83: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8803 - precision: 0.8671 - recall: 0.9091 - val_loss: 0.3405 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1701 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 84: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8824 - precision: 0.8676 - recall: 0.9131 - val_loss: 0.3423 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2613 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 85: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.8824 - precision: 0.8704 - recall: 0.9091 - val_loss: 0.3408 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4705 - accuracy: 0.7500 - precision: 0.7619 - recall: 0.8421\n",
            "Epoch 86: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8834 - precision: 0.8707 - recall: 0.9111 - val_loss: 0.3391 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3965 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 87: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8813 - precision: 0.8673 - recall: 0.9111 - val_loss: 0.3380 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2478 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 88: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3077 - accuracy: 0.8813 - precision: 0.8673 - recall: 0.9111 - val_loss: 0.3370 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4174 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 89: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8824 - precision: 0.8676 - recall: 0.9131 - val_loss: 0.3372 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3166 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8834 - precision: 0.8678 - recall: 0.9152 - val_loss: 0.3365 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1706 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 91: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3056 - accuracy: 0.8845 - precision: 0.8709 - recall: 0.9131 - val_loss: 0.3365 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3522 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 92: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3046 - accuracy: 0.8824 - precision: 0.8690 - recall: 0.9111 - val_loss: 0.3353 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2392 - accuracy: 0.9375 - precision: 0.8571 - recall: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8803 - precision: 0.8671 - recall: 0.9091 - val_loss: 0.3354 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3436 - accuracy: 0.7812 - precision: 0.7273 - recall: 0.9412\n",
            "Epoch 94: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.8855 - precision: 0.8712 - recall: 0.9152 - val_loss: 0.3360 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1778 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 95: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8866 - precision: 0.8772 - recall: 0.9091 - val_loss: 0.3342 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3954 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 96: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8866 - precision: 0.8700 - recall: 0.9192 - val_loss: 0.3351 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3309 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 97: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8813 - precision: 0.8702 - recall: 0.9071 - val_loss: 0.3339 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 98/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3167 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 98: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8845 - precision: 0.8723 - recall: 0.9111 - val_loss: 0.3326 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 99/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1152 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 99: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.8845 - precision: 0.8709 - recall: 0.9131 - val_loss: 0.3334 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 100/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3377 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 100: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.8908 - precision: 0.8767 - recall: 0.9192 - val_loss: 0.3327 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 101/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2511 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.9565\n",
            "Epoch 101: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8908 - precision: 0.8781 - recall: 0.9172 - val_loss: 0.3332 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 102/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1843 - accuracy: 0.9375 - precision: 0.9000 - recall: 1.0000\n",
            "Epoch 102: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8866 - precision: 0.8728 - recall: 0.9152 - val_loss: 0.3331 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 103/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2310 - accuracy: 0.9375 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 103: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8918 - precision: 0.8798 - recall: 0.9172 - val_loss: 0.3314 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 104/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2731 - accuracy: 0.9062 - precision: 0.8235 - recall: 1.0000\n",
            "Epoch 104: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8887 - precision: 0.8719 - recall: 0.9212 - val_loss: 0.3319 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 105/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2496 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 105: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8855 - precision: 0.8726 - recall: 0.9131 - val_loss: 0.3332 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 106/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2581 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9091\n",
            "Epoch 106: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8908 - precision: 0.8752 - recall: 0.9212 - val_loss: 0.3327 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 107/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1910 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 107: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2937 - accuracy: 0.8939 - precision: 0.8788 - recall: 0.9232 - val_loss: 0.3328 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 108/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3747 - accuracy: 0.8750 - precision: 0.7692 - recall: 0.9091\n",
            "Epoch 108: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8897 - precision: 0.8779 - recall: 0.9152 - val_loss: 0.3306 - val_accuracy: 0.8697 - val_precision: 0.8872 - val_recall: 0.8806\n",
            "Epoch 109/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4919 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 109: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8908 - precision: 0.8752 - recall: 0.9212 - val_loss: 0.3333 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 110/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1572 - accuracy: 0.9375 - precision: 0.9524 - recall: 0.9524\n",
            "Epoch 110: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8939 - precision: 0.8788 - recall: 0.9232 - val_loss: 0.3316 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 111/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6024 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 111: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8950 - precision: 0.8835 - recall: 0.9192 - val_loss: 0.3296 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 112/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1295 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 112: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8929 - precision: 0.8801 - recall: 0.9192 - val_loss: 0.3292 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 113/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2657 - accuracy: 0.9375 - precision: 0.8750 - recall: 1.0000\n",
            "Epoch 113: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8960 - precision: 0.8808 - recall: 0.9253 - val_loss: 0.3282 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 114/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4217 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 114: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8971 - precision: 0.8810 - recall: 0.9273 - val_loss: 0.3292 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 115/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4546 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 115: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8960 - precision: 0.8837 - recall: 0.9212 - val_loss: 0.3282 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 116/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3219 - accuracy: 0.8750 - precision: 0.9333 - recall: 0.8235\n",
            "Epoch 116: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2872 - accuracy: 0.8992 - precision: 0.8815 - recall: 0.9313 - val_loss: 0.3278 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 117/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3270 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 117: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2867 - accuracy: 0.8971 - precision: 0.8810 - recall: 0.9273 - val_loss: 0.3286 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 118/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2167 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 118: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8981 - precision: 0.8812 - recall: 0.9293 - val_loss: 0.3298 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 119/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3663 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 119: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8960 - precision: 0.8793 - recall: 0.9273 - val_loss: 0.3274 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 120/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3279 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 120: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8992 - precision: 0.8844 - recall: 0.9273 - val_loss: 0.3267 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 121/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1352 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 121: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8939 - precision: 0.8774 - recall: 0.9253 - val_loss: 0.3281 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 122/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3645 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 122: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2834 - accuracy: 0.8981 - precision: 0.8827 - recall: 0.9273 - val_loss: 0.3263 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 123/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2307 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 123: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8960 - precision: 0.8808 - recall: 0.9253 - val_loss: 0.3248 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 124/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2833 - accuracy: 0.9062 - precision: 0.8462 - recall: 0.9167\n",
            "Epoch 124: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8960 - precision: 0.8779 - recall: 0.9293 - val_loss: 0.3258 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 125/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1976 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 125: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2807 - accuracy: 0.8960 - precision: 0.8808 - recall: 0.9253 - val_loss: 0.3237 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 126/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3758 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 126: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8971 - precision: 0.8795 - recall: 0.9293 - val_loss: 0.3243 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 127/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2607 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 127: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8960 - precision: 0.8808 - recall: 0.9253 - val_loss: 0.3241 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 128/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1754 - accuracy: 0.9375 - precision: 0.8750 - recall: 1.0000\n",
            "Epoch 128: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8950 - precision: 0.8776 - recall: 0.9273 - val_loss: 0.3219 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 129/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3061 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 129: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8960 - precision: 0.8764 - recall: 0.9313 - val_loss: 0.3238 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 130/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3334 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 130: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8960 - precision: 0.8793 - recall: 0.9273 - val_loss: 0.3225 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 131/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3602 - accuracy: 0.7812 - precision: 0.6000 - recall: 0.9000\n",
            "Epoch 131: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8981 - precision: 0.8827 - recall: 0.9273 - val_loss: 0.3220 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 132/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3885 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 132: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.9002 - precision: 0.8846 - recall: 0.9293 - val_loss: 0.3222 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 133/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4034 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.7692\n",
            "Epoch 133: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2754 - accuracy: 0.8981 - precision: 0.8812 - recall: 0.9293 - val_loss: 0.3220 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 134/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2163 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 134: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.8981 - precision: 0.8827 - recall: 0.9273 - val_loss: 0.3205 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 135/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1843 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 135: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.9002 - precision: 0.8831 - recall: 0.9313 - val_loss: 0.3210 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 136/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3196 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 136: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.9002 - precision: 0.8846 - recall: 0.9293 - val_loss: 0.3186 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 137/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3126 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 137: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8992 - precision: 0.8829 - recall: 0.9293 - val_loss: 0.3212 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 138/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1984 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 138: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8971 - precision: 0.8810 - recall: 0.9273 - val_loss: 0.3206 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 139/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4449 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 139: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2712 - accuracy: 0.8971 - precision: 0.8795 - recall: 0.9293 - val_loss: 0.3216 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 140/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3208 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 140: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2712 - accuracy: 0.8971 - precision: 0.8810 - recall: 0.9273 - val_loss: 0.3195 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 141/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1916 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 141: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8971 - precision: 0.8795 - recall: 0.9293 - val_loss: 0.3212 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 142/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2690 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 142: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.9002 - precision: 0.8861 - recall: 0.9273 - val_loss: 0.3183 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 143/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4982 - accuracy: 0.8438 - precision: 0.7222 - recall: 1.0000\n",
            "Epoch 143: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.9002 - precision: 0.8861 - recall: 0.9273 - val_loss: 0.3187 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 144/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3439 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 144: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.9002 - precision: 0.8846 - recall: 0.9293 - val_loss: 0.3213 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 145/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3671 - accuracy: 0.8750 - precision: 0.9231 - recall: 0.8000\n",
            "Epoch 145: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.9002 - precision: 0.8846 - recall: 0.9293 - val_loss: 0.3208 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 146/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2281 - accuracy: 0.9375 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 146: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2673 - accuracy: 0.9002 - precision: 0.8831 - recall: 0.9313 - val_loss: 0.3208 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 147/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2887 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 147: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2669 - accuracy: 0.8992 - precision: 0.8829 - recall: 0.9293 - val_loss: 0.3216 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 148/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4240 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 148: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.9002 - precision: 0.8861 - recall: 0.9273 - val_loss: 0.3187 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 149/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1642 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 149: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.9034 - precision: 0.8882 - recall: 0.9313 - val_loss: 0.3193 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 150/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2693 - accuracy: 0.9062 - precision: 0.9130 - recall: 0.9545\n",
            "Epoch 150: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2647 - accuracy: 0.9023 - precision: 0.8880 - recall: 0.9293 - val_loss: 0.3170 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 151/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1840 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8571\n",
            "Epoch 151: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.9023 - precision: 0.8851 - recall: 0.9333 - val_loss: 0.3190 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 152/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1520 - accuracy: 0.9375 - precision: 0.9545 - recall: 0.9545\n",
            "Epoch 152: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.9002 - precision: 0.8876 - recall: 0.9253 - val_loss: 0.3178 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 153/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4076 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 153: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9055 - precision: 0.8902 - recall: 0.9333 - val_loss: 0.3186 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 154/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2993 - accuracy: 0.9062 - precision: 0.9048 - recall: 0.9500\n",
            "Epoch 154: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.9013 - precision: 0.8878 - recall: 0.9273 - val_loss: 0.3191 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 155/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3065 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8636\n",
            "Epoch 155: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.9002 - precision: 0.8876 - recall: 0.9253 - val_loss: 0.3176 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 156/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2368 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 156: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.9013 - precision: 0.8863 - recall: 0.9293 - val_loss: 0.3169 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 157/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3045 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 157: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8992 - precision: 0.8859 - recall: 0.9253 - val_loss: 0.3174 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 158/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3076 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 158: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.9034 - precision: 0.8868 - recall: 0.9333 - val_loss: 0.3194 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 159/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4232 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 159: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.9044 - precision: 0.8885 - recall: 0.9333 - val_loss: 0.3174 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 160/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1373 - accuracy: 0.9375 - precision: 0.9565 - recall: 0.9565\n",
            "Epoch 160: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.9034 - precision: 0.8897 - recall: 0.9293 - val_loss: 0.3187 - val_accuracy: 0.8697 - val_precision: 0.8931 - val_recall: 0.8731\n",
            "Epoch 161/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4269 - accuracy: 0.8438 - precision: 0.7647 - recall: 0.9286\n",
            "Epoch 161: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.9034 - precision: 0.8913 - recall: 0.9273 - val_loss: 0.3187 - val_accuracy: 0.8697 - val_precision: 0.8931 - val_recall: 0.8731\n",
            "Epoch 162/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3821 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 162: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2590 - accuracy: 0.9044 - precision: 0.8900 - recall: 0.9313 - val_loss: 0.3142 - val_accuracy: 0.8697 - val_precision: 0.8931 - val_recall: 0.8731\n",
            "Epoch 163/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1582 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 163: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.9055 - precision: 0.8902 - recall: 0.9333 - val_loss: 0.3165 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 164/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3072 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 164: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.9023 - precision: 0.8880 - recall: 0.9293 - val_loss: 0.3201 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 165/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2355 - accuracy: 0.9062 - precision: 0.9048 - recall: 0.9500\n",
            "Epoch 165: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.9034 - precision: 0.8913 - recall: 0.9273 - val_loss: 0.3191 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 166/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2807 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 166: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.9055 - precision: 0.8917 - recall: 0.9313 - val_loss: 0.3178 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 167/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1779 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9130\n",
            "Epoch 167: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.9055 - precision: 0.8932 - recall: 0.9293 - val_loss: 0.3155 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 168/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2767 - accuracy: 0.9062 - precision: 0.8333 - recall: 0.9091\n",
            "Epoch 168: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.9055 - precision: 0.8932 - recall: 0.9293 - val_loss: 0.3138 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 169/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2763 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 169: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.9076 - precision: 0.8951 - recall: 0.9313 - val_loss: 0.3165 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 170/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1610 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9286\n",
            "Epoch 170: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.9013 - precision: 0.8878 - recall: 0.9273 - val_loss: 0.3172 - val_accuracy: 0.8739 - val_precision: 0.9062 - val_recall: 0.8657\n",
            "Epoch 171/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4432 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 171: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.9065 - precision: 0.8934 - recall: 0.9313 - val_loss: 0.3152 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 172/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4730 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 172: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.9055 - precision: 0.8917 - recall: 0.9313 - val_loss: 0.3161 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 173/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2786 - accuracy: 0.8750 - precision: 0.8000 - recall: 1.0000\n",
            "Epoch 173: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9065 - precision: 0.8919 - recall: 0.9333 - val_loss: 0.3163 - val_accuracy: 0.8697 - val_precision: 0.8992 - val_recall: 0.8657\n",
            "Epoch 174/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2422 - accuracy: 0.9062 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 174: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9044 - precision: 0.8930 - recall: 0.9273 - val_loss: 0.3145 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 175/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2554 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 175: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.9034 - precision: 0.8897 - recall: 0.9293 - val_loss: 0.3148 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 176/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2503 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 176: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9065 - precision: 0.8934 - recall: 0.9313 - val_loss: 0.3131 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 177/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2050 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 177: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2519 - accuracy: 0.9065 - precision: 0.8919 - recall: 0.9333 - val_loss: 0.3131 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 178/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2129 - accuracy: 0.9062 - precision: 0.8462 - recall: 0.9167\n",
            "Epoch 178: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.9055 - precision: 0.8947 - recall: 0.9273 - val_loss: 0.3123 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 179/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3706 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.9231\n",
            "Epoch 179: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.9076 - precision: 0.8921 - recall: 0.9354 - val_loss: 0.3152 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 180/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2014 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 180: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.9055 - precision: 0.8963 - recall: 0.9253 - val_loss: 0.3086 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 181/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1454 - accuracy: 0.9375 - precision: 0.9524 - recall: 0.9524\n",
            "Epoch 181: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.9076 - precision: 0.8936 - recall: 0.9333 - val_loss: 0.3091 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 182/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2295 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 182: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.9023 - precision: 0.8911 - recall: 0.9253 - val_loss: 0.3079 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 183/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1432 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 183: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.9076 - precision: 0.8936 - recall: 0.9333 - val_loss: 0.3078 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 184/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1986 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 184: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.9023 - precision: 0.8865 - recall: 0.9313 - val_loss: 0.3098 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 185/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3161 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 185: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9034 - precision: 0.8913 - recall: 0.9273 - val_loss: 0.3083 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 186/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3248 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 186: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9065 - precision: 0.8949 - recall: 0.9293 - val_loss: 0.3054 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 187/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1803 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 187: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.9034 - precision: 0.8853 - recall: 0.9354 - val_loss: 0.3068 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 188/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3259 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 188: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9055 - precision: 0.8917 - recall: 0.9313 - val_loss: 0.3029 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 189/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2471 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 189: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.9065 - precision: 0.8889 - recall: 0.9374 - val_loss: 0.3066 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 190/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1253 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 190: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.9065 - precision: 0.8919 - recall: 0.9333 - val_loss: 0.3062 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 191/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3521 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 191: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.9076 - precision: 0.8906 - recall: 0.9374 - val_loss: 0.3095 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 192/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2092 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 192: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9044 - precision: 0.8900 - recall: 0.9313 - val_loss: 0.3074 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 193/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1028 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9333\n",
            "Epoch 193: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.9086 - precision: 0.8953 - recall: 0.9333 - val_loss: 0.3079 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 194/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1228 - accuracy: 0.9688 - precision: 0.9474 - recall: 1.0000\n",
            "Epoch 194: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.9076 - precision: 0.8921 - recall: 0.9354 - val_loss: 0.3085 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 195/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3004 - accuracy: 0.8750 - precision: 0.7895 - recall: 1.0000\n",
            "Epoch 195: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2437 - accuracy: 0.9086 - precision: 0.8923 - recall: 0.9374 - val_loss: 0.3110 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 196/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1819 - accuracy: 0.9375 - precision: 0.9583 - recall: 0.9583\n",
            "Epoch 196: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9086 - precision: 0.8953 - recall: 0.9333 - val_loss: 0.3096 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 197/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2946 - accuracy: 0.8438 - precision: 0.7619 - recall: 1.0000\n",
            "Epoch 197: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9065 - precision: 0.8965 - recall: 0.9273 - val_loss: 0.3081 - val_accuracy: 0.8866 - val_precision: 0.9084 - val_recall: 0.8881\n",
            "Epoch 198/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2816 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 198: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.9086 - precision: 0.8953 - recall: 0.9333 - val_loss: 0.3096 - val_accuracy: 0.8782 - val_precision: 0.9070 - val_recall: 0.8731\n",
            "Epoch 199/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.0877 - accuracy: 0.9688 - precision: 0.9444 - recall: 1.0000\n",
            "Epoch 199: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9086 - precision: 0.8938 - recall: 0.9354 - val_loss: 0.3087 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 200/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1584 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 200: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.9086 - precision: 0.8969 - recall: 0.9313 - val_loss: 0.3051 - val_accuracy: 0.8824 - val_precision: 0.9015 - val_recall: 0.8881\n",
            "Epoch 201/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1509 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 201: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2409 - accuracy: 0.9065 - precision: 0.8919 - recall: 0.9333 - val_loss: 0.3120 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 202/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1714 - accuracy: 0.9375 - precision: 0.9167 - recall: 0.9167\n",
            "Epoch 202: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9118 - precision: 0.8990 - recall: 0.9354 - val_loss: 0.3115 - val_accuracy: 0.8782 - val_precision: 0.9008 - val_recall: 0.8806\n",
            "Epoch 203/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2149 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8750\n",
            "Epoch 203: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.9107 - precision: 0.8942 - recall: 0.9394 - val_loss: 0.3170 - val_accuracy: 0.8824 - val_precision: 0.9141 - val_recall: 0.8731\n",
            "Epoch 204/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4038 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 204: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.9118 - precision: 0.8990 - recall: 0.9354 - val_loss: 0.3123 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 205/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3441 - accuracy: 0.8750 - precision: 0.8696 - recall: 0.9524\n",
            "Epoch 205: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2402 - accuracy: 0.9076 - precision: 0.8951 - recall: 0.9313 - val_loss: 0.3119 - val_accuracy: 0.8739 - val_precision: 0.9000 - val_recall: 0.8731\n",
            "Epoch 206/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1946 - accuracy: 0.8750 - precision: 0.9474 - recall: 0.8571\n",
            "Epoch 206: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.9097 - precision: 0.8971 - recall: 0.9333 - val_loss: 0.3100 - val_accuracy: 0.8824 - val_precision: 0.9015 - val_recall: 0.8881\n",
            "Epoch 207/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1290 - accuracy: 0.9688 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 207: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.9076 - precision: 0.8951 - recall: 0.9313 - val_loss: 0.3077 - val_accuracy: 0.8824 - val_precision: 0.9015 - val_recall: 0.8881\n",
            "Epoch 208/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1270 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 208: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9107 - precision: 0.8988 - recall: 0.9333 - val_loss: 0.3054 - val_accuracy: 0.8824 - val_precision: 0.9015 - val_recall: 0.8881\n",
            "Epoch 208: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8x4x1"
      ],
      "metadata": {
        "id": "mNiuVB8BD3Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eightxfourxone_model = Sequential(name=\"8x4x1_model\")\n",
        "eightxfourxone_model.add(Input(shape=(X.shape[1],)))\n",
        "eightxfourxone_model.add(Dense(8, activation='relu'))\n",
        "eightxfourxone_model.add(Dense(4, activation='relu'))\n",
        "eightxfourxone_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "eightxfourxone_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "eightxfourxone_history = eightxfourxone_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVSbkRNODCfH",
        "outputId": "c5987591-4796-4013-bf57-57514f5e4054"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7090 - accuracy: 0.5938 - precision: 0.8509 - recall: 0.8896\n",
            "Epoch 1: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7190 - accuracy: 0.5032 - precision: 0.5614 - recall: 0.9300 - val_loss: 0.6770 - val_accuracy: 0.5588 - val_precision: 0.5656 - val_recall: 0.9328\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6998 - accuracy: 0.4688 - precision: 0.5000 - recall: 0.8824\n",
            "Epoch 2: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.5399 - precision: 0.5323 - recall: 0.9475 - val_loss: 0.6415 - val_accuracy: 0.5840 - val_precision: 0.5814 - val_recall: 0.9328\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7399 - accuracy: 0.5312 - precision: 0.5185 - recall: 0.8750\n",
            "Epoch 3: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6008 - precision: 0.5692 - recall: 0.9556 - val_loss: 0.6100 - val_accuracy: 0.6555 - val_precision: 0.6287 - val_recall: 0.9478\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6631 - accuracy: 0.5625 - precision: 0.5000 - recall: 0.9286\n",
            "Epoch 4: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.6901 - precision: 0.6333 - recall: 0.9596 - val_loss: 0.5826 - val_accuracy: 0.7227 - val_precision: 0.6789 - val_recall: 0.9627\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5762 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.8824\n",
            "Epoch 5: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7353 - precision: 0.6748 - recall: 0.9475 - val_loss: 0.5603 - val_accuracy: 0.7437 - val_precision: 0.7086 - val_recall: 0.9254\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5590 - accuracy: 0.8125 - precision: 0.7143 - recall: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5628 - accuracy: 0.7731 - precision: 0.7183 - recall: 0.9273 - val_loss: 0.5419 - val_accuracy: 0.7605 - val_precision: 0.7333 - val_recall: 0.9030\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5553 - accuracy: 0.7812 - precision: 0.7083 - recall: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5473 - accuracy: 0.7847 - precision: 0.7324 - recall: 0.9232 - val_loss: 0.5267 - val_accuracy: 0.7857 - val_precision: 0.7643 - val_recall: 0.8955\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5138 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.9474\n",
            "Epoch 8: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7952 - precision: 0.7483 - recall: 0.9131 - val_loss: 0.5136 - val_accuracy: 0.7941 - val_precision: 0.7778 - val_recall: 0.8881\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6118 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.8824\n",
            "Epoch 9: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.8078 - precision: 0.7680 - recall: 0.9030 - val_loss: 0.5031 - val_accuracy: 0.8067 - val_precision: 0.7933 - val_recall: 0.8881\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5751 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 10: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5154 - accuracy: 0.8130 - precision: 0.7737 - recall: 0.9051 - val_loss: 0.4947 - val_accuracy: 0.8193 - val_precision: 0.8095 - val_recall: 0.8881\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5322 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.9375\n",
            "Epoch 11: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.8172 - precision: 0.7791 - recall: 0.9051 - val_loss: 0.4877 - val_accuracy: 0.8193 - val_precision: 0.8138 - val_recall: 0.8806\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5147 - accuracy: 0.7500 - precision: 0.7727 - recall: 0.8500\n",
            "Epoch 12: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.8298 - precision: 0.7957 - recall: 0.9051 - val_loss: 0.4815 - val_accuracy: 0.8151 - val_precision: 0.8214 - val_recall: 0.8582\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4065 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 13: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.8340 - precision: 0.8025 - recall: 0.9030 - val_loss: 0.4763 - val_accuracy: 0.8193 - val_precision: 0.8273 - val_recall: 0.8582\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4460 - accuracy: 0.8125 - precision: 0.7826 - recall: 0.9474\n",
            "Epoch 14: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4898 - accuracy: 0.8361 - precision: 0.8122 - recall: 0.8909 - val_loss: 0.4713 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4221 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 15: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.8435 - precision: 0.8216 - recall: 0.8929 - val_loss: 0.4664 - val_accuracy: 0.8193 - val_precision: 0.8370 - val_recall: 0.8433\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4852 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 16: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.4620 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4865 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 17: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.8414 - precision: 0.8282 - recall: 0.8768 - val_loss: 0.4579 - val_accuracy: 0.8361 - val_precision: 0.8626 - val_recall: 0.8433\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4594 - accuracy: 0.7812 - precision: 0.8696 - recall: 0.8333\n",
            "Epoch 18: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4707 - accuracy: 0.8477 - precision: 0.8391 - recall: 0.8747 - val_loss: 0.4547 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4413 - accuracy: 0.7812 - precision: 0.8750 - recall: 0.7368\n",
            "Epoch 19: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.8487 - precision: 0.8421 - recall: 0.8727 - val_loss: 0.4511 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3101 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4626 - accuracy: 0.8487 - precision: 0.8448 - recall: 0.8687 - val_loss: 0.4472 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4217 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 21: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.8529 - precision: 0.8515 - recall: 0.8687 - val_loss: 0.4434 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4724 - accuracy: 0.8125 - precision: 0.8095 - recall: 0.8947\n",
            "Epoch 22: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.4398 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4293 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 23: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8519 - precision: 0.8569 - recall: 0.8586 - val_loss: 0.4362 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4746 - accuracy: 0.8750 - precision: 0.7857 - recall: 0.9167\n",
            "Epoch 24: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8529 - precision: 0.8600 - recall: 0.8566 - val_loss: 0.4330 - val_accuracy: 0.8319 - val_precision: 0.8730 - val_recall: 0.8209\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4140 - accuracy: 0.8125 - precision: 0.9375 - recall: 0.7500\n",
            "Epoch 25: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8519 - precision: 0.8642 - recall: 0.8485 - val_loss: 0.4301 - val_accuracy: 0.8319 - val_precision: 0.8730 - val_recall: 0.8209\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3474 - accuracy: 0.9375 - precision: 0.8824 - recall: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8582 - precision: 0.8689 - recall: 0.8566 - val_loss: 0.4271 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3441 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8750\n",
            "Epoch 27: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8571 - precision: 0.8701 - recall: 0.8525 - val_loss: 0.4244 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2793 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8571 - precision: 0.8701 - recall: 0.8525 - val_loss: 0.4214 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3972 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 29: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4282 - accuracy: 0.8592 - precision: 0.8722 - recall: 0.8545 - val_loss: 0.4180 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4492 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 30: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8613 - precision: 0.8773 - recall: 0.8525 - val_loss: 0.4138 - val_accuracy: 0.8319 - val_precision: 0.8790 - val_recall: 0.8134\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3282 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 31: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8613 - precision: 0.8773 - recall: 0.8525 - val_loss: 0.4106 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5089 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 32: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8624 - precision: 0.8792 - recall: 0.8525 - val_loss: 0.4063 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4787 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 33: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8603 - precision: 0.8787 - recall: 0.8485 - val_loss: 0.4018 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5621 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 34: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4053 - accuracy: 0.8613 - precision: 0.8789 - recall: 0.8505 - val_loss: 0.3971 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2630 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 35: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8603 - precision: 0.8787 - recall: 0.8485 - val_loss: 0.3932 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3694 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 36: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8613 - precision: 0.8805 - recall: 0.8485 - val_loss: 0.3895 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3601 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 37: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8634 - precision: 0.8810 - recall: 0.8525 - val_loss: 0.3864 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4102 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 38: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8603 - precision: 0.8803 - recall: 0.8465 - val_loss: 0.3830 - val_accuracy: 0.8445 - val_precision: 0.9008 - val_recall: 0.8134\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4700 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 39: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8603 - precision: 0.8819 - recall: 0.8444 - val_loss: 0.3796 - val_accuracy: 0.8445 - val_precision: 0.9008 - val_recall: 0.8134\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3539 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 40: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8613 - precision: 0.8805 - recall: 0.8485 - val_loss: 0.3774 - val_accuracy: 0.8445 - val_precision: 0.9008 - val_recall: 0.8134\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4575 - accuracy: 0.7812 - precision: 0.8095 - recall: 0.8500\n",
            "Epoch 41: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8613 - precision: 0.8805 - recall: 0.8485 - val_loss: 0.3748 - val_accuracy: 0.8445 - val_precision: 0.9008 - val_recall: 0.8134\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4785 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 42: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8603 - precision: 0.8787 - recall: 0.8485 - val_loss: 0.3727 - val_accuracy: 0.8445 - val_precision: 0.9008 - val_recall: 0.8134\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3519 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.9474\n",
            "Epoch 43: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8592 - precision: 0.8784 - recall: 0.8465 - val_loss: 0.3700 - val_accuracy: 0.8487 - val_precision: 0.9016 - val_recall: 0.8209\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4408 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 44: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8571 - precision: 0.8779 - recall: 0.8424 - val_loss: 0.3678 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3913 - accuracy: 0.8750 - precision: 0.8000 - recall: 1.0000\n",
            "Epoch 45: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8592 - precision: 0.8784 - recall: 0.8465 - val_loss: 0.3656 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4250 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 46: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8613 - precision: 0.8789 - recall: 0.8505 - val_loss: 0.3640 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3342 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 47: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8592 - precision: 0.8784 - recall: 0.8465 - val_loss: 0.3623 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2638 - accuracy: 0.9688 - precision: 0.9524 - recall: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8582 - precision: 0.8782 - recall: 0.8444 - val_loss: 0.3601 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2708 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 49: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8582 - precision: 0.8766 - recall: 0.8465 - val_loss: 0.3575 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3590 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 50: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8634 - precision: 0.8763 - recall: 0.8586 - val_loss: 0.3571 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2935 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 51: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8592 - precision: 0.8753 - recall: 0.8505 - val_loss: 0.3552 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2704 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8235\n",
            "Epoch 52: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8613 - precision: 0.8758 - recall: 0.8545 - val_loss: 0.3546 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3769 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 53: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8624 - precision: 0.8776 - recall: 0.8545 - val_loss: 0.3535 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2546 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 54: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8634 - precision: 0.8763 - recall: 0.8586 - val_loss: 0.3528 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2349 - accuracy: 0.9062 - precision: 0.8333 - recall: 0.9091\n",
            "Epoch 55: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8613 - precision: 0.8773 - recall: 0.8525 - val_loss: 0.3516 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3675 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 56: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8624 - precision: 0.8776 - recall: 0.8545 - val_loss: 0.3508 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6306 - accuracy: 0.7500 - precision: 0.7059 - recall: 0.8000\n",
            "Epoch 57: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8634 - precision: 0.8778 - recall: 0.8566 - val_loss: 0.3492 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2646 - accuracy: 0.9062 - precision: 0.9545 - recall: 0.9130\n",
            "Epoch 58: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8666 - precision: 0.8786 - recall: 0.8626 - val_loss: 0.3486 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4074 - accuracy: 0.7812 - precision: 0.8667 - recall: 0.7222\n",
            "Epoch 59: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8666 - precision: 0.8786 - recall: 0.8626 - val_loss: 0.3486 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3597 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 60: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8666 - precision: 0.8786 - recall: 0.8626 - val_loss: 0.3480 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3639 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 61: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8687 - precision: 0.8791 - recall: 0.8667 - val_loss: 0.3477 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2110 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 62: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8697 - precision: 0.8793 - recall: 0.8687 - val_loss: 0.3481 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 63: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8676 - precision: 0.8804 - recall: 0.8626 - val_loss: 0.3468 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3512 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 64: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8718 - precision: 0.8814 - recall: 0.8707 - val_loss: 0.3464 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2076 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 65: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8718 - precision: 0.8814 - recall: 0.8707 - val_loss: 0.3467 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2533 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 66: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8718 - precision: 0.8798 - recall: 0.8727 - val_loss: 0.3463 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2332 - accuracy: 0.9375 - precision: 0.8824 - recall: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8687 - precision: 0.8776 - recall: 0.8687 - val_loss: 0.3458 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3570 - accuracy: 0.8750 - precision: 0.8182 - recall: 0.8182\n",
            "Epoch 68: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8718 - precision: 0.8798 - recall: 0.8727 - val_loss: 0.3454 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1971 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9333\n",
            "Epoch 69: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8708 - precision: 0.8796 - recall: 0.8707 - val_loss: 0.3450 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2358 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8729 - precision: 0.8801 - recall: 0.8747 - val_loss: 0.3454 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3186 - accuracy: 0.9688 - precision: 0.9474 - recall: 1.0000\n",
            "Epoch 71: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3452 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1916 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9375\n",
            "Epoch 72: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3452 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3454 - accuracy: 0.8750 - precision: 0.8000 - recall: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8750 - precision: 0.8821 - recall: 0.8768 - val_loss: 0.3451 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3284 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 74: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8803 - precision: 0.8864 - recall: 0.8828 - val_loss: 0.3436 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2549 - accuracy: 0.9062 - precision: 0.8333 - recall: 0.9091\n",
            "Epoch 75: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3440 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1705 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8792 - precision: 0.8815 - recall: 0.8869 - val_loss: 0.3450 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4523 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 77: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3451 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4192 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 78: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3439 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3104 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8333\n",
            "Epoch 79: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3447 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3309 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 80: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3447 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4567 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 81: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3451 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4151 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 82: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3453 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3277 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.8462\n",
            "Epoch 83: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8813 - precision: 0.8851 - recall: 0.8869 - val_loss: 0.3452 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3713 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 84: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3451 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3567 - accuracy: 0.8438 - precision: 0.8500 - recall: 0.8947\n",
            "Epoch 85: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8813 - precision: 0.8835 - recall: 0.8889 - val_loss: 0.3452 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1709 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9500\n",
            "Epoch 86: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8782 - precision: 0.8813 - recall: 0.8848 - val_loss: 0.3450 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3715 - accuracy: 0.8750 - precision: 0.8000 - recall: 1.0000\n",
            "Epoch 87: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3443 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5181 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 88: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8813 - precision: 0.8866 - recall: 0.8848 - val_loss: 0.3440 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3494 - accuracy: 0.9062 - precision: 0.8750 - recall: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3445 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3338 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 90: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3445 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2913 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8333\n",
            "Epoch 91: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8803 - precision: 0.8833 - recall: 0.8869 - val_loss: 0.3443 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3720 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 92: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3440 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1883 - accuracy: 0.9688 - precision: 0.9286 - recall: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3442 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2951 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 94: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8845 - precision: 0.8889 - recall: 0.8889 - val_loss: 0.3438 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 94: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8x1\n",
        "- val_accuracy: 0.8193\n",
        "- val_loss: 0.3666\n",
        "- val_precision: 0.8346\n",
        "- val_recall: 0.8409"
      ],
      "metadata": {
        "id": "rGqsl9q0b3bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eightx1_model = Sequential(name=\"8x1_model\")\n",
        "eightx1_model.add(Input(shape=(X.shape[1],)))\n",
        "eightx1_model.add(Dense(8, activation='relu'))\n",
        "eightx1_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "eightx1_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "eightx1_history = eightx1_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ob6mGikgblE1",
        "outputId": "0d52ff25-a523-4e26-a350-6bb899a7b37d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 14s - loss: 0.9243 - accuracy: 0.3438 - precision: 0.8467 - recall: 0.7582\n",
            "Epoch 1: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7013 - accuracy: 0.5557 - precision: 0.7172 - recall: 0.4436 - val_loss: 0.6917 - val_accuracy: 0.5714 - val_precision: 0.7500 - val_recall: 0.3582\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5851 - accuracy: 0.6875 - precision: 0.8000 - recall: 0.5000\n",
            "Epoch 2: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6481 - precision: 0.7685 - recall: 0.4626 - val_loss: 0.6321 - val_accuracy: 0.6345 - val_precision: 0.8219 - val_recall: 0.4478\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6338 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.6667\n",
            "Epoch 3: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5880 - accuracy: 0.7027 - precision: 0.8252 - recall: 0.5434 - val_loss: 0.5826 - val_accuracy: 0.7227 - val_precision: 0.8778 - val_recall: 0.5896\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6101 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.4615\n",
            "Epoch 4: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7574 - precision: 0.8492 - recall: 0.6485 - val_loss: 0.5436 - val_accuracy: 0.7689 - val_precision: 0.8762 - val_recall: 0.6866\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6476 - accuracy: 0.7812 - precision: 0.8462 - recall: 0.6875\n",
            "Epoch 5: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5176 - accuracy: 0.7868 - precision: 0.8427 - recall: 0.7253 - val_loss: 0.5128 - val_accuracy: 0.7899 - val_precision: 0.8889 - val_recall: 0.7164\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4339 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 6: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.8046 - precision: 0.8441 - recall: 0.7657 - val_loss: 0.4858 - val_accuracy: 0.8151 - val_precision: 0.9018 - val_recall: 0.7537\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4997 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 7: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.8151 - precision: 0.8445 - recall: 0.7899 - val_loss: 0.4644 - val_accuracy: 0.8319 - val_precision: 0.9052 - val_recall: 0.7836\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5349 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 8: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.8183 - precision: 0.8426 - recall: 0.8000 - val_loss: 0.4471 - val_accuracy: 0.8277 - val_precision: 0.8843 - val_recall: 0.7985\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6034 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.6429\n",
            "Epoch 9: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.8214 - precision: 0.8436 - recall: 0.8061 - val_loss: 0.4332 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4989 - accuracy: 0.7188 - precision: 0.6000 - recall: 0.5455\n",
            "Epoch 10: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8235 - precision: 0.8413 - recall: 0.8141 - val_loss: 0.4213 - val_accuracy: 0.8277 - val_precision: 0.8720 - val_recall: 0.8134\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4275 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 11: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8172 - precision: 0.8309 - recall: 0.8141 - val_loss: 0.4128 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5600 - accuracy: 0.7812 - precision: 0.8462 - recall: 0.6875\n",
            "Epoch 12: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8204 - precision: 0.8306 - recall: 0.8222 - val_loss: 0.4051 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5692 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 13: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8225 - precision: 0.8327 - recall: 0.8242 - val_loss: 0.3992 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4171 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 14: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8246 - precision: 0.8374 - recall: 0.8222 - val_loss: 0.3942 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2698 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 15: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4001 - accuracy: 0.8267 - precision: 0.8381 - recall: 0.8263 - val_loss: 0.3894 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2772 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 16: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8256 - precision: 0.8364 - recall: 0.8263 - val_loss: 0.3859 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4568 - accuracy: 0.7188 - precision: 0.7857 - recall: 0.6471\n",
            "Epoch 17: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8267 - precision: 0.8381 - recall: 0.8263 - val_loss: 0.3825 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3385 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 18: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3895 - accuracy: 0.8277 - precision: 0.8371 - recall: 0.8303 - val_loss: 0.3796 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3769 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 19: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8288 - precision: 0.8388 - recall: 0.8303 - val_loss: 0.3772 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4482 - accuracy: 0.7812 - precision: 0.7368 - recall: 0.8750\n",
            "Epoch 20: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8288 - precision: 0.8374 - recall: 0.8323 - val_loss: 0.3746 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3972 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 21: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8298 - precision: 0.8391 - recall: 0.8323 - val_loss: 0.3722 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4328 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 22: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8340 - precision: 0.8404 - recall: 0.8404 - val_loss: 0.3702 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2654 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8000\n",
            "Epoch 23: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8330 - precision: 0.8401 - recall: 0.8384 - val_loss: 0.3689 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2621 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8235\n",
            "Epoch 24: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8351 - precision: 0.8407 - recall: 0.8424 - val_loss: 0.3674 - val_accuracy: 0.8109 - val_precision: 0.8397 - val_recall: 0.8209\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3449 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 25: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8351 - precision: 0.8407 - recall: 0.8424 - val_loss: 0.3667 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2880 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 26: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8403 - precision: 0.8451 - recall: 0.8485 - val_loss: 0.3660 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2893 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 27: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8414 - precision: 0.8454 - recall: 0.8505 - val_loss: 0.3654 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2890 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8333\n",
            "Epoch 28: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8424 - precision: 0.8471 - recall: 0.8505 - val_loss: 0.3646 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3365 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 29: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.8424 - precision: 0.8471 - recall: 0.8505 - val_loss: 0.3643 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3176 - accuracy: 0.8438 - precision: 0.9048 - recall: 0.8636\n",
            "Epoch 30: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8414 - precision: 0.8468 - recall: 0.8485 - val_loss: 0.3637 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2577 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 31: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8424 - precision: 0.8485 - recall: 0.8485 - val_loss: 0.3632 - val_accuracy: 0.8235 - val_precision: 0.8538 - val_recall: 0.8284\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2598 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8750\n",
            "Epoch 32: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8435 - precision: 0.8488 - recall: 0.8505 - val_loss: 0.3628 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3066 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 33: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8424 - precision: 0.8499 - recall: 0.8465 - val_loss: 0.3623 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4488 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 34: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8466 - precision: 0.8511 - recall: 0.8545 - val_loss: 0.3622 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5028 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.5833\n",
            "Epoch 35: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8414 - precision: 0.8482 - recall: 0.8465 - val_loss: 0.3619 - val_accuracy: 0.8235 - val_precision: 0.8485 - val_recall: 0.8358\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4352 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 36: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8435 - precision: 0.8488 - recall: 0.8505 - val_loss: 0.3614 - val_accuracy: 0.8235 - val_precision: 0.8485 - val_recall: 0.8358\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3955 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.7692\n",
            "Epoch 37: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8435 - precision: 0.8488 - recall: 0.8505 - val_loss: 0.3608 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2933 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 38: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8456 - precision: 0.8494 - recall: 0.8545 - val_loss: 0.3607 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2282 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 39: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8466 - precision: 0.8497 - recall: 0.8566 - val_loss: 0.3602 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3357 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.7692\n",
            "Epoch 40: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3546 - accuracy: 0.8456 - precision: 0.8494 - recall: 0.8545 - val_loss: 0.3597 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3864 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 41: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3537 - accuracy: 0.8487 - precision: 0.8531 - recall: 0.8566 - val_loss: 0.3598 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3954 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.8182\n",
            "Epoch 42: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8477 - precision: 0.8528 - recall: 0.8545 - val_loss: 0.3596 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4312 - accuracy: 0.7812 - precision: 0.8182 - recall: 0.6429\n",
            "Epoch 43: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8466 - precision: 0.8511 - recall: 0.8545 - val_loss: 0.3588 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3596 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 44: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8477 - precision: 0.8543 - recall: 0.8525 - val_loss: 0.3584 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4624 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.6923\n",
            "Epoch 45: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8487 - precision: 0.8545 - recall: 0.8545 - val_loss: 0.3577 - val_accuracy: 0.8277 - val_precision: 0.8550 - val_recall: 0.8358\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4143 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 46: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8519 - precision: 0.8554 - recall: 0.8606 - val_loss: 0.3577 - val_accuracy: 0.8361 - val_precision: 0.8682 - val_recall: 0.8358\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2395 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9474\n",
            "Epoch 47: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8529 - precision: 0.8571 - recall: 0.8606 - val_loss: 0.3574 - val_accuracy: 0.8361 - val_precision: 0.8682 - val_recall: 0.8358\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3008 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 48: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3488 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3576 - val_accuracy: 0.8361 - val_precision: 0.8682 - val_recall: 0.8358\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3152 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 49: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8529 - precision: 0.8571 - recall: 0.8606 - val_loss: 0.3574 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5610 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.6667\n",
            "Epoch 50: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8550 - precision: 0.8592 - recall: 0.8626 - val_loss: 0.3570 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3680 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 51: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8550 - precision: 0.8592 - recall: 0.8626 - val_loss: 0.3566 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4748 - accuracy: 0.8750 - precision: 0.7647 - recall: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8540 - precision: 0.8589 - recall: 0.8606 - val_loss: 0.3561 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4033 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 53: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8561 - precision: 0.8609 - recall: 0.8626 - val_loss: 0.3556 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2714 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8182\n",
            "Epoch 54: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8540 - precision: 0.8589 - recall: 0.8606 - val_loss: 0.3555 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2433 - accuracy: 0.9688 - precision: 0.9545 - recall: 1.0000\n",
            "Epoch 55: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8540 - precision: 0.8574 - recall: 0.8626 - val_loss: 0.3552 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5198 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.8750\n",
            "Epoch 56: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8540 - precision: 0.8574 - recall: 0.8626 - val_loss: 0.3549 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3606 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 57: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3428 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3547 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5104 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.6667\n",
            "Epoch 58: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8550 - precision: 0.8592 - recall: 0.8626 - val_loss: 0.3540 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6603 - accuracy: 0.7188 - precision: 0.7647 - recall: 0.7222\n",
            "Epoch 59: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.8540 - precision: 0.8560 - recall: 0.8646 - val_loss: 0.3540 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3792 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 60: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8550 - precision: 0.8606 - recall: 0.8606 - val_loss: 0.3532 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3912 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 61: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8508 - precision: 0.8523 - recall: 0.8626 - val_loss: 0.3528 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3971 - accuracy: 0.8750 - precision: 0.9412 - recall: 0.8421\n",
            "Epoch 62: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3525 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3928 - accuracy: 0.8125 - precision: 0.8889 - recall: 0.8000\n",
            "Epoch 63: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8561 - precision: 0.8623 - recall: 0.8606 - val_loss: 0.3519 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3836 - accuracy: 0.7812 - precision: 0.7000 - recall: 0.9333\n",
            "Epoch 64: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8561 - precision: 0.8623 - recall: 0.8606 - val_loss: 0.3513 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3281 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 65: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8550 - precision: 0.8592 - recall: 0.8626 - val_loss: 0.3510 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3244 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 66: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8550 - precision: 0.8606 - recall: 0.8606 - val_loss: 0.3501 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2038 - accuracy: 0.9062 - precision: 0.8182 - recall: 0.9000\n",
            "Epoch 67: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3501 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4492 - accuracy: 0.7500 - precision: 0.7857 - recall: 0.6875\n",
            "Epoch 68: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8550 - precision: 0.8606 - recall: 0.8606 - val_loss: 0.3496 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3163 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 69: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.8540 - precision: 0.8532 - recall: 0.8687 - val_loss: 0.3494 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2772 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8561 - precision: 0.8580 - recall: 0.8667 - val_loss: 0.3490 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3061 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 71: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8529 - precision: 0.8543 - recall: 0.8646 - val_loss: 0.3494 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3002 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8889\n",
            "Epoch 72: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3344 - accuracy: 0.8561 - precision: 0.8594 - recall: 0.8646 - val_loss: 0.3484 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4422 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 73: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8540 - precision: 0.8546 - recall: 0.8667 - val_loss: 0.3484 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3176 - accuracy: 0.8750 - precision: 0.8333 - recall: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.3481 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2208 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 75: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.3480 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3627 - accuracy: 0.7188 - precision: 0.6875 - recall: 0.7333\n",
            "Epoch 76: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8561 - precision: 0.8552 - recall: 0.8707 - val_loss: 0.3480 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3445 - accuracy: 0.8438 - precision: 0.9167 - recall: 0.7333\n",
            "Epoch 77: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8582 - precision: 0.8571 - recall: 0.8727 - val_loss: 0.3480 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3051 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 78: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.3473 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2488 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 79: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3470 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2676 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 80: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8550 - precision: 0.8549 - recall: 0.8687 - val_loss: 0.3466 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5479 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.7692\n",
            "Epoch 81: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3467 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2438 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 82: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.3465 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2125 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 83: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3464 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2222 - accuracy: 0.9375 - precision: 0.8750 - recall: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3466 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2128 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 85: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.3460 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4948 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 86: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3459 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6002 - accuracy: 0.6562 - precision: 0.3846 - recall: 0.6250\n",
            "Epoch 87: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3455 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5389 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 88: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.3459 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2655 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 89: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3461 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2996 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 90: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3458 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4281 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 91: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3451 - val_accuracy: 0.8319 - val_precision: 0.8615 - val_recall: 0.8358\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1955 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 92: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.3450 - val_accuracy: 0.8361 - val_precision: 0.8682 - val_recall: 0.8358\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2854 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8571\n",
            "Epoch 93: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.3452 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2213 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 94: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8603 - precision: 0.8635 - recall: 0.8687 - val_loss: 0.3446 - val_accuracy: 0.8403 - val_precision: 0.8750 - val_recall: 0.8358\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4173 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 95: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8592 - precision: 0.8617 - recall: 0.8687 - val_loss: 0.3443 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3230 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 96: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8613 - precision: 0.8652 - recall: 0.8687 - val_loss: 0.3449 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4663 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 97: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8624 - precision: 0.8669 - recall: 0.8687 - val_loss: 0.3443 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 98/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4386 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 98: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8634 - precision: 0.8687 - recall: 0.8687 - val_loss: 0.3446 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 99/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4302 - accuracy: 0.7500 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 99: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3439 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 100/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2556 - accuracy: 0.8438 - precision: 0.9167 - recall: 0.7333\n",
            "Epoch 100: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8634 - precision: 0.8687 - recall: 0.8687 - val_loss: 0.3437 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 101/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2557 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9000\n",
            "Epoch 101: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8624 - precision: 0.8669 - recall: 0.8687 - val_loss: 0.3435 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 102/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2264 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 102: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8666 - precision: 0.8740 - recall: 0.8687 - val_loss: 0.3438 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 103/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3887 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 103: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8645 - precision: 0.8704 - recall: 0.8687 - val_loss: 0.3439 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 104/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1970 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 104: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3440 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 105/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3100 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 105: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3437 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 106/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3584 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 106: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8666 - precision: 0.8740 - recall: 0.8687 - val_loss: 0.3434 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 107/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2622 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 107: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8676 - precision: 0.8758 - recall: 0.8687 - val_loss: 0.3433 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 108/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3560 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 108: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3208 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3432 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 109/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4835 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 109: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8697 - precision: 0.8793 - recall: 0.8687 - val_loss: 0.3428 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 110/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1342 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 110: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8655 - precision: 0.8707 - recall: 0.8707 - val_loss: 0.3436 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 111/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2465 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 111: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8666 - precision: 0.8740 - recall: 0.8687 - val_loss: 0.3434 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 112/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3560 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 112: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3433 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 113/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2156 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 113: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3432 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 114/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5343 - accuracy: 0.6875 - precision: 0.7333 - recall: 0.6471\n",
            "Epoch 114: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8666 - precision: 0.8740 - recall: 0.8687 - val_loss: 0.3433 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 115/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4112 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 115: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3186 - accuracy: 0.8697 - precision: 0.8778 - recall: 0.8707 - val_loss: 0.3430 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 116/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3367 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 116: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8676 - precision: 0.8742 - recall: 0.8707 - val_loss: 0.3433 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 117/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3632 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 117: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8697 - precision: 0.8778 - recall: 0.8707 - val_loss: 0.3432 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 118/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2788 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8000\n",
            "Epoch 118: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8739 - precision: 0.8850 - recall: 0.8707 - val_loss: 0.3428 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 119/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4611 - accuracy: 0.7812 - precision: 0.8095 - recall: 0.8500\n",
            "Epoch 119: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8687 - precision: 0.8760 - recall: 0.8707 - val_loss: 0.3429 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 120/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3249 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 120: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8708 - precision: 0.8796 - recall: 0.8707 - val_loss: 0.3430 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 121/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3451 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 121: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8729 - precision: 0.8816 - recall: 0.8727 - val_loss: 0.3427 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 122/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4686 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 122: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8708 - precision: 0.8780 - recall: 0.8727 - val_loss: 0.3432 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 123/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3894 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 123: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8739 - precision: 0.8834 - recall: 0.8727 - val_loss: 0.3426 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 124/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2137 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9000\n",
            "Epoch 124: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8687 - precision: 0.8745 - recall: 0.8727 - val_loss: 0.3431 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 125/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3513 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 125: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8708 - precision: 0.8780 - recall: 0.8727 - val_loss: 0.3427 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 126/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2707 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7895\n",
            "Epoch 126: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8718 - precision: 0.8798 - recall: 0.8727 - val_loss: 0.3430 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 127/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2732 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 127: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3157 - accuracy: 0.8729 - precision: 0.8816 - recall: 0.8727 - val_loss: 0.3429 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 128/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4154 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 128: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8718 - precision: 0.8798 - recall: 0.8727 - val_loss: 0.3430 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 129/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2678 - accuracy: 0.8750 - precision: 0.9231 - recall: 0.8000\n",
            "Epoch 129: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3152 - accuracy: 0.8761 - precision: 0.8855 - recall: 0.8747 - val_loss: 0.3427 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 130/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3385 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 130: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3149 - accuracy: 0.8729 - precision: 0.8801 - recall: 0.8747 - val_loss: 0.3422 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 131/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2209 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8000\n",
            "Epoch 131: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.8718 - precision: 0.8798 - recall: 0.8727 - val_loss: 0.3423 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 132/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2885 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 132: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8729 - precision: 0.8770 - recall: 0.8788 - val_loss: 0.3425 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 133/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5211 - accuracy: 0.7188 - precision: 0.6250 - recall: 0.7692\n",
            "Epoch 133: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8739 - precision: 0.8819 - recall: 0.8747 - val_loss: 0.3426 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 134/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3089 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 134: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8750 - precision: 0.8852 - recall: 0.8727 - val_loss: 0.3428 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 135/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2430 - accuracy: 0.9062 - precision: 0.9048 - recall: 0.9500\n",
            "Epoch 135: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.8739 - precision: 0.8819 - recall: 0.8747 - val_loss: 0.3433 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 136/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2415 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 136: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8729 - precision: 0.8801 - recall: 0.8747 - val_loss: 0.3434 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 137/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2533 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 137: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3132 - accuracy: 0.8739 - precision: 0.8803 - recall: 0.8768 - val_loss: 0.3435 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 138/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4618 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 138: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8771 - precision: 0.8873 - recall: 0.8747 - val_loss: 0.3430 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 139/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4210 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 139: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8750 - precision: 0.8837 - recall: 0.8747 - val_loss: 0.3432 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 140/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2677 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 140: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8761 - precision: 0.8855 - recall: 0.8747 - val_loss: 0.3428 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 141/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2698 - accuracy: 0.8750 - precision: 0.9474 - recall: 0.8571\n",
            "Epoch 141: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8761 - precision: 0.8839 - recall: 0.8768 - val_loss: 0.3435 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 142/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4184 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 142: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3120 - accuracy: 0.8761 - precision: 0.8839 - recall: 0.8768 - val_loss: 0.3436 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 143/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3370 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 143: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8761 - precision: 0.8855 - recall: 0.8747 - val_loss: 0.3431 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 144/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4516 - accuracy: 0.7812 - precision: 0.7895 - recall: 0.8333\n",
            "Epoch 144: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8782 - precision: 0.8828 - recall: 0.8828 - val_loss: 0.3437 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 145/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2270 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 145: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8782 - precision: 0.8859 - recall: 0.8788 - val_loss: 0.3435 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 146/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2927 - accuracy: 0.8750 - precision: 0.7647 - recall: 1.0000\n",
            "Epoch 146: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3111 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3436 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 147/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2921 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 147: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.8792 - precision: 0.8846 - recall: 0.8828 - val_loss: 0.3434 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 148/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2811 - accuracy: 0.8750 - precision: 0.7692 - recall: 0.9091\n",
            "Epoch 148: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8803 - precision: 0.8880 - recall: 0.8808 - val_loss: 0.3424 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 149/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3096 - accuracy: 0.8125 - precision: 0.8889 - recall: 0.8000\n",
            "Epoch 149: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8803 - precision: 0.8818 - recall: 0.8889 - val_loss: 0.3432 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 150/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3148 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 150: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3434 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 150: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4x1\n",
        "- val_accuracy: 0.8361\n",
        "- val_loss: 0.4390\n",
        "- val_precision: 0.8298\n",
        "- val_recall: 0.8864"
      ],
      "metadata": {
        "id": "rHTCDqy7duzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fourx1_model = Sequential(name=\"4x1_model\")\n",
        "fourx1_model.add(Input(shape=(X.shape[1],)))\n",
        "fourx1_model.add(Dense(4, activation='relu'))\n",
        "fourx1_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "fourx1_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "fourx1_history = fourx1_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S-33CxGlde5w",
        "outputId": "855f28cb-600e-4ae4-b87e-488818c1616b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 14s - loss: 0.9306 - accuracy: 0.3438 - precision: 0.8000 - recall: 0.8322\n",
            "Epoch 1: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 1s 9ms/step - loss: 0.8762 - accuracy: 0.4748 - precision: 0.5554 - recall: 0.7488 - val_loss: 0.8039 - val_accuracy: 0.5042 - val_precision: 0.5488 - val_recall: 0.6716\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.8895 - accuracy: 0.4375 - precision: 0.4400 - recall: 0.7333\n",
            "Epoch 2: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7915 - accuracy: 0.5147 - precision: 0.5242 - recall: 0.7232 - val_loss: 0.7373 - val_accuracy: 0.5336 - val_precision: 0.5752 - val_recall: 0.6567\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.8237 - accuracy: 0.4375 - precision: 0.4091 - recall: 0.6429\n",
            "Epoch 3: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7283 - accuracy: 0.5641 - precision: 0.5641 - recall: 0.7111 - val_loss: 0.6859 - val_accuracy: 0.5798 - val_precision: 0.6197 - val_recall: 0.6567\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6198 - accuracy: 0.6562 - precision: 0.5455 - recall: 0.9231\n",
            "Epoch 4: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.6261 - precision: 0.6196 - recall: 0.7273 - val_loss: 0.6430 - val_accuracy: 0.6261 - val_precision: 0.6642 - val_recall: 0.6791\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6740 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.6667\n",
            "Epoch 5: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6355 - accuracy: 0.6712 - precision: 0.6698 - recall: 0.7253 - val_loss: 0.6079 - val_accuracy: 0.6681 - val_precision: 0.7068 - val_recall: 0.7015\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5054 - accuracy: 0.8125 - precision: 0.9286 - recall: 0.7222\n",
            "Epoch 6: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6011 - accuracy: 0.7006 - precision: 0.6996 - recall: 0.7434 - val_loss: 0.5796 - val_accuracy: 0.6765 - val_precision: 0.7244 - val_recall: 0.6866\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5437 - accuracy: 0.7188 - precision: 0.5625 - recall: 0.8182\n",
            "Epoch 7: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5727 - accuracy: 0.7321 - precision: 0.7290 - recall: 0.7717 - val_loss: 0.5559 - val_accuracy: 0.6933 - val_precision: 0.7402 - val_recall: 0.7015\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5265 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 8: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5499 - accuracy: 0.7479 - precision: 0.7485 - recall: 0.7758 - val_loss: 0.5360 - val_accuracy: 0.6975 - val_precision: 0.7422 - val_recall: 0.7090\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4416 - accuracy: 0.8438 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7511 - precision: 0.7529 - recall: 0.7758 - val_loss: 0.5170 - val_accuracy: 0.7143 - val_precision: 0.7538 - val_recall: 0.7313\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5342 - accuracy: 0.6562 - precision: 0.5625 - recall: 0.6923\n",
            "Epoch 10: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.7574 - precision: 0.7629 - recall: 0.7737 - val_loss: 0.4985 - val_accuracy: 0.7437 - val_precision: 0.7874 - val_recall: 0.7463\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6451 - accuracy: 0.6250 - precision: 0.7059 - recall: 0.6316\n",
            "Epoch 11: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.7637 - precision: 0.7711 - recall: 0.7758 - val_loss: 0.4818 - val_accuracy: 0.7521 - val_precision: 0.8000 - val_recall: 0.7463\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5305 - accuracy: 0.6875 - precision: 0.6500 - recall: 0.8125\n",
            "Epoch 12: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4865 - accuracy: 0.7763 - precision: 0.7854 - recall: 0.7838 - val_loss: 0.4661 - val_accuracy: 0.7689 - val_precision: 0.8110 - val_recall: 0.7687\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5160 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.6000\n",
            "Epoch 13: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7847 - precision: 0.7947 - recall: 0.7899 - val_loss: 0.4525 - val_accuracy: 0.7773 - val_precision: 0.8140 - val_recall: 0.7836\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4619 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.7895\n",
            "Epoch 14: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.7952 - precision: 0.8061 - recall: 0.7980 - val_loss: 0.4402 - val_accuracy: 0.7815 - val_precision: 0.8106 - val_recall: 0.7985\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5216 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 15: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.8046 - precision: 0.8134 - recall: 0.8101 - val_loss: 0.4302 - val_accuracy: 0.7773 - val_precision: 0.8092 - val_recall: 0.7910\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4245 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 16: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8067 - precision: 0.8154 - recall: 0.8121 - val_loss: 0.4219 - val_accuracy: 0.7899 - val_precision: 0.8231 - val_recall: 0.7985\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5218 - accuracy: 0.8125 - precision: 0.9500 - recall: 0.7917\n",
            "Epoch 17: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8141 - precision: 0.8206 - recall: 0.8222 - val_loss: 0.4146 - val_accuracy: 0.7941 - val_precision: 0.8346 - val_recall: 0.7910\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3988 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8889\n",
            "Epoch 18: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8193 - precision: 0.8276 - recall: 0.8242 - val_loss: 0.4084 - val_accuracy: 0.7983 - val_precision: 0.8413 - val_recall: 0.7910\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3976 - accuracy: 0.7812 - precision: 0.9231 - recall: 0.6667\n",
            "Epoch 19: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8225 - precision: 0.8327 - recall: 0.8242 - val_loss: 0.4021 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4832 - accuracy: 0.7188 - precision: 0.8235 - recall: 0.7000\n",
            "Epoch 20: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8277 - precision: 0.8343 - recall: 0.8343 - val_loss: 0.3976 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4480 - accuracy: 0.8125 - precision: 0.7619 - recall: 0.9412\n",
            "Epoch 21: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8277 - precision: 0.8371 - recall: 0.8303 - val_loss: 0.3934 - val_accuracy: 0.8109 - val_precision: 0.8560 - val_recall: 0.7985\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4960 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 22: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8330 - precision: 0.8471 - recall: 0.8283 - val_loss: 0.3895 - val_accuracy: 0.8109 - val_precision: 0.8560 - val_recall: 0.7985\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4678 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 23: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8298 - precision: 0.8419 - recall: 0.8283 - val_loss: 0.3861 - val_accuracy: 0.8109 - val_precision: 0.8560 - val_recall: 0.7985\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4872 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 24: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8277 - precision: 0.8384 - recall: 0.8283 - val_loss: 0.3831 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4095 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.9231\n",
            "Epoch 25: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8267 - precision: 0.8354 - recall: 0.8303 - val_loss: 0.3805 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4313 - accuracy: 0.7188 - precision: 0.7333 - recall: 0.6875\n",
            "Epoch 26: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4115 - accuracy: 0.8267 - precision: 0.8340 - recall: 0.8323 - val_loss: 0.3783 - val_accuracy: 0.8151 - val_precision: 0.8571 - val_recall: 0.8060\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2386 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 27: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8288 - precision: 0.8374 - recall: 0.8323 - val_loss: 0.3764 - val_accuracy: 0.8151 - val_precision: 0.8571 - val_recall: 0.8060\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.8125 - precision: 0.7391 - recall: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4074 - accuracy: 0.8288 - precision: 0.8374 - recall: 0.8323 - val_loss: 0.3746 - val_accuracy: 0.8277 - val_precision: 0.8780 - val_recall: 0.8060\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3181 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 29: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4057 - accuracy: 0.8277 - precision: 0.8357 - recall: 0.8323 - val_loss: 0.3730 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5101 - accuracy: 0.7188 - precision: 0.8125 - recall: 0.6842\n",
            "Epoch 30: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8288 - precision: 0.8347 - recall: 0.8364 - val_loss: 0.3716 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3517 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 31: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8298 - precision: 0.8364 - recall: 0.8364 - val_loss: 0.3705 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 32: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8309 - precision: 0.8340 - recall: 0.8424 - val_loss: 0.3695 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5131 - accuracy: 0.7188 - precision: 0.5833 - recall: 0.6364\n",
            "Epoch 33: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3999 - accuracy: 0.8309 - precision: 0.8340 - recall: 0.8424 - val_loss: 0.3686 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4195 - accuracy: 0.8438 - precision: 0.8696 - recall: 0.9091\n",
            "Epoch 34: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8319 - precision: 0.8343 - recall: 0.8444 - val_loss: 0.3678 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2999 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 35: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8319 - precision: 0.8343 - recall: 0.8444 - val_loss: 0.3672 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2584 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9048\n",
            "Epoch 36: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.8309 - precision: 0.8327 - recall: 0.8444 - val_loss: 0.3667 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2255 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 37: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8319 - precision: 0.8330 - recall: 0.8465 - val_loss: 0.3662 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2802 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 38: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8298 - precision: 0.8323 - recall: 0.8424 - val_loss: 0.3655 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2698 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 39: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8288 - precision: 0.8307 - recall: 0.8424 - val_loss: 0.3649 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3132 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 40: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8309 - precision: 0.8327 - recall: 0.8444 - val_loss: 0.3644 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5244 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.7778\n",
            "Epoch 41: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8298 - precision: 0.8310 - recall: 0.8444 - val_loss: 0.3638 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4354 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 42: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3923 - accuracy: 0.8319 - precision: 0.8330 - recall: 0.8465 - val_loss: 0.3633 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3465 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.9375\n",
            "Epoch 43: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8309 - precision: 0.8327 - recall: 0.8444 - val_loss: 0.3629 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2665 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 44: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8309 - precision: 0.8313 - recall: 0.8465 - val_loss: 0.3625 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4290 - accuracy: 0.8125 - precision: 0.7222 - recall: 0.9286\n",
            "Epoch 45: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8351 - precision: 0.8327 - recall: 0.8545 - val_loss: 0.3622 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5627 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.7143\n",
            "Epoch 46: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8340 - precision: 0.8337 - recall: 0.8505 - val_loss: 0.3619 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3389 - accuracy: 0.7812 - precision: 0.8889 - recall: 0.7619\n",
            "Epoch 47: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8319 - precision: 0.8317 - recall: 0.8485 - val_loss: 0.3617 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2991 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 48: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8361 - precision: 0.8356 - recall: 0.8525 - val_loss: 0.3614 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5120 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 49: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8340 - precision: 0.8337 - recall: 0.8505 - val_loss: 0.3612 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5033 - accuracy: 0.7812 - precision: 0.7273 - recall: 0.9412\n",
            "Epoch 50: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8361 - precision: 0.8356 - recall: 0.8525 - val_loss: 0.3610 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3087 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 51: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3865 - accuracy: 0.8361 - precision: 0.8356 - recall: 0.8525 - val_loss: 0.3607 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4924 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 52: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8372 - precision: 0.8360 - recall: 0.8545 - val_loss: 0.3605 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4113 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 53: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8403 - precision: 0.8383 - recall: 0.8586 - val_loss: 0.3604 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 54: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8403 - precision: 0.8383 - recall: 0.8586 - val_loss: 0.3601 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3232 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 55: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8393 - precision: 0.8366 - recall: 0.8586 - val_loss: 0.3602 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3201 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 56: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8414 - precision: 0.8386 - recall: 0.8606 - val_loss: 0.3600 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4718 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 57: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8424 - precision: 0.8402 - recall: 0.8606 - val_loss: 0.3598 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4501 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 58: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.8414 - precision: 0.8399 - recall: 0.8586 - val_loss: 0.3596 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5262 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7895\n",
            "Epoch 59: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.8435 - precision: 0.8406 - recall: 0.8626 - val_loss: 0.3593 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5328 - accuracy: 0.7812 - precision: 0.8182 - recall: 0.6429\n",
            "Epoch 60: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8435 - precision: 0.8406 - recall: 0.8626 - val_loss: 0.3590 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6935 - accuracy: 0.7188 - precision: 0.5789 - recall: 0.9167\n",
            "Epoch 61: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8435 - precision: 0.8419 - recall: 0.8606 - val_loss: 0.3582 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4823 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 62: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8435 - precision: 0.8406 - recall: 0.8626 - val_loss: 0.3580 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4230 - accuracy: 0.7500 - precision: 0.8421 - recall: 0.7619\n",
            "Epoch 63: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8435 - precision: 0.8392 - recall: 0.8646 - val_loss: 0.3576 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3005 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 64: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8456 - precision: 0.8425 - recall: 0.8646 - val_loss: 0.3574 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5283 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 65: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.8445 - precision: 0.8409 - recall: 0.8646 - val_loss: 0.3575 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2230 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 66: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8414 - precision: 0.8359 - recall: 0.8646 - val_loss: 0.3574 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 67: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8435 - precision: 0.8392 - recall: 0.8646 - val_loss: 0.3572 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2552 - accuracy: 0.9375 - precision: 0.9583 - recall: 0.9583\n",
            "Epoch 68: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8445 - precision: 0.8395 - recall: 0.8667 - val_loss: 0.3571 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4928 - accuracy: 0.7188 - precision: 0.6500 - recall: 0.8667\n",
            "Epoch 69: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3793 - accuracy: 0.8414 - precision: 0.8346 - recall: 0.8667 - val_loss: 0.3570 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4431 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 70: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8414 - precision: 0.8346 - recall: 0.8667 - val_loss: 0.3569 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4105 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.9474\n",
            "Epoch 71: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8414 - precision: 0.8346 - recall: 0.8667 - val_loss: 0.3568 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4053 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 72: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8414 - precision: 0.8346 - recall: 0.8667 - val_loss: 0.3569 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3366 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 73: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8403 - precision: 0.8343 - recall: 0.8646 - val_loss: 0.3569 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4485 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 74: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8414 - precision: 0.8359 - recall: 0.8646 - val_loss: 0.3568 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1612 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 75: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 0.8435 - precision: 0.8379 - recall: 0.8667 - val_loss: 0.3570 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5647 - accuracy: 0.8125 - precision: 0.7368 - recall: 0.9333\n",
            "Epoch 76: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8435 - precision: 0.8366 - recall: 0.8687 - val_loss: 0.3569 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2649 - accuracy: 0.8750 - precision: 0.7273 - recall: 0.8889\n",
            "Epoch 77: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8456 - precision: 0.8385 - recall: 0.8707 - val_loss: 0.3567 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3524 - accuracy: 0.8438 - precision: 0.7333 - recall: 0.9167\n",
            "Epoch 78: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8445 - precision: 0.8382 - recall: 0.8687 - val_loss: 0.3563 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4939 - accuracy: 0.6562 - precision: 0.6087 - recall: 0.8750\n",
            "Epoch 79: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8445 - precision: 0.8369 - recall: 0.8707 - val_loss: 0.3564 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4643 - accuracy: 0.7812 - precision: 0.7619 - recall: 0.8889\n",
            "Epoch 80: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8456 - precision: 0.8385 - recall: 0.8707 - val_loss: 0.3567 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3721 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 81: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8456 - precision: 0.8372 - recall: 0.8727 - val_loss: 0.3568 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3417 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 82: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8466 - precision: 0.8388 - recall: 0.8727 - val_loss: 0.3571 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4390 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 83: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8445 - precision: 0.8369 - recall: 0.8707 - val_loss: 0.3570 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4439 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8889\n",
            "Epoch 84: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8445 - precision: 0.8369 - recall: 0.8707 - val_loss: 0.3572 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3096 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 85: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8456 - precision: 0.8385 - recall: 0.8707 - val_loss: 0.3572 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2999 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 86: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8466 - precision: 0.8402 - recall: 0.8707 - val_loss: 0.3575 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3645 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 87: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8477 - precision: 0.8418 - recall: 0.8707 - val_loss: 0.3573 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3924 - accuracy: 0.8125 - precision: 0.6667 - recall: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8456 - precision: 0.8398 - recall: 0.8687 - val_loss: 0.3573 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2809 - accuracy: 0.8750 - precision: 0.9091 - recall: 0.9091\n",
            "Epoch 89: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.8466 - precision: 0.8402 - recall: 0.8707 - val_loss: 0.3574 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3072 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9091\n",
            "Epoch 90: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8445 - precision: 0.8382 - recall: 0.8687 - val_loss: 0.3575 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4952 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 91: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8498 - precision: 0.8451 - recall: 0.8707 - val_loss: 0.3571 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2435 - accuracy: 0.9062 - precision: 0.8667 - recall: 0.9286\n",
            "Epoch 92: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8477 - precision: 0.8445 - recall: 0.8667 - val_loss: 0.3569 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3992 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 93: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8487 - precision: 0.8421 - recall: 0.8727 - val_loss: 0.3572 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3619 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 94: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8456 - precision: 0.8425 - recall: 0.8646 - val_loss: 0.3575 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3685 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.8421\n",
            "Epoch 95: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8477 - precision: 0.8445 - recall: 0.8667 - val_loss: 0.3576 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2348 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8333\n",
            "Epoch 96: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8445 - precision: 0.8422 - recall: 0.8626 - val_loss: 0.3574 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3714 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 97: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8456 - precision: 0.8425 - recall: 0.8646 - val_loss: 0.3573 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 98/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4333 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 98: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8456 - precision: 0.8425 - recall: 0.8646 - val_loss: 0.3577 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 98: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2x1\n",
        "- val_accuracy: 0.8277\n",
        "- val_loss: 0.4333\n",
        "- val_precision: 0.8273\n",
        "- val_recall: 0.8712"
      ],
      "metadata": {
        "id": "oRS6lBE4egF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twox1_model = Sequential(name=\"2x1_model\")\n",
        "twox1_model.add(Input(shape=(X.shape[1],)))\n",
        "twox1_model.add(Dense(2, activation='relu'))\n",
        "twox1_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "twox1_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "twox1_history = twox1_model.fit(XTRAIN, YTRAIN, epochs=256, verbose=1, validation_data=(XVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6BnkkMhfeVud",
        "outputId": "eded8572-92fa-45cd-fa0c-5cccef458895"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            " 1/30 [>.............................] - ETA: 14s - loss: 0.8907 - accuracy: 0.3750 - precision: 0.8357 - recall: 0.7748\n",
            "Epoch 1: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 1s 9ms/step - loss: 0.7491 - accuracy: 0.5105 - precision: 0.6071 - recall: 0.5676 - val_loss: 0.7226 - val_accuracy: 0.5000 - val_precision: 0.5641 - val_recall: 0.4925\n",
            "Epoch 2/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6079 - accuracy: 0.5625 - precision: 0.6000 - recall: 0.5294\n",
            "Epoch 2: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7143 - accuracy: 0.5441 - precision: 0.5688 - recall: 0.5091 - val_loss: 0.6932 - val_accuracy: 0.5378 - val_precision: 0.6071 - val_recall: 0.5075\n",
            "Epoch 3/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7487 - accuracy: 0.5000 - precision: 0.5882 - recall: 0.5263\n",
            "Epoch 3: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.5798 - precision: 0.6123 - recall: 0.5232 - val_loss: 0.6658 - val_accuracy: 0.5714 - val_precision: 0.6404 - val_recall: 0.5448\n",
            "Epoch 4/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6094 - accuracy: 0.6250 - precision: 0.7333 - recall: 0.5789\n",
            "Epoch 4: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6602 - accuracy: 0.6197 - precision: 0.6626 - recall: 0.5475 - val_loss: 0.6415 - val_accuracy: 0.6050 - val_precision: 0.6818 - val_recall: 0.5597\n",
            "Epoch 5/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7400 - accuracy: 0.5938 - precision: 0.4545 - recall: 0.4167\n",
            "Epoch 5: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6481 - precision: 0.6951 - recall: 0.5758 - val_loss: 0.6202 - val_accuracy: 0.6471 - val_precision: 0.7315 - val_recall: 0.5896\n",
            "Epoch 6/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5855 - accuracy: 0.7812 - precision: 0.6471 - recall: 0.9167\n",
            "Epoch 6: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6155 - accuracy: 0.6807 - precision: 0.7324 - recall: 0.6081 - val_loss: 0.5989 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 7/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6262 - accuracy: 0.5625 - precision: 0.7500 - recall: 0.4500\n",
            "Epoch 7: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.7132 - precision: 0.7748 - recall: 0.6323 - val_loss: 0.5790 - val_accuracy: 0.7059 - val_precision: 0.8019 - val_recall: 0.6343\n",
            "Epoch 8/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5670 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 8: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.7300 - precision: 0.8005 - recall: 0.6404 - val_loss: 0.5604 - val_accuracy: 0.7353 - val_precision: 0.8257 - val_recall: 0.6716\n",
            "Epoch 9/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5471 - accuracy: 0.6875 - precision: 0.8000 - recall: 0.5000\n",
            "Epoch 9: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.7479 - precision: 0.8164 - recall: 0.6646 - val_loss: 0.5408 - val_accuracy: 0.7479 - val_precision: 0.8304 - val_recall: 0.6940\n",
            "Epoch 10/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6426 - accuracy: 0.7812 - precision: 0.6875 - recall: 0.8462\n",
            "Epoch 10: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7637 - precision: 0.8230 - recall: 0.6949 - val_loss: 0.5237 - val_accuracy: 0.7395 - val_precision: 0.8214 - val_recall: 0.6866\n",
            "Epoch 11/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5635 - accuracy: 0.7812 - precision: 0.7619 - recall: 0.8889\n",
            "Epoch 11: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5151 - accuracy: 0.7721 - precision: 0.8278 - recall: 0.7091 - val_loss: 0.5067 - val_accuracy: 0.7479 - val_precision: 0.8304 - val_recall: 0.6940\n",
            "Epoch 12/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4913 - accuracy: 0.7500 - precision: 0.8235 - recall: 0.7368\n",
            "Epoch 12: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7805 - precision: 0.8326 - recall: 0.7232 - val_loss: 0.4916 - val_accuracy: 0.7563 - val_precision: 0.8393 - val_recall: 0.7015\n",
            "Epoch 13/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5130 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 13: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.7899 - precision: 0.8422 - recall: 0.7333 - val_loss: 0.4785 - val_accuracy: 0.7605 - val_precision: 0.8407 - val_recall: 0.7090\n",
            "Epoch 14/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5458 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7059\n",
            "Epoch 14: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.7983 - precision: 0.8531 - recall: 0.7394 - val_loss: 0.4675 - val_accuracy: 0.7647 - val_precision: 0.8421 - val_recall: 0.7164\n",
            "Epoch 15/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4180 - accuracy: 0.7812 - precision: 0.9333 - recall: 0.7000\n",
            "Epoch 15: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.8046 - precision: 0.8568 - recall: 0.7495 - val_loss: 0.4581 - val_accuracy: 0.7689 - val_precision: 0.8496 - val_recall: 0.7164\n",
            "Epoch 16/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3952 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.8462\n",
            "Epoch 16: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.8078 - precision: 0.8578 - recall: 0.7556 - val_loss: 0.4494 - val_accuracy: 0.7773 - val_precision: 0.8522 - val_recall: 0.7313\n",
            "Epoch 17/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3426 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 17: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8120 - precision: 0.8624 - recall: 0.7596 - val_loss: 0.4427 - val_accuracy: 0.7857 - val_precision: 0.8547 - val_recall: 0.7463\n",
            "Epoch 18/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3975 - accuracy: 0.8125 - precision: 1.0000 - recall: 0.7000\n",
            "Epoch 18: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.8120 - precision: 0.8607 - recall: 0.7616 - val_loss: 0.4368 - val_accuracy: 0.7899 - val_precision: 0.8559 - val_recall: 0.7537\n",
            "Epoch 19/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3407 - accuracy: 0.8750 - precision: 0.9375 - recall: 0.8333\n",
            "Epoch 19: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.8109 - precision: 0.8588 - recall: 0.7616 - val_loss: 0.4313 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 20/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4090 - accuracy: 0.7812 - precision: 0.8750 - recall: 0.7368\n",
            "Epoch 20: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8109 - precision: 0.8571 - recall: 0.7636 - val_loss: 0.4266 - val_accuracy: 0.8025 - val_precision: 0.8595 - val_recall: 0.7761\n",
            "Epoch 21/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3605 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.8333\n",
            "Epoch 21: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4215 - accuracy: 0.8088 - precision: 0.8533 - recall: 0.7636 - val_loss: 0.4227 - val_accuracy: 0.7941 - val_precision: 0.8512 - val_recall: 0.7687\n",
            "Epoch 22/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3722 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 22: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4177 - accuracy: 0.8130 - precision: 0.8546 - recall: 0.7717 - val_loss: 0.4191 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 23/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3522 - accuracy: 0.7500 - precision: 0.9286 - recall: 0.6500\n",
            "Epoch 23: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4145 - accuracy: 0.8130 - precision: 0.8530 - recall: 0.7737 - val_loss: 0.4159 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 24/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3520 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.7059\n",
            "Epoch 24: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8162 - precision: 0.8524 - recall: 0.7818 - val_loss: 0.4132 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 25/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3923 - accuracy: 0.8125 - precision: 0.9444 - recall: 0.7727\n",
            "Epoch 25: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8172 - precision: 0.8527 - recall: 0.7838 - val_loss: 0.4110 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 26/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5689 - accuracy: 0.7188 - precision: 0.7647 - recall: 0.7222\n",
            "Epoch 26: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4076 - accuracy: 0.8162 - precision: 0.8524 - recall: 0.7818 - val_loss: 0.4090 - val_accuracy: 0.7899 - val_precision: 0.8559 - val_recall: 0.7537\n",
            "Epoch 27/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4703 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 27: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4060 - accuracy: 0.8151 - precision: 0.8521 - recall: 0.7798 - val_loss: 0.4072 - val_accuracy: 0.7899 - val_precision: 0.8559 - val_recall: 0.7537\n",
            "Epoch 28/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.7812 - precision: 0.6364 - recall: 0.7000\n",
            "Epoch 28: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8151 - precision: 0.8537 - recall: 0.7778 - val_loss: 0.4058 - val_accuracy: 0.7899 - val_precision: 0.8559 - val_recall: 0.7537\n",
            "Epoch 29/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4977 - accuracy: 0.7500 - precision: 0.7727 - recall: 0.8500\n",
            "Epoch 29: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4036 - accuracy: 0.8130 - precision: 0.8530 - recall: 0.7737 - val_loss: 0.4043 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 30/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4238 - accuracy: 0.7500 - precision: 0.8333 - recall: 0.7500\n",
            "Epoch 30: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8130 - precision: 0.8499 - recall: 0.7778 - val_loss: 0.4031 - val_accuracy: 0.7941 - val_precision: 0.8571 - val_recall: 0.7612\n",
            "Epoch 31/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4689 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 31: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8151 - precision: 0.8505 - recall: 0.7818 - val_loss: 0.4019 - val_accuracy: 0.8067 - val_precision: 0.8729 - val_recall: 0.7687\n",
            "Epoch 32/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4941 - accuracy: 0.8125 - precision: 0.8182 - recall: 0.6923\n",
            "Epoch 32: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8162 - precision: 0.8509 - recall: 0.7838 - val_loss: 0.4007 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 33/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4189 - accuracy: 0.7812 - precision: 0.8947 - recall: 0.7727\n",
            "Epoch 33: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3998 - accuracy: 0.8172 - precision: 0.8512 - recall: 0.7859 - val_loss: 0.3997 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 34/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3346 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 34: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8183 - precision: 0.8515 - recall: 0.7879 - val_loss: 0.3987 - val_accuracy: 0.8109 - val_precision: 0.8739 - val_recall: 0.7761\n",
            "Epoch 35/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4986 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 35: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8193 - precision: 0.8519 - recall: 0.7899 - val_loss: 0.3976 - val_accuracy: 0.8151 - val_precision: 0.8750 - val_recall: 0.7836\n",
            "Epoch 36/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3737 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 36: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3976 - accuracy: 0.8204 - precision: 0.8522 - recall: 0.7919 - val_loss: 0.3968 - val_accuracy: 0.8151 - val_precision: 0.8750 - val_recall: 0.7836\n",
            "Epoch 37/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4055 - accuracy: 0.8125 - precision: 0.9167 - recall: 0.6875\n",
            "Epoch 37: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.8204 - precision: 0.8522 - recall: 0.7919 - val_loss: 0.3958 - val_accuracy: 0.8151 - val_precision: 0.8750 - val_recall: 0.7836\n",
            "Epoch 38/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3430 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 38: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8214 - precision: 0.8525 - recall: 0.7939 - val_loss: 0.3950 - val_accuracy: 0.8193 - val_precision: 0.8824 - val_recall: 0.7836\n",
            "Epoch 39/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.7188 - precision: 0.8000 - recall: 0.6667\n",
            "Epoch 39: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8225 - precision: 0.8543 - recall: 0.7939 - val_loss: 0.3941 - val_accuracy: 0.8193 - val_precision: 0.8824 - val_recall: 0.7836\n",
            "Epoch 40/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2642 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 40: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8256 - precision: 0.8553 - recall: 0.8000 - val_loss: 0.3934 - val_accuracy: 0.8193 - val_precision: 0.8824 - val_recall: 0.7836\n",
            "Epoch 41/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3356 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 41: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8235 - precision: 0.8547 - recall: 0.7960 - val_loss: 0.3927 - val_accuracy: 0.8193 - val_precision: 0.8824 - val_recall: 0.7836\n",
            "Epoch 42/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4002 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 42: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8235 - precision: 0.8547 - recall: 0.7960 - val_loss: 0.3920 - val_accuracy: 0.8193 - val_precision: 0.8824 - val_recall: 0.7836\n",
            "Epoch 43/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3066 - accuracy: 0.9062 - precision: 0.9524 - recall: 0.9091\n",
            "Epoch 43: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8235 - precision: 0.8547 - recall: 0.7960 - val_loss: 0.3913 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 44/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6409 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 44: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8214 - precision: 0.8510 - recall: 0.7960 - val_loss: 0.3905 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 45/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3920 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 45: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3921 - accuracy: 0.8214 - precision: 0.8510 - recall: 0.7960 - val_loss: 0.3899 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 46/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3352 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.7500\n",
            "Epoch 46: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8225 - precision: 0.8528 - recall: 0.7960 - val_loss: 0.3892 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 47/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4638 - accuracy: 0.7500 - precision: 0.8667 - recall: 0.6842\n",
            "Epoch 47: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8246 - precision: 0.8534 - recall: 0.8000 - val_loss: 0.3885 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 48/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3534 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 48: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8235 - precision: 0.8531 - recall: 0.7980 - val_loss: 0.3875 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 49/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5737 - accuracy: 0.6562 - precision: 0.8571 - recall: 0.5714\n",
            "Epoch 49: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8235 - precision: 0.8531 - recall: 0.7980 - val_loss: 0.3866 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 50/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2730 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 50: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8235 - precision: 0.8531 - recall: 0.7980 - val_loss: 0.3858 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 51/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5401 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 51: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3883 - accuracy: 0.8256 - precision: 0.8538 - recall: 0.8020 - val_loss: 0.3849 - val_accuracy: 0.8235 - val_precision: 0.8833 - val_recall: 0.7910\n",
            "Epoch 52/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4103 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 52: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8309 - precision: 0.8553 - recall: 0.8121 - val_loss: 0.3840 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 53/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3812 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 53: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3872 - accuracy: 0.8288 - precision: 0.8517 - recall: 0.8121 - val_loss: 0.3833 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 54/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3332 - accuracy: 0.8125 - precision: 0.7692 - recall: 0.7692\n",
            "Epoch 54: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8298 - precision: 0.8520 - recall: 0.8141 - val_loss: 0.3827 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 55/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2544 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 55: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8298 - precision: 0.8520 - recall: 0.8141 - val_loss: 0.3819 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 56/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2778 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.8462\n",
            "Epoch 56: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3857 - accuracy: 0.8298 - precision: 0.8491 - recall: 0.8182 - val_loss: 0.3815 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 57/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3437 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 57: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8309 - precision: 0.8523 - recall: 0.8162 - val_loss: 0.3807 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 58/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4928 - accuracy: 0.8438 - precision: 0.8667 - recall: 0.8125\n",
            "Epoch 58: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8288 - precision: 0.8487 - recall: 0.8162 - val_loss: 0.3802 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 59/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3910 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 59: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3842 - accuracy: 0.8298 - precision: 0.8491 - recall: 0.8182 - val_loss: 0.3797 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 60/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5847 - accuracy: 0.7812 - precision: 0.9167 - recall: 0.6471\n",
            "Epoch 60: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8319 - precision: 0.8526 - recall: 0.8182 - val_loss: 0.3794 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 61/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3691 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 61: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8298 - precision: 0.8505 - recall: 0.8162 - val_loss: 0.3790 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 62/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5135 - accuracy: 0.7812 - precision: 0.9231 - recall: 0.6667\n",
            "Epoch 62: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3832 - accuracy: 0.8319 - precision: 0.8526 - recall: 0.8182 - val_loss: 0.3785 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 63/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3441 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 63: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.8309 - precision: 0.8508 - recall: 0.8182 - val_loss: 0.3782 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 64/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2496 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 64: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8298 - precision: 0.8505 - recall: 0.8162 - val_loss: 0.3777 - val_accuracy: 0.8319 - val_precision: 0.8852 - val_recall: 0.8060\n",
            "Epoch 65/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3056 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 65: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8330 - precision: 0.8360 - recall: 0.8444 - val_loss: 0.3776 - val_accuracy: 0.8403 - val_precision: 0.9000 - val_recall: 0.8060\n",
            "Epoch 66/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1574 - accuracy: 0.9688 - precision: 0.9474 - recall: 1.0000\n",
            "Epoch 66: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8340 - precision: 0.8377 - recall: 0.8444 - val_loss: 0.3774 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 67/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2497 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 67: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8372 - precision: 0.8360 - recall: 0.8545 - val_loss: 0.3770 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 68/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2894 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 68: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3817 - accuracy: 0.8382 - precision: 0.8363 - recall: 0.8566 - val_loss: 0.3767 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 69/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4752 - accuracy: 0.8125 - precision: 0.9333 - recall: 0.7368\n",
            "Epoch 69: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8403 - precision: 0.8383 - recall: 0.8586 - val_loss: 0.3764 - val_accuracy: 0.8319 - val_precision: 0.8672 - val_recall: 0.8284\n",
            "Epoch 70/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3950 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 70: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8382 - precision: 0.8350 - recall: 0.8586 - val_loss: 0.3759 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 71/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2204 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 71: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8393 - precision: 0.8366 - recall: 0.8586 - val_loss: 0.3756 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 72/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3684 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 72: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8372 - precision: 0.8333 - recall: 0.8586 - val_loss: 0.3755 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 73/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4943 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 73: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8382 - precision: 0.8350 - recall: 0.8586 - val_loss: 0.3751 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 74/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2787 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 74: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8382 - precision: 0.8350 - recall: 0.8586 - val_loss: 0.3748 - val_accuracy: 0.8277 - val_precision: 0.8605 - val_recall: 0.8284\n",
            "Epoch 75/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3867 - accuracy: 0.9062 - precision: 0.8125 - recall: 1.0000\n",
            "Epoch 75: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8372 - precision: 0.8333 - recall: 0.8586 - val_loss: 0.3746 - val_accuracy: 0.8361 - val_precision: 0.8626 - val_recall: 0.8433\n",
            "Epoch 76/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3387 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 76: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8372 - precision: 0.8320 - recall: 0.8606 - val_loss: 0.3745 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 77/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2709 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 77: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8372 - precision: 0.8320 - recall: 0.8606 - val_loss: 0.3744 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 78/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5773 - accuracy: 0.6562 - precision: 0.8235 - recall: 0.6364\n",
            "Epoch 78: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3793 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3744 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 79/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4357 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 79: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3741 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 80/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4318 - accuracy: 0.8125 - precision: 0.9333 - recall: 0.7368\n",
            "Epoch 80: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8361 - precision: 0.8304 - recall: 0.8606 - val_loss: 0.3741 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 81/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2199 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 81: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3740 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 82/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3401 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 82: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3740 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 83/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4451 - accuracy: 0.7812 - precision: 0.7895 - recall: 0.8333\n",
            "Epoch 83: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3738 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 84/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2373 - accuracy: 0.8750 - precision: 0.9545 - recall: 0.8750\n",
            "Epoch 84: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3737 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 85/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3869 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.8750\n",
            "Epoch 85: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3736 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 86/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3266 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 86: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8382 - precision: 0.8337 - recall: 0.8606 - val_loss: 0.3735 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 87/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3011 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.7273\n",
            "Epoch 87: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8372 - precision: 0.8320 - recall: 0.8606 - val_loss: 0.3734 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 88/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3156 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 88: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8372 - precision: 0.8320 - recall: 0.8606 - val_loss: 0.3731 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 89/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2976 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 89: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8372 - precision: 0.8320 - recall: 0.8606 - val_loss: 0.3729 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 90/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4416 - accuracy: 0.8125 - precision: 0.6471 - recall: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8361 - precision: 0.8304 - recall: 0.8606 - val_loss: 0.3725 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 91/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5956 - accuracy: 0.7188 - precision: 0.7333 - recall: 0.6875\n",
            "Epoch 91: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8361 - precision: 0.8304 - recall: 0.8606 - val_loss: 0.3724 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 92/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3675 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 92: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8361 - precision: 0.8304 - recall: 0.8606 - val_loss: 0.3723 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 93/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3575 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 93: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8372 - precision: 0.8307 - recall: 0.8626 - val_loss: 0.3718 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 94/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3990 - accuracy: 0.8125 - precision: 0.8421 - recall: 0.8421\n",
            "Epoch 94: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.8382 - precision: 0.8311 - recall: 0.8646 - val_loss: 0.3717 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 95/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4253 - accuracy: 0.7812 - precision: 0.8824 - recall: 0.7500\n",
            "Epoch 95: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8382 - precision: 0.8311 - recall: 0.8646 - val_loss: 0.3715 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 96/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5097 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 96: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8382 - precision: 0.8311 - recall: 0.8646 - val_loss: 0.3713 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 97/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5848 - accuracy: 0.7812 - precision: 0.7222 - recall: 0.8667\n",
            "Epoch 97: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3712 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 98/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4512 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 98: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3709 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 99/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3558 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 99: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8382 - precision: 0.8311 - recall: 0.8646 - val_loss: 0.3708 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 100/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2640 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 100: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3706 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 101/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1247 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 101: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3705 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 102/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4070 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 102: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3704 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 103/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4745 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 103: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8403 - precision: 0.8330 - recall: 0.8667 - val_loss: 0.3702 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 104/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1927 - accuracy: 0.9375 - precision: 0.9000 - recall: 1.0000\n",
            "Epoch 104: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8403 - precision: 0.8330 - recall: 0.8667 - val_loss: 0.3699 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 105/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3024 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 105: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.8414 - precision: 0.8333 - recall: 0.8687 - val_loss: 0.3697 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 106/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3464 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 106: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.8414 - precision: 0.8333 - recall: 0.8687 - val_loss: 0.3696 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 107/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4849 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 107: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8414 - precision: 0.8333 - recall: 0.8687 - val_loss: 0.3697 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 108/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4582 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 108: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8403 - precision: 0.8330 - recall: 0.8667 - val_loss: 0.3697 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 109/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4671 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 109: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8414 - precision: 0.8333 - recall: 0.8687 - val_loss: 0.3696 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 110/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4719 - accuracy: 0.7188 - precision: 0.6875 - recall: 0.7333\n",
            "Epoch 110: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8393 - precision: 0.8327 - recall: 0.8646 - val_loss: 0.3695 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 111/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 111: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8393 - precision: 0.8314 - recall: 0.8667 - val_loss: 0.3696 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 112/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4381 - accuracy: 0.8750 - precision: 0.8636 - recall: 0.9500\n",
            "Epoch 112: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.8372 - precision: 0.8307 - recall: 0.8626 - val_loss: 0.3696 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 113/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3196 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 113: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8372 - precision: 0.8307 - recall: 0.8626 - val_loss: 0.3694 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 114/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3411 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.9565\n",
            "Epoch 114: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8372 - precision: 0.8307 - recall: 0.8626 - val_loss: 0.3693 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 115/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2154 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 115: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8351 - precision: 0.8275 - recall: 0.8626 - val_loss: 0.3693 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 116/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6819 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.7778\n",
            "Epoch 116: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8382 - precision: 0.8285 - recall: 0.8687 - val_loss: 0.3688 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 117/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3486 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 117: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8382 - precision: 0.8285 - recall: 0.8687 - val_loss: 0.3687 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 118/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6680 - accuracy: 0.6875 - precision: 0.6250 - recall: 0.7143\n",
            "Epoch 118: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8393 - precision: 0.8288 - recall: 0.8707 - val_loss: 0.3686 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 119/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4277 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 119: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8393 - precision: 0.8288 - recall: 0.8707 - val_loss: 0.3685 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 120/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4325 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.6667\n",
            "Epoch 120: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8393 - precision: 0.8288 - recall: 0.8707 - val_loss: 0.3686 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 121/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2630 - accuracy: 0.9375 - precision: 0.8824 - recall: 1.0000\n",
            "Epoch 121: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8393 - precision: 0.8288 - recall: 0.8707 - val_loss: 0.3687 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 122/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3402 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 122: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8414 - precision: 0.8295 - recall: 0.8747 - val_loss: 0.3687 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 123/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4212 - accuracy: 0.7188 - precision: 0.6000 - recall: 0.7500\n",
            "Epoch 123: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3698 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.3685 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 124/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3433 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 124: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8435 - precision: 0.8302 - recall: 0.8788 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 125/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4732 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 125: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3696 - accuracy: 0.8445 - precision: 0.8305 - recall: 0.8808 - val_loss: 0.3683 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 126/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1843 - accuracy: 0.9062 - precision: 0.8667 - recall: 0.9286\n",
            "Epoch 126: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8435 - precision: 0.8302 - recall: 0.8788 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 127/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4104 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 127: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3683 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 128/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4086 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 128: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8456 - precision: 0.8308 - recall: 0.8828 - val_loss: 0.3683 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 129/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3379 - accuracy: 0.7812 - precision: 0.7059 - recall: 0.8571\n",
            "Epoch 129: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.3685 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 130/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2347 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 130: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 131/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1848 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 131: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 132/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3949 - accuracy: 0.8438 - precision: 0.8500 - recall: 0.8947\n",
            "Epoch 132: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 133/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3874 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 133: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8466 - precision: 0.8324 - recall: 0.8828 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 134/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3598 - accuracy: 0.8438 - precision: 0.7619 - recall: 1.0000\n",
            "Epoch 134: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 135/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2249 - accuracy: 0.8750 - precision: 0.9091 - recall: 0.9091\n",
            "Epoch 135: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8456 - precision: 0.8321 - recall: 0.8808 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 136/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2684 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 136: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8466 - precision: 0.8324 - recall: 0.8828 - val_loss: 0.3685 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 137/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4663 - accuracy: 0.8438 - precision: 0.7059 - recall: 1.0000\n",
            "Epoch 137: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8477 - precision: 0.8327 - recall: 0.8848 - val_loss: 0.3683 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 138/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1542 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 138: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8477 - precision: 0.8327 - recall: 0.8848 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 139/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4417 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.9231\n",
            "Epoch 139: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8466 - precision: 0.8324 - recall: 0.8828 - val_loss: 0.3682 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 140/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3632 - accuracy: 0.8125 - precision: 0.6667 - recall: 1.0000\n",
            "Epoch 140: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8477 - precision: 0.8327 - recall: 0.8848 - val_loss: 0.3683 - val_accuracy: 0.8361 - val_precision: 0.8519 - val_recall: 0.8582\n",
            "Epoch 141/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2354 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 141: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8466 - precision: 0.8311 - recall: 0.8848 - val_loss: 0.3681 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 142/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3887 - accuracy: 0.8125 - precision: 0.8261 - recall: 0.9048\n",
            "Epoch 142: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 143/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5366 - accuracy: 0.6875 - precision: 0.6250 - recall: 0.7143\n",
            "Epoch 143: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8445 - precision: 0.8292 - recall: 0.8828 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 144/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3006 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 144: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 145/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4038 - accuracy: 0.8438 - precision: 0.7692 - recall: 0.8333\n",
            "Epoch 145: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 146/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4190 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 146: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 147/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3439 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 147: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3679 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 148/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4024 - accuracy: 0.8125 - precision: 0.6471 - recall: 1.0000\n",
            "Epoch 148: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8445 - precision: 0.8292 - recall: 0.8828 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 149/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5122 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 149: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3676 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 150/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4188 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 150: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8435 - precision: 0.8277 - recall: 0.8828 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 151/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3037 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 151: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3674 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 152/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3170 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 152: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3673 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 153/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2586 - accuracy: 0.8750 - precision: 0.9333 - recall: 0.8235\n",
            "Epoch 153: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3673 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3676 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 154/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3649 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 154: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3673 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3676 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 155/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3290 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 155: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 156/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6439 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.8000\n",
            "Epoch 156: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 157/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5542 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.7692\n",
            "Epoch 157: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 158/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2775 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 158: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 159/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2844 - accuracy: 0.9062 - precision: 0.8235 - recall: 1.0000\n",
            "Epoch 159: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3679 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 160/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5128 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.8333\n",
            "Epoch 160: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3678 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 161/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2955 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 161: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 0.8456 - precision: 0.8283 - recall: 0.8869 - val_loss: 0.3677 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 162/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2205 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 162: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3679 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 163/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4234 - accuracy: 0.8125 - precision: 0.9091 - recall: 0.8333\n",
            "Epoch 163: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3682 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 164/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3415 - accuracy: 0.8125 - precision: 0.7143 - recall: 1.0000\n",
            "Epoch 164: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3680 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 165/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3891 - accuracy: 0.8125 - precision: 0.9474 - recall: 0.7826\n",
            "Epoch 165: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3682 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 166/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4464 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 166: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3681 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 167/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3567 - accuracy: 0.8438 - precision: 0.9130 - recall: 0.8750\n",
            "Epoch 167: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3682 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 168/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3383 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 168: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3681 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 169/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3961 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.9000\n",
            "Epoch 169: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3683 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 170/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3087 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 170: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3682 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 171/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3071 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 171: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3684 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 172/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3425 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 172: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8445 - precision: 0.8280 - recall: 0.8848 - val_loss: 0.3684 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 173/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8438 - precision: 0.8500 - recall: 0.8947\n",
            "Epoch 173: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3684 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 174/256\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4578 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 174: val_loss did not improve from 0.28055\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.8456 - precision: 0.8295 - recall: 0.8848 - val_loss: 0.3683 - val_accuracy: 0.8319 - val_precision: 0.8456 - val_recall: 0.8582\n",
            "Epoch 174: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twox1_history.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCprC7940Pr8",
        "outputId": "aa4ad048-77de-4285-f15d-346390cbd606"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'verbose': 1, 'epochs': 256, 'steps': 30}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot learning curves"
      ],
      "metadata": {
        "id": "0-ec9JYA7Ttt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(history, title):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title(title)\n",
        "  plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oUdiIetp1bNV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history, \"baseline regression model\")\n",
        "plot_learning_curves(all_eights_history, \"8-8-8-1\")\n",
        "plot_learning_curves(three_layer_history, \"16-8-1 model\")\n",
        "plot_learning_curves(eightx1_history, \"8-1 model\")\n",
        "plot_learning_curves(eightxfourxone_history, \"8-4-1\")\n",
        "plot_learning_curves(fourx1_history, \"4-1 model\")\n",
        "plot_learning_curves(twox1_history, \"2-1 model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4TcF6yNq1_nP",
        "outputId": "f6423250-bb33-476f-c195-adf571087ca0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACLF0lEQVR4nOzdd3iTVfvA8W/SvUt3C4WWvQsUqIAM2UMEnAxZiiiC40VeFUVR9CdOxAniK0NFQBRwsEGGzELZq2wKdNFCN13J8/vjadKGDtrSJtDen+vK1eSZ50naPnfOuc85GkVRFIQQQgghqhGtpQsghBBCCGFuEgAJIYQQotqRAEgIIYQQ1Y4EQEIIIYSodiQAEkIIIUS1IwGQEEIIIaodCYCEEEIIUe1IACSEEEKIakcCICGEEEJUOxIACSHEXSIoKIgxY8aUa1+NRsM777xToeURoiqTAEgIUay0tDSmT59O37598fDwQKPRsHDhwmK31+v1zJkzh1atWuHg4ICnpyfdu3fn8OHDpTrfmTNnGDp0KLVq1cLR0ZHGjRszY8YMMjIySrX/smXLePLJJ2nQoAEajYZu3bqVaj8hRPVjbekCCCHuXgkJCcyYMYPatWsTEhLC1q1bS9z+qaeeYvHixYwaNYpJkyaRnp7OwYMHiY+Pv+25Ll++TPv27XFzc2PSpEl4eHiwe/dupk+fTkREBH/88cdtjzFnzhwiIiJo164diYmJpb1MIUQ1JAGQEKJY/v7+xMTE4Ofnx/79+2nXrl2x2/76668sWrSIFStWMGTIkDKf66effiIpKYkdO3bQrFkzAMaPH49er+fHH3/kxo0b1KhR47bHqFmzJlqtlubNm5e5DEKI6kOawIQQxbKzs8PPz69U286aNYv27dszZMgQ9Ho96enpZTpXSkoKAL6+vibL/f390Wq12Nra3vYYgYGBaLXl+7e2detWNBoNv/76K++++y41a9bExcWFRx99lOTkZLKysnj55Zfx8fHB2dmZsWPHkpWVZXKM3Nxc3nvvPerVq4ednR1BQUG88cYbhbZTFIX333/f2NT3wAMPcPz48SLLlZSUxMsvv0xgYCB2dnbUr1+fjz76CL1eX67rFEKoJAASQtyxlJQUwsPDadeuHW+88QZubm44OztTt25dfv3111Idw5Cv8/TTT3Po0CEuX77MsmXLmDNnDi+++CJOTk6VeAX5Zs6cyfr163n99dd56qmnWLFiBc899xxPPfUUp0+f5p133uHhhx9m4cKFfPTRRyb7jhs3jrfffps2bdrw+eef07VrV2bOnMnQoUNNtnv77bd56623CAkJ4ZNPPqFu3br07t27UNCYkZFB165d+fnnnxk1ahRffvklnTp1YurUqUyePLnS3wshqjRFCCFKYd++fQqgLFiwoNC6AwcOKIDi6emp+Pr6Kt9++62yePFipX379opGo1HWrl1bqnO89957ioODgwIYH2+++Wa5ytusWTOla9eupd5+y5YtCqA0b95cyc7ONi4fNmyYotFolH79+pls36FDB6VOnTrG14cOHVIAZdy4cSbbTZkyRQGUf/75R1EURYmPj1dsbW2VAQMGKHq93rjdG2+8oQDK6NGjjcvee+89xcnJSTl9+rTJMV9//XXFyspKiYqKMi4DlOnTp5f6eoWo7qQGSAhxx9LS0gBITEzkjz/+YMKECQwfPpzNmzfj6enJ+++/X6rjBAUF0aVLF+bNm8fvv//OU089xQcffMDXX39dmcU3MWrUKGxsbIyvw8LCUBSFp556ymS7sLAwLl++TG5uLgBr1qwBKFQz88orrwCwevVqADZt2kR2djYvvPACGo3GuN3LL79cqCzLly+nc+fO1KhRg4SEBOOjZ8+e6HQ6tm/ffucXLEQ1JUnQQog75uDgAEBwcDBhYWHG5c7OzgwcOJCff/6Z3NxcNBoN165dM9nXw8MDW1tbli5dyvjx4zl9+jS1atUC4OGHH0av1/Paa68xbNgwPD09uX79OtnZ2SbndnNzq7BrqV27tslrw7EDAwMLLdfr9SQnJ+Pp6cmlS5fQarXUr1/fZDs/Pz/c3d25dOkSgPFngwYNTLbz9vYulOR95swZjhw5gre3d5FlLU3vOiFE0SQAEkLcsYCAAKBwAjOAj48POTk5pKenc+PGDYKDg03Wb9myhW7duvHtt9/SunVrY/Bj8NBDD7Fw4UIOHjxIz549efjhh9m2bZtx/ejRo0scm6isrKysyrRcURST1wVrde6UXq+nV69evPrqq0Wub9iwYYWdS4jqRgIgIcQdCwgIwM/Pj6tXrxZaFx0djb29PS4uLtjZ2bFx40aT9SEhIQDExcUV2c09JycHwNjU9Nlnn3Hjxg2Tc98N6tSpg16v58yZMzRp0sS4PC4ujqSkJOrUqWPcDtTanbp16xq3u3btmsl1AdSrV4+0tDR69uxphisQonqRHCAhRIV44oknuHz5skmAk5CQwB9//EH37t3RarXY29vTs2dPk4ch6GnYsCEHDx7k9OnTJsddsmQJWq2Wli1bAhAaGmqyf9OmTc13kSXo378/ALNnzzZZPmvWLAAGDBgAQM+ePbGxseGrr74yqT26dT+Axx9/nN27d7N+/fpC65KSkoxBoRCi7KQGSAhRoq+//pqkpCSio6MB+Ouvv7hy5QoAL7zwgjFHZurUqfz666888sgjTJ48GTc3N+bOnUtOTg4ffPDBbc/z3//+l7Vr19K5c2cmTZqEp6cnf//9N2vXrmXcuHGlqunZvn27MTH42rVrpKenGxOwu3TpQpcuXcr1HpRGSEgIo0ePZt68eSQlJdG1a1fCw8NZtGgRgwcP5oEHHgDUXJ8pU6Ywc+ZMHnzwQfr378/BgwdZu3YtXl5eJsf873//y59//smDDz7ImDFjCA0NJT09naNHj/Lbb79x8eLFQvsIIUrJwr3QhBB3uTp16ph0Sy/4uHDhgsm2586dU4YMGaK4uroqDg4OSvfu3ZXw8PBSn2vv3r1Kv379FD8/P8XGxkZp2LCh8n//939KTk5OqfafPn16sWW9XRdxQzf45cuXmyxfsGCBAij79u0r8lzXrl0zLsvJyVHeffddJTg4WLGxsVECAwOVqVOnKpmZmSb76nQ65d1331X8/f0VBwcHpVu3bsqxY8eUOnXqmHSDVxRFSU1NVaZOnarUr19fsbW1Vby8vJSOHTsqn376qUl3/dJcoxAin0ZRbsngE0IIIYSo4iQHSAghhBDVjgRAQgghhKh2JAASQgghRLUjAZAQQgghqh0JgIQQQghR7UgAJIQQQohqRwZCLIJeryc6OhoXF5cKnddHCCGEEJVHURRSU1MJCAhAqy25jkcCoCJER0cXmvlZCCGEEPeGy5cvF5pY+VYSABXBxcUFUN9AV1dXC5dGCCGEEKWRkpJCYGCg8T5eEgmAimBo9nJ1dZUASAghhLjHlCZ9RZKghRBCCFHtSAAkhBBCiGpHAiAhhBBCVDsSAAkhhBCi2pEASAghhBDVjgRAQgghhKh2JAASQgghRLUjAZAQQgghqh0JgIQQQghR7UgAJIQQQohqRwIgIYQQQlQ7EgAJIYQQotqRAEgIIYQQZZaZo0NRFJNlN7MLLzPIztWTmaMzR9FKRQIgIYQQQpTJyZgU7v/oHx6bu5ub2WpQ8++Za7R5byMDvtzB0SvJxm0VRWHN0Rg6ffQP7f9vE8v2RRUbJJmTRrkbSnGXSUlJwc3NjeTkZFxdXS1dHCGEEKJS7DiTwKXr6QxrVxutVmOyLiUzh592X+Kx0Fr4uNoblyffzOGhr3dwKTEDgCGtazK5V0MGfr2DpIwcALQaGNSqJu6ONpy7ls7209dMjn1fXQ9mPtySYC+nCr2esty/rSv0zEIIIYS4611LzWLG3yf463A0AI62VgxpXctkm682n+H7fy+w+1wiP48LA0CvV5i87BCXEjPwcbEjMT2blQev8u+ZayRl5NCylhtBnk78eTialQevGo9lY6VhQrf6uNhZ89nGSPacv85Ha08xd2So+S76FhIACSGEEFVIWlYun6w7xbrjsXz6WAidG3ibrD8Tl8rj3+3mRl5tDcDS8MsmAZCiKKw7HgvAjrMJ7DqbQMf6Xny95SybT8Vja61l/ph27DmfyPurT5KQlk0NRxvmPBlKTXcHnmgXyO5ziSgoWGu1PNjSnwa+LgD0aebH+6tP8NbApmZ4N4onAZAQQghRRfx75hqv/naEmORMAOZtP28SAKVm5vDszxHcyMihsZ8Lr/RuxLM/7Wfvheucv5ZGXW9nAE7FpnL5+k3jfp9siORFnZ7PN50G4P3BzWle041mAa6cu5bOumMxfD28DTXdHQDoVN+LTvW9iixjbU9H5o1qWynXXxaSBC2EEEJUAUkZ2Tzz435ikjMJcFNzdnafSyT5plrToygK/11+hPPX0vF3s+fncWH0aupLt0Y+ACzbf9l4rPV5tT9tartjb6PlYFQSz/4UgaLA8LDaPN42EACNRsPMh1uwf1qvYgOeu5UEQEIIIUQVsPLgVTJz9DTydWHzK92o7+NMrl5ha2Q8APN3XmTd8VhsrbR8O6INXs52ADzRTg1mfo+4QnauHoANx+MAGNq+NmM6BgNqN/aQQHemF9F0ZXVLAvW9QAIgIYQQ4i517Goy8SmZt91OURSWhqs1OCPuq42DrRW9m/oCsOFEHMkZOczOa75668EmtK5dw7hv98Y+eLvYkZCWzT+n4rh8PYMTMSloNdCziS/Pda2Lt4sdPi52zBnRBjtrq0q4UvOTHCAhhBDiLhOfksn0P4+z9lgsNd0d2DS5Kw62xQcehy4nERmXip21lkGtagJqsvG3W8+x9VQ8X/5zhtTMXBr7uTAirI7JvjZWWh4NrcWcred4fcVR2gV5ANAuyAMPJ1sANr/SFQ3gYm9TORdsAVIDJIQQQpiRoih8t+0cb648WuTIyDvPJtBj1jbWHlPzcK4m3eSnPRcBNTCa+MsBvtlylqzc/H2X7VNrfwa08MfNQQ1SWtR0w8/VnvRsHT/suADAlN6NCo33AzDu/mCa+ruSlJHDxhNq81efZn7G9a72NlUq+AEJgIQQQog7di01i+QC3cpLsnhvFDPXnmLx3ijeXHnMZFTkHJ2eqSuOkpqZS8tabjzfrR4Ac7ae40Z6Ns8vPsDqIzF8sj6SB7/cwcYTcew5n8ifeeP5GPJ5ALRaDb3ymsEAWtd2p0cTnyLL5Olsxx+TOvFa38bYWWtxsrWib3O/IretKqQJTAghhLgDV25k0G/2v7g72bDh5ZKbqg5G3eDdv44bX/9+4Aqta7vz5H1qs9Sv+y8TdT0DL2c7lo6/D1srLeuPx3LuWjoPfbODy9dv4mJnjZ2NljPxaTzz437jsep6OdE+2MPkfL2b+fLTnksA/Ld3IzSa4pOVbay0TOhWj8fa1uJmto6AvC7tVZUEQEIIISqVXq/w7dazBHs5M6Clv6WLc0cyc3Qs2HkRvaIwoWs9tFoNX24+Q2pWLqlZufy4+yLPdq1n3F6vV/gt4gpbT8ej18P+SzfI0Sn0a+5HSKA7H649xbt/HadmDQc61PXky81nAJj0QD0cbdVb9ORejZj4ywHjuDyznmhFu6AafLI+Mm+wQXWk5VeKCHA61PXk4dY18XKxo2Mpu6kbeodVdRIACSGEqFQ7zyXw6YbT2Fpr6VTfE3dH2zLtn5mjw96m6FqVm9k6bK21Ze6GrSgK6dk6nO2sTZZl5uhNanB0eoXYlEwUReHctXSm/3GMi3lzYCmKQr8W/vwWccW4/Zxt5xgWVhtXexvOxqcxdcUR9l28YXLuet5OfPJYCE62Vhy5ksSao7GMXbCPpv6uxKVkUdPdgWFhtY3b92vuR4uabhy9msykB+obm7X+b0iL216ntZWWWU+0KtN7U11IACSEEKJSGQbVy87Vs+rgVcZ0Ci71vov3XmLaqmP0bebHuw81M5mU80DUDUb9EE67oBrMH9OuxOadW3279Ryfbojk/cHNGRFWB71e4cWlB1l7LJaxHYOY3Lshhy4n8ebKY1xISDfZ183BhuSbOXy28TRrj8WiV+CBRt5cvnGTs/FpzN16DnsbK77+5yzZOj2OtlaM66x2Jbe10tC3mb8x8PrssVb4uJxi0e6LnIhJAeClHg1MupprtRp+GN2WI1eS6d646BweUXYyG3wRZDZ4IURVoNcrLNp9kfbBHjQLcCvVPqsOXsXd0cY4OnBpJWVk8+PuSzzcpia1ajialKHDh5uJS8kCoLGfC2tf6mwSrOy7eJ3I2FSGtzedkTz5Zg5dPt5iHMnYxd6aqf2aMLRdIInp2Tz41b/G4y56qj1dG+ZP+aAoCmuOxhKfmsmw9rVNapDiUzPp8vEWMnP0WGs1LHv2Pvacv84n6yON23g62ZKYng2og/xZazXYWGkZ3DqAV/s25oPVJ1m6L3/k5LUvdeZCQjrPLz5g8r480Mib9wY3N3lPihJx6QYfrDmJu4MN340MxdpK+iiVh8wGL4QQgrXHYnn3rxP4uNix/dUHim1GMth5NoGXlx3C1krL7qnd8SxDLsi7f51g5cGrrDp0lT8mdjJ2mT5yNZm4lCycbK3I1Sucik3l8JVkWgW6A3AjPZunFuwjNSsXR1srHm6TPyHn//49T/LNHOp6OeFsb82RK8m8sfIoqw5dRVEU4lKysNJq0OkVPll/ii4NvNBoNEQlZvDGyqPsOJsAwE97LjFzSAvC6noC8M0/Z8nM0WOl1ZCrVxi3aD9JeUHW2E5BbDgex9UkNd/myftq82rfxrje0gX8nYeacSImhSNXkhkYEkATf1ca+brQvKYrx66m4Olky9sDm/JQSECpaqZC69Tg9wkdS/1+izsnIaYQQtzD/jh0lbbvb2Jd3pgxBf175hoA8alZ/Lj7YonHURTFWAOSrdOz8uBVAKKTbtL1ky1MXXHEuO2xq8mEvreRD9eeAiAyNpVVh9Ttz19L57/Ljxi7dm/Ia/7q1tiHAS3UBOhl+6KMx5q7/RypWbkAfL7ptHEqhoS0LOPYNa/2bczK5zsxbUATHGysCL9wnX0Xb+Bka8WSZ+7DydaKY1dT+PtIDN9tO0fv2dvYcTYBO2stXs62nL+WzhPz9jB1xVGORyfzS7h6/rlPhlLfx5kbGTkoCgxtF8j0gc3Y8J8uvDOwKSue78j7g1sUCn4A7G2sWDCmHdMGNOH9Qc0Btanqf6PaMWNQMzZN7sqgVjXL1CwnzEsCICGEuIf9eSiahLQsJv96iNNxqSbrdp5LMD6fs/UcqZk5pGXl8sveKOZtP8e87efYfDIORVHYdDKeQ5eTjNsvCY9CURQ+33iaS4kZLAm/TMSl6yiKwvurT5CYns3cbef4LeIKszZGoigQEuiOrZWWdcdj+WbLWRRFMeb/9G7qaxyj5s9D0aRm5hCfksmiXRcBsLPWcvn6TX7Nm5Dz63/OkpGto2UtN/o088VKq2Fc57ps+E8XHmjkjYONFbOeaEX7YA+evl/NKXpx6UFmrj1FZo6eDnU9Wf9yFza/0o1h7QON1/TgVzvI0SncX9+LXk19+W5kKDXdHejcwIt3HmoGgJOdNWM6BdOmwHQRRfF0tmNc57q4OeYHSH5u9ozqEEQNp7IlegvzkxygIkgOkBDiXtHtky3GXkl1vZxYNakTrvY2XL6eQeePt2Ct1VCrhgMXEzPo38KPg1FJxCSbzi3VuYEXscmZnIlPY1SHOizff4WbOTo+fqQlr684gj7vLnFfXQ8mPlCfkT+EG/e1tdKSrdOj1cD6l7sQfvE6b648BkCn+p7sPJuIjZWGiLd64WJnTffPtnEhIZ1gLyeCvZz451Q8oXVqMLClP+/kNde1C/Zg9ZEYAH58qj1dCuT2GOj0irHnV0pmDp0/UnOF3BxseHNAEx4LrWVS+7LnfCJvrDjK+byE5lUTOxmb4XR6Ba0Gqa2pAiQHSAghqoGsXB1R19Xgx9PJlvMJ6Uz9/SjfjGjDrrzan5BAd57qFMzEXw6w5qhaG1Pbw5G2QTXI0ak1NP+eUbd1sbfmlV6NuJmtY3nEFaauPIpegbZ1anDkSjJ7zl/n/LXDAIzpGMSlxHS2RKrNbENa16KBrwv1fZy5npbNF5vPsPNsIgAd6nkZm5E+frQlExcf4EJCurF31ZTejWhTx53v/73A1aSbrD4Sg5VWw8Ru9ejcoOixawp2e3e1t+H7UW3ZcTaBJ++rjY+LfaHt76vryZqXOrN4bxTuDjbG4OfWY4nqQ5rAhBDiLnAiOoWEtCzja0VR2HEmgb8OR/PX4WjOxqcW2udSYgZ6BVzsrPlhTDustBpWH41h/8XrxuCjUz1P+jX3o32wB1ZaDc92rcv6l7sw6/FWfDWsNetf7sJ9ddXRgyf3aoibow1D26tj0Oj0ChoNvDe4OcPzxqWJT83CwcaKiQ/UZ/YTranr5YSLnTUv92wAqLUoL/RowN8v3m8MMoa3z5+eoV2QB5te6cqIvOP1aeZLh3qe2FlbMbV/YzQaaBbgyh8TOzH5NiMXF9Q+2IPJvRoWGfwY2NtY8fT9wTwSWqvYbUT1IU1gRZAmMCGEOR27msxDX++geU03/px0PwDrjsXw3M/5Xao1GvhhdFu6N86f22nt0RgmLD5ASKA7f0zsxNQVR1gSfpn2wR6cv5ZOQloWS8ffx311PcnM0ZGVqzdOlFmQoigkpmcbRwBWFIXen2/nTHwag1oF8MXQ1lxLzaLLx1u4maNj4gP1+G+fxoA6EGFWrq7IwQ31eoWUzJxiBz68kZ6Ni721SZfvxLQsajjaFjlhpxC3U5b7t9QACSFEBVm2L4rXfjtS5AzfkbGpPPvTfv46HM2t3zv/OhyNXoEjV5I5dy0tb5maA1PXy4nGfi4oCry89BCXEvMH5TNsW8/bCYAXujfA1lpL+IXrJKRlYW+jpXVtd0Ct/Sgq+AG11qbg9AcajYb3BzdnSOuavNm/CQDeLnZ88lhLhrYLZEK3+sZtHWytig1wtFpNiaM+13CyLTTejaeznQQ/wiwkABJCiAqQmaPjnT9PsGz/ZWPPp4LeX32C9cfjeGHJQZ5etJ/ovHFmCvaUAth4Io7MHB1bI+MB+PyJVvw56X5a13YnJTOX534+wM1sNcA6d00Nhup5OwMQ4O7AyLxJNUFtbio4onBZhNX15PMnWpmMvPxgywA+fKSlyfQRQtyrJAASQogKsONMAjfzan42nIgzWXf5eoZxUD4bKw3/nIrn0Tm7yMjO5Wx8mrEXF6jj5uw6l0B6tg4/V3ta1HTD1lrLnBGheDnbcjImha/+USfMNNQA1fdxNu4/oVs9HPPmsupUyskvhaiOLB4AffPNNwQFBWFvb09YWBjh4eHFbpuTk8OMGTOoV68e9vb2hISEsG7dujs6phBCVISCtThbT8WbNIMt338ZRVG7ha99qTM13R2ITs5k0a5LxmCpeU01X+Hg5SQW71EH6uvdzNfYHOTnZs9bDzYF4O8jMerknPGGJrD8AMjL2Y73BzenU31PHmkjyb5CFMeiAdCyZcuYPHky06dP58CBA4SEhNCnTx/i4+OL3H7atGl89913fPXVV5w4cYLnnnuOIUOGcPDgwXIfUwhRfej0Cnp96ft95Oj0hfJ1cnV6cnX6Qss2nVQDGWuthvRsHbvPJRrP+et+dbbwJ9rVpr6PC6/0bgjA3G3n+CNvBOURYXVoFeiOosDmU+r/q95N/UzO07OJL7bWWqKuZ7Dt9DXSs3VYazXU8TSdZ+rhNrVYPO4+vF1KP5WFENWNRQOgWbNm8cwzzzB27FiaNm3K3LlzcXR0ZP78+UVu/9NPP/HGG2/Qv39/6taty4QJE+jfvz+fffZZuY8phKgeYpMzafPeRl5edqhU22fm6Oj/xb+0fX8Tqw5eJVen53//nqfluxsY8b+9JoFUxKUb3MhQB+F7rK3a5XvDCbVGaPvpa8SmZOLuaEOfZmoPrkGtatLAx5nkmzmcjktDo1GDm97N8nt4udhbE5bXPd3Ayc6aLnnj4ny79RwAtT0dsZGJM4UoM4v91WRnZxMREUHPnj3zC6PV0rNnT3bv3l3kPllZWdjbm47x4ODgwI4dO8p9TCFE9bA1Mp7kmzmsPhpDambObbdffzyWM/FpJKZn8/KyQ9w3czPvrz5JRraOvReu8/fRGOO2hmasHk3y57vaeCKOXJ2en/ZcAuDh1rWMCclWWo2xFgggtHYNvF3sTGp8ejT2KTKwMWwTfuE6YNr8JYQoPYsFQAkJCeh0Onx9fU2W+/r6EhtbuAcFQJ8+fZg1axZnzpxBr9ezceNGVqxYQUxMTLmPCWpglZKSYvIQQlQtB6JuAGqTlCF4KMnScHVOqtA6NbC11pKQpo5Z072xDwCzNkQam8jy57vyI6yuBy721iSkZdP3i3/5J685yzAPlkGfZn60rOUGQN/malBT38eZBnkJzX2b+xdZrh5NfCjYS1wCICHK556qN/3iiy9o0KABjRs3xtbWlkmTJjF27Fi02ju7jJkzZ+Lm5mZ8BAYG3n4nIcQ9JeLSDeNzwyjJkbGptH1/IzPXnjTZ9mJCOrvPJ6LRwFfDWrPupc5MG9CEzZO78uWw1ng62XIxMYPl+6+wYOdFrty4ib2Nlq4NvbGx0tIjL0g6G5+Go60VMx9uQSM/F5NzaDQavhsZyrsPNWNUhyDj8m9HtOHTx0KMzWW38nS2o21QftOYYQwgIUTZWCwA8vLywsrKirg40+6icXFx+Pn5FbmPt7c3q1atIj09nUuXLnHq1CmcnZ2pW7duuY8JMHXqVJKTk42Py5cv3+HVCSHuJkkZ2cYxcwDjPFnf/3uehLRsvtt2nuX78//ul+U979rQmwB3B+p6OzOuc118XO1xtrPm+QfUgQDfXHWUGX+fAOCRNrVwyOt+PrJDEM521vRo7MPGyV0Zlje1xK383RwY3TEIW+v8f8UNfF149JaJPG/Vu2l+cFTPR2qAhCgPiwVAtra2hIaGsnnzZuMyvV7P5s2b6dChQ4n72tvbU7NmTXJzc/n9998ZNGjQHR3Tzs4OV1dXk4cQouo4eDkJwNgr6lRsKhcT0o0zjgNMW3WMY1eTydHp+S1C7bU1tF3RtcEjwmrj72aPooCTrRXvPtSMGYOaG9eH1qnB0Xd688OYdtR0d6jw6+nTTP1CZ6XVSBOYEOVk0eE8J0+ezOjRo2nbti3t27dn9uzZpKenM3bsWABGjRpFzZo1mTlzJgB79+7l6tWrtGrViqtXr/LOO++g1+t59dVXS31MIUT1czCv+atzAy9ORKdwKjaVqSuOcjNHR30fZ2p7OKqDE87dhZOtdd68WLb0aFJ0M5S9jRXzRrZl3fEYhofVKTLIKe0knuUR6OHI7CdaodVqip3eQghRMosGQE888QTXrl3j7bffJjY2llatWrFu3TpjEnNUVJRJfk9mZibTpk3j/PnzODs7079/f3766Sfc3d1LfUwhRNUTl5LJzWwdQV5F58NE5CVAh9apQQ1HW07FprL7vJoHNLRdII+FBvLo3F2ciU8jMycbgJH3BZXYvbxFLTda5CUxW8Lg1jUtdm4hqgKZDb4IMhu8EPeOs/GpDPp6J1m5ehaPCyOsrqfJep1eoeU760nP1rH2pc7EJN/kqYX7AXVair1v9MTDyZYcnZ7TcakA2FlrqeftXKm1OEKIiiezwQsh7jm7ziXw1MJ9nM+b36o0UjNzGP9TBOnZOnL1ChN/OUhcSqbJNqfjUknP1uFsZ01DXxfaB3tindePvHczPzyc1NnKbay0NAtwo1mAG/V9XCT4EaKKkyl9hRCVIjNHR1JGDn5u9rfd9kJCOs/+GEFqVi7OdtZ8Oaz1bfdRFIX/Lj/C+Wvp+Lna4+ZgQ2RcKhN+jmBq/yYYwpctebOqtwp0x0qrwdnOms4NvNh2+hqjC3Q/F0JULxIACSEqxaRfDrA18hqrJnaiec3ic2UysnN57ic1+AFYdyyWG+nZ1MirmSnOX0diWHc8FhsrDXOebEMNR1sGfr2DA1FJPDa38MjvbWq7G59/Maw18SlZJrOoCyGqF2kCE0JUuPPX0th0Mp5cvcLKg1eNy1cfieGzDZFcT1cTjTOyc3nl18NExqXi7WJHPW8nsnV6k32K89fhaADGd6lL69o1CPJy4ruRobSs5Uawl5PJIyTQnUdC82dGd7W3keBHiGpOaoCEEBVuWYFBBTeciGXagCbcyMjh5WUHydEpLN4bxdP3B7MkPIorN25irdXw7Yg2nIpJ4a0/jrNs32XGdgoqNg/nZraOf89cA2BAiwDj8o71vPhz0v2Ve3FCiCpBaoCEEBUqR6fn97yBBAEuX7/JyZhUVhy4Qo5O7XR6PT2bT9ZHcuXGTWq6O7BgbDvaBXnwUKua2FlriYxLNQ5eWJTtZ66RmaOnVg0Hmvi7FLudEEIURwIgIUSF2nwyjoS0bLyc7XigkTegzqy+bJ9aKzR9YFOm9G6In6s9T3UKZsN/utC5gbqdm4ONcTb1peFRxmNm5er4afdFTkSrExVvOK5Od9OnmZ/01hJClIs0gQkhKtTSvEDn0dBa1PN2YkvkNRbuukjyzRwcbKx4JLQWrvY2TOreoMj9h7avzYqDV1lx4CqPtQ2kXZAH7/x5giXhUbjYWbPi+Y5sPqUGQAXnxBJCiLKQGiAhqrkP1pyk7fsbOR6dfEfHURSF3yKusO20mpvzRLtAejTxRauB5Js5AAxo6Y+rfclTN7QLqsFDIQHk6hWeX3yAb7acZUlebVBqVi5PzNtDUkYOHk62hNapcUdlFkJUX1IDJEQ1sOd8IkeuJAHgYGvNQy0DcHO04dd9l5m3/TwAM9ec4udxYeU6fmxyJlOWH2bHWXWW9X7N/QjOm5aiXZAHey9cB4qfXLQgjUbDh4+0IDI2lci4VD5ZHwnAU52C+ftINPGpWQD0aOyDdQlTVQghREkkABKiijt/LY0R/9uLTp8/680Xm84wrnMwszaeNi7bcTaBXecS6FjPq0zHVxSFF5ccJPzideystbzcsyHjOgcb1/dp5sfeC9ep5+1U6hobR1tr5o4M5aGvdpCalUvPJj5MG9CEfi38GDZvD7l6hd55M6ILIUR5SAAkxD1s7rZzLNp10SS4AWga4Mrnj7eihpMtn286g06v0NDXmeY13Th0OYnz19L5cO0pAHo28cHfzYGf9lzi0/WRfDfShQ/XnuLKjQy+HNYaX9eSR3Ledvoa4RevY2utZfWL91Pfx7RX1vCw2iRlZNO7jAnLwV5OLH4mjC2nrvHU/UFotRraBXkwb1QoJ6JT6NHYp9THEkKIW8lkqEWQyVBFRTkZk4K7ow3+bg4VfuxVB6/y8rJDxa7v0tCbV/s04sGvdgCw5sXONA1wJStXxzdbzjFn61lqeziy4vlOZOXq6PLxFjJz9DjYWHEzRweos6cveeY+bK21nLuWxs1sncmozoqiMPDrHRy7msIznYN5c0DTCr9OIYQorbLcv6UGSIhKcvRKMoO/3YmXsy1bpnTD0bbi/txOxqTw+oojgDoS8qBW+YMBXkvN4rmfI9h++pox7+fBlv40DVD/GdhZWzG5V0Oe7hSMnY0WexsrwIYxHYOZu+0cN3N0NPV35fKNDCIu3eCdv47jbGfN//49j15Re3dNG9AEd0db1h2L5djVFJxsrZjQrX6FXZ8QQlQ2CYCEqCSfbIhEp1eIS8li4a6LPF9BAcLpuFSe/SmCzBw9XRp681rfxlhpTZuWZj7cgv8sO0xSRg5WWg2TezUsdBw3R9PeWBMfqEdKZg71vJ0Z3aEOWyOvMe7H/fyyN8pku98irrD5ZBy1PZ24lJgOwNOd6xpnVRdCiHuBdKEQohLsPZ/I9rzu4ABzt54zdgW/laIoXExIN8njURSF89fS0BdYlpWrY9aGSAZ8+S9R1zOoVcOBL55oVSj4ARjSuhZjOgYBanf0ut63n/fKxd6GD4a04On7g7G20tKzqS8vdleDtgA3e+aPacvvEzrQwMeZGxk5HL6cRFJGDp5OtiZJz0IIcS+QHKAiSA6QuBOKovD4d7vZd/EGw9oHEnHpBqfj0pj0QH2m9Glksu3VpJtMW3mULZHX6NnEl3kjQ9FqNbzz53EW7rrI1H6NebZrPQCmrTrKz3vU2pieTXx5f3Bz/NyKT1BWFIVDl5NoFuCGrXX5vusoisLx6BTqejsZm/Cyc/Xsu3idzLw8oSb+rgS4V3yOkxBClJXkAAlhQVsjr7Hv4g3srLW81KMhhy4n8dzPEczfeYHBrWtS38cZnV5h0a6LfLohkoxsNZDYdDKOr/45S6CHAwt3XQRg0a6LjOtcl/TsXH7Lm1/rk0db8mhordv2qNJoNLSufWcDBWo0GpOkZwBbay2d6petq7wQQtxtJAASogLFp2by2u9qcvLojkH4udnTx9WXkEB3Dl9Oov+X/zK+c13+PZvA4bzJPtsF1aBLA28+23ia2ZtPY1NgcL/o5Ez+PXONKzdukpmjp4GPc6mCHyGEECWTAEiICpKj0zNp8UHiU7No4OPMSz3Uua40Gg1zn2zDq78d4d8zCXy95SwALnbWvN6/McPa1Uar1RCTkskve6PIztXTrZE3tT0c+XH3JZbtu8zlGxmAms8jwY8QQtw5CYCEqCAfrT1F+MXrONtZ893IUJzs8v+8/N0c+PGp9qw6dJVZG0/TsqY7bz3Y1CSHZ/rApiRlZHM9PZvZT7QiJjmTH3dfYv3xWPQK2FppebhNLUtcmhBCVDkSAAlRAW6kZzN/5wUAPns8pMheVxqNhiGtazGkddFBjJ21Fd+OCDW+dne0NTadAfRu5itdzYUQooJIN3ghKsCBqBvoFajn7USfCpyjaliByUOHtqtdYccVQojqTgIgISrAgagbALS5w15Xt3owJIC6Xuokoh3reVbosYUQojqTJjAhKsCBS0kAtCnlbOel5WxnzT9TulXoMYUQQkgNkBB3LFen51Benk5oBQdAQgghKocEQEKUQ0zyTU7GpABwKjaVmzk6XOytqV+KKSeEEEJYnjSBCVFGGdm5DPlmF9fSslj5fEdjL61Wge5oi5iXSwghxN1HAiAhymjRrkvEpmQC8OmG03jkzaouzV9CCHHvkABIiDJIvpnD3G3njK+3n76Go60VUPE9wIQQQlQeyQESogz+9+95km/m0MDHmWHt1XF5MrJ1aDTQqra7ZQsnhBCi1CQAEtWeoijM3XaOn3ZfLHG7hLQsftihjvb8Su9GvNyzAXbW6p9QAx9nXO1tKruoQgghKog0gYlqL+LSDT5cewoAB1trHg1Vp6rIzNFhY6XFKi+xec7Wc2Rk62hZy40+zXzRaDSM6RTEd9vO06m+l8XKL4QQouwkABLV3tJ9l43P31x5lPo+zuw8m8CXm8/QKtCdH59uT2JaNj/tuQTAlN6NjDOy/7d3I9rW8aCDjNIshBD3FAmARLWWkpnD30eiAWjs58Kp2FSGfLsTRVHX771wnRl/nUCvKGTn6gkL9qBzg/zaHmsrLb2a+lqi6EIIIe6A5ACJau3PQ9Fk5uhp4OPMsvEdqO3hiKJADUcbnutaD40GFu+NMtYS/bdPfu2PEEKIe5fUAIlqbVleYDO0fW3cHG1YMv4+NhyP5aGQADyd7XCwseLzTadRFOje2Ie2QR4WLrEQQoiKIAGQqBbSsnLZcDyWzg288XaxA+DEhSu0il1OJ5schuWchp1W1ATG2ruC/XAAXuhen9PxqWw9Fc+rfRuVvwApMXB5DzQdDFKDJIR5xJ+EjEQIut/SJREAVw9AbhbU6WDpkgASAIlq4J9TcUxbeYzo5Ezq+zizamIn7K21XP5tKu/Z/KlutO2WnTJToNOLaLUavh7WGr2CsTdYufw2FqJ2w0NfQZtR5T+OEKJ0FAV+ehjS4uCFCPAItnSJqreM67CgP+iy4YX94FHX0iWSHCBRtc3edJqnFu4nOlmduuJsfBqv/XaEz9ccpkPaJgDSgvtCyDD1UfcBdceIhRgyoTUazZ0FP/En1eAHYP+C8h9HCFF6SZcgNRoUHVzZb+nSiCPLIPem+nkc/NnSpQEkABJVmKIoLNh5EYAxHYP4+ekwbKw0rD4aQ8zupbhqMshwrIXzyCUwZK76eOJnsHWG6+fg0s6KKciBn/KfRx+A2KMVc1whRPFiDhd4fshixRCoXyYP/Jj/+uBi0OVarjx5JAASVVZ8ahbJN3Ow0mp4vV9j7m/gxVsPNgXgCeutADjeNwa0Bf4M7Jyh+SPq84J/sOWVmwWHl6jPXWtV3HGFECUzCYAOF7+dqHxXIyD+BFjbg6MnpMXCmQ2WLpUEQKLqOhWbCkCQpyP2NuqEpSPvq8Mn3RwI055C0Wih1YjCO7YZrf488QfcvHGHhVgNN6+DSwA8+Lm67MgyyLl5Z8cVQpTs1gBIr7dcWaq7A4vUn00HQ6vhecss/0VQkqBFlRUZmwJAYz9X4zKNRsNj2n/U5w16g2tA4R1rtgHf5hB3DHZ9BQ16l78Q4d+rP1s/CfV7glttSI6CPd9CnU7g4gc1gsp/fFH15NyEm0ng6m/pkpQs+Qo4+4LVXTgHnqJA9KH811kpkHTR/Im3KTFg7wq2TvnLkqLA2Q+sbdXXiqI2i+dkmO5r6wy+zfJ7jeZmQVo8uAcWPo+iqDUsWeqXPmoEqf9bbpWbrZ5Ln2O63MkbPOvlv76ZBNdOleVKi6fPhaO/q89DR4Ojl/p/9cx6SIku+n+wmUgAJKqsyNg0ABr6uuQvzM2GQ3lNUoaanltpNGpPrbWvwr+fqY87olEDIK1W/bn1A9g8Q12ltYZn/wXfpnd4DlFlrHxOrTl8aj3UCrV0aYp2ORx+6AWtnoTB31i6NIWlRENGAmiswKshXDup1gKZMwCKPQbfPwCBYTDmb3XZmU2w+BG15nnwt+qyff+DNVOKPkbBXqOrnodjv8OoP6BuV9Ptjv4GK8blv7Z3gxcPgeMt45atmZJfG1OQRgvjNkHNUNDr4H89IfFMmS+5RJ71oXYH9f9r7Q5qx5BDv0CXYq7dDCQAElVWZJxaA9TIr0AAdHqt+o/R2a/kmp2QYRC5Vv22dqeaDYYaddTn7Z6GC9sgNVZtGrt5A/bPhwGf3vl5xL1Pr4ezm9Rv6OHzoNZ3li5R0QwdBI4shZ7TwdnHsuW5laH5y7sxBLbLD4CaDTFfGfb9T+3yffFf9dz+IWrNL6jN4D3fBScv2Jv3GbvWVHNkQK0FTI1W17UeqQZ0x1cAirrs1gBo71z1p7MfZKdBZrJ6jvsm5G9z8wYcXqo+rxGsBj2G5Tevw74f1ADo3BY1+LGyBbciapvKw8oWuk/Lr81qMwoSz4GNY8Ucv5wkABJVkk6vcCZOrQEyCYAM7c6thoNVCb/+9q4walXFF8zJC8auUZ+f+wd+GgJHfoXe74GNQ8WfT9xbrp9Xb2AAJ1ZBv4/Awd2SJSrajYvqT32umuTf6SWLFqcQQwDkH6I+Ci4zh+x0tVbG4MBP0KmG+jcP+e9brXZqsGHjBBP3gl3e/6qM6/BZY7UZPvognN0MSl4O0+l16hcoQxNX3HG4ul+tTX7uXzV3cc0U9X9d2HP5QceR5aDLUpv3n9uRvzxqL8zvDcdXQt+Z+TVEbZ9Sf/8qQ/NHocVjFm8+lSRoUSVdSkwnK1ePvY2W2h553zKSotR/JKA2RVlacDdwrw1ZyXDiT0uXRtwNCnbXzs2Eo8stVpQS3biU//zAj8Yxs+4aRQVA0YfMV87jqyA7Nb+G48ivefmASv6yAz+q440BNB+SH/yA2nTV9CH1ecRCOJj3xc3GUR1H59Av+dsavtQ16qfWxLV4DKwd1Jwgw/hHipIf2LQZbToafWB7taYsJwP2zIXIvC9olTlgq7WtxYMfkABIVFGReT3AGvq65A9ieHAxoEBQZ9OEP0vRaqF13j+Zu6BHhLgLGAIgOzf15936e5FUIABKPAuXdlmuLEUpGAD5NFNrR25eVxO3zcHwuXWenP8lZ/fX6rI+H6g1Poln4Oiv6rI2YwofwxCAHPxJ/fJm5wa9ZuQfX1EgJzO/WctwDAd3tdkd8oOe6ANqbZKVHbR8zPQ8hpxHgG0fqrVTNduqCdhVnARAokqKjE1Bg55GPk5qXoUuJ3/00dAxFi2biVbD1bb4Szsg4aylS2N5en3hR0V8ay94vDulKBVbtoIMN+4ur6h5E7FH1CaQ2ynt9VXE+6DXQZI6iTD1uqs/DyyquPf3TqXFq/kzaMCvBdjYg3cTdV30wdt/bkX9DpblEX9KnfdPY6Xm7xi+5Ch6cKih5hc2fzh/mXcTqNW2cDnq3K/m6hiavlo+rv6/sHWBGxfgwnY4+RdkJqljjNV7IH9fQ0BzbIWaDxRh6IY+SC3DrVoOVX/fDOeqJtP1SA6QqBJikzOZ8fdxIi7dYN5Ab8buGcTL9jfgBDCjwIb27tD4QQuVsghuNaF+L7VL6MEf87/hVUcHf4a/Xi7cRdfeHcauLX9PufDvYd3r6jdbgHbPlD/p/OYN+L67mqtTUtlijsDiR6HbVGg7tnTHVpT8AKjuA+oxjv2mftsPaF38foeXwZ+T1IRbUBN9H1tYeLs/JuZ/CdBaQ/9P1DyPskqJVj8jrQ10fV3NazmyTH1obeChL/PHeinKv5/Bvvlqz6iyzM+VnQ4LH1RzX4b+otZcpMbCwgHq+2X4TA3d370aqAObgloTFHcUfh2pvg68T83F01qZnmPLTNj2EVABgW3DvmpZWw1Xe34qejXQsLFXm6EO5o0Q32ZU0RMka7Xqus3v5m9n6wQtHoWIBfDjQ/nbtn7S9FpqdwDPBmot04e185eHFtPz1clT/b94fIXa/d4wGGwVJzVA4p5y6HIS87afM3l8tiGSXrO2seZoLHEpWRxd8QluumIGMOwwUf0HdDcxfNs69ItaU1Ud6fWw7ePCwQ+o33ANvWfK4/CS/OAH1PdZryvnsZbmBz8lle3Yb+oknFs+KP1nmnRJ/bZuZavmZBh+L47+pt78i3NkaX7wA2oya9wJ022uXzCdf0mfm990UlaGBGj3QDV/JKhzgePmwPZPi69hyUqF7Z9ByhW19qIsjq9Um3Ii16jd8EHtQZl4Vu1xZWjeOr5C/Vm7wIzjTQaqQZ/B5T35+YAFy7brKyok+NHaqP9rQP2S03qkGiy3f0ZdVqutWnvmVhtChhZ/nNYj1d5hDfuBf0t1WdizahOagYNH4RobjQbufxkoEFjVaq+OPVacji+oOUb3PZ8fOFZxUgMk7hk7zyYw8oe96Iv5/xQS6E5u1k36JW8FDUzKfoHpLz2Pt0tewKO1UsfHuNs07KMOKJcWp/bwaDLQ0iUyv4vb1QDAzhUmhoO1nbo8+gD8/Ihald93pmmiaGnoctTxWADG/QOLHoScdLULrnfDsh2r4HxGvf9PDVIWF1M2Q01OejycXg9NSlHraNjHp6maJBrUWR3Q7sZFtWdPUbUqBWuNRv2pdoeOXKOWs9+H+dsZgp/grtDjbfhfj7wB8XSFa0Fux5D/415HvdGO/kutGcu5Cd+0z59HL+j+wvse+119/wteb2kVzIc6sEgNIoxBnaI+D3tOTUAGNXgwaNQXXo9SBxPc8gHs+149RsMCQ2EYyubZQB2DqahamdKytgfbAl28H/pSfRhoNDBy5e2P4+wNk28JZn2awKvn8wdOtHXOH1SxoNZPqk1ehgDcoUbJ11SzDbwZc/syVSFSAyTM7q/D0Qz48l/OxqeWep+rSTd5YclB9Aq0rVODh9vUNHl8MKQFKyZ0ZEHHBDw1qcQqNdht1wkvH3+1R4Wjx90Z/IDaG8JwczO01Vc3hptbi8fUEZANn1m9HuoNKSddvUGVVcJpteuvrYvajOTXQl1eni7RxvmMHPJG9u6hDu6Wk64GQQYFg5KC13Y7BRN3IS9JPu8mXtzvRcpVyEhUazcCwyA0r7ntyFI1QRbUSScPLVaftx2rvg82TuoNNLEceWeGHmCGEcw1GvWzcqt5+3n0Ci4vywSl8afg8t7818dXqo+Uq/nLDv6s9rbKvVl0Xo2tk1rOdk+rr0+vg9S4wmVrM0ptEjL8DpbnYVvJ49vY2Oefq6jgx8DOJX+7OwnoqigJgIRZpWTm8NYfxzgencI3W84VWq/XK/x75hp/HY42eTz/cwTX07NpUdONn8eFMevxViaP4WG1sdJq8DmtVuuv0HejU0M/NPfKH73hRnd2k/l6qtwtMq7nN4cUVZVvWFaeHlHGoKKlGlAYx4Q5VPZjGbosNxus9rQxKVuBACUpSq0R0eTVrJzdCMkFbtTFMeSuGMoI6ojBGiu1yeZaZPH7eDdRb4r1e6hNJjdvwKm/88+fGqNOQtlogFrjYwgEC04XUVqGJjDD4J4FlTSPXuwxNYg0vC+JZyEzpXTnNHb1HgBejdTg7e//qMtCx6pfbpIvwz/v5ZWjmLwaUGtQarXPawb8xbRsWhs1SVlUCxIACbP6378XSMpQq2TXHI0hOSM/P+JMXCqPfbebkT+E88KSgyaPw1eScXe04dsRbYwTmxZy4yKc3wLA8Ofe4PMnWlXy1VQgz3p5uRRKXnf9auTIMjWHxT8EAloVXh8yTL0xXY3Ib84qrVtrVco7KF5Wan4tT8EgLWS4WvtSsGyGY/s2U3MuFH1+DUxxCtYa+bfKX+7qrzaRQtEBYKFaI6v8Ma4M2xt+hgzLry24k8EBk26pASqoZhu123lupjrwXkGGpN/GA9QgDdSu2beTm6XmcYGaxGt4/7Pygqf249XkYsMyK9uS82rANKgu2LTZuL/a7CSqBckBEmaTmJbFD/+qCaTOdtakZeWy6tBVRncM4qc9l5jx13HcdEkssptHTVvTiQG1Gg2+rnY4/V7Cr2xGovqzbjfca5Yxv+Nu0GaUOmy+YaLAgur1gO5vqs+zM+CP5wtP06Gxgo6T1Hb/OxWxSG1y6P9p+avzj/6mNjMM+Kz45kdFyW/eKa7rrbO3emM68QcsGaa+dq8Ng769fdlurVUpeOPX69WxYVZPVoODRv0Kl239m2rtS1Zafn5IweRaZ29o1B9O/qne4Pt9lB9UBLRSuzJf2ql+pqfXFV9ORZ8/d9WtPcrajFLzevbPV+dPsrJT83jqdCgcAIEaAG37WJ1yZd4D+bVdBd/f4gIgvQ7Wv6EOqNf5laLLakyCLqIGSKNRg5S1r8KW/1Ob4gwMidmho9XzpFxVz1+nY3HviurUavVzcglQ/w5qhsKmd9SE61rt1PerzSgIz5tSosnAwnNg3arZELVn4PXz8F3n/CEoqkn3b6GSAEiYzZyt50jP1tGiphsPt6nJu3+dYEl4FMFeTrz9xzEUBd713U7X5INQVMeZhFKeqN24229zN2ryEDhPU5Ohr0aYrrsaoebHeDdUbyrHi0mgXPu62kxQ0jQft3PzhnoDy81UbzCl7cZdUM5N+HuyOgCcb/O8HilFuLJfnafJ2kEdHr847capAVBylPq4GqEm9JZUNr1OTfSF/FoV78ZqDYFhdvCIRepxr+xX54YrmBB8ZT/suWWiz3bjCjettBmtBkCHl6rzOxUMSpo+BBvehPRrhT/TogS2LzwlSv1earCRdCn/GJumw9MbTIMtA/faajAXuUZNIgcI7gLejfK3MQRAsUfUQFCb1xhw7p/8eaWaDAKv+qZlybmp/n5C0TVAoP6e/vO+2kPu1mv2qKt2Wb8SAZGrS1cDZaidaT1C/b128lLHxDm0GNo/q67za67WoF7cUbq/fztntXkx/Lv83xGPelC3++33FVWGBECi0mXm6Ph2y1kW7roIwCu9G9Iq0J2Za09xKjaV536OQFFgeFt/+l/MmyvngTfBr2XZT+bood5E7kU29vDMlvx/yAa7v1Zrhg7+CL3fz68xaf9s/kB0KOps0anRcG5zfrNJeRxZrgY/oOa2lCcAOvGnGvyAegPr9FLRORmG3BlDXk1xgruo701avHp94fPU45ZUtsRzaq2NtYM6JgyoCee+zdQB8a5E5E8pkHJV7RZdsFfQgYXqz4Z91TwTW6eiuxHXe0CdNDL5sprLZKhx8W+lBjPjt5au6U6jUQPOW1lZw9Mb1TLnZsJvT6m1c+e3QlqsOpDmraP2PjwPovaoQaBGW/hvwruRWpOUlaIOqmcYGd2Q5wRFj0tlqHW0cy16QD1Q/waf3V50zlLNNmqQWXB6ipIUaNY2mb5mwCy1O3jBmq+hiyElBnwal3xMg14z1N5hudkFyiZZIdWJBECiUoVfuM7rK45w/pra9fXhNjXp2tAbjUZD/+Z+rDoUTUa2jpa13HinSTSaY3Hg6AWdXi65d0NV5VZTfZhQ1ADo0BJoOkS9wWptoOtram8Vg5Bhao3FgR/LHwAVnDMI1JtuzJH8MUhKq2C+SnHdok3yaooZoK2gmm3yfobC/gVq7Ubs0fyE3lsZahf8WpjW7PiHqNf172dqN3VjmQt0i85MyS9bp5fV5qbiGPJuts7MO+a1vKasvKDErZb6uBMuvurNGtSeTpGr1do+AK+GanBWkJ0LNOhV/PGMgeAB9X3yrKf2iCrYTHfoF+j+lumcTQUToEvqYOARXPIgh4bAJSFSbdItrinT0M297gOmNU429qbBD6jNrGXp6WljX+ALhKiOJNwVFepGejZXbmRwMSGdN1Ye5fHvdnP+WjreLnbMGdGGzx4LMfbMGh6m5hDUcLRhzpOh2B7OS5JsNax6Bj/Fqd8LnP3UHJEVeQOpNXnQNPiB/PyFyLXqCLnlEX0wf86gunlD6xuSV0sr4aw6tYdGqzYrQdEJvMdWFMirua/0xzfkBBV3XANjTcwtN0rD62sn1Z+N8o5VsFv08RVqTyOvhqUrW6sRgCb/mN6NCjdlVRTD52w4163XV1q35gEZBowMaKOOS5V+rXDekqELfFH5P2Xh4gdOPmruU9zxorfR5eZ3CJDcHFEJJAASFeafU3G0eX8j93+0hW6fbuWXvWp1+bD2gWz6T1f6tfA36ZbePtiDxePC+GPi/dTU3oAzG9QVpakNqE6srNX8B1BrU6DoG4JPY3UsmFtniy4LQ+1P00HQ6UX1+ZFlau5HaRlmrq7fS50qAYruFm2cnbqELsvFMfyOlFS2ohKEi3rd+/3C3aIjylg290C1C3px56hI9XuqCcF3eq6CQwIU7AnV9qnix6Uy1gAFle+cBhrN7YckOLtJbdJ18FB7jglRwaQJTJRL8s0clu2LIizYk5BAd3R6hf9bfRJFARsrDT20B+jjEMl9dT3wd3CAbUUfx5hRce2U+m2wdsf8fA2Rr/WTavMKqMPnB3crers2o9T8kPDv1XyZsjr6W/5x6nRSz5UcBSvG53ddvp0jy/KPYegWHX8cVj6nTu4Iai7L1Qi1C3l5xl2p+0Dhsvm3zL9xK4radAeFAwSfZmoTlaJTE2c966llvRIOe+eptRzRB9RmRkP36tJoM0q9aRd1zopkCIi3f3Jn5zLsdyVCzR+7fk4dVbjZEDXRecfn6vWseVWtzQN1TCG48wDIcP6zG00TobNSYeeX6s+L/6rLQobljwwuRAWSAEiU2bpjMbz9x3HiU7NwtrNm1cROHLqcxLlr6bg72vDvpBa4fD0KsnPgVBkPXtxkfdWdR12o201NfG0zqvhkzWZDYN1U9Zvz3jnlPFc9NV/HMNDflvfVXk5l4eyr5iEV7BZdVDfwxgPKN+6KVgttRqpdrQuWzbe5GgjFHFKTsK3s1J5fBRnyR6IP5NckFXzfIhaUr2wN+6nNOunxRSczVyRDQKy1KT4H6nZ8mqpzP2Wn5td8tXhU7SFl55zXq+rf/O7lBXlVwDAThgleL2zP74m2Yzb8e8tEtW1GFtpViIpg8QDom2++4ZNPPiE2NpaQkBC++uor2rcvvhfP7NmzmTNnDlFRUXh5efHoo48yc+ZM7O3V+Z7eeecd3n33XZN9GjVqxKlTZb0Ti6LM+OsE83deAMBaqyEtK5fnfo4gM0edXHJC13q4nPpNHaPDo57au6e0nH3VLrSiaIO+Vbs2l9REaOsEw5fl10SUlUYLTQfnN/t0nKQmwRoGnSvdQdQcHUPybNun1PmIbl433czK7s5yOzq+kN+l/dwWNaA58KM6K7ihOafJg0Xnkw2ZC1cPqDd8UG/4I37Nf9/KUzZrW3jyd3X6jVunYahoNYLUc2m05Z/ixcYehi1RAxBQc5baPp2/ftDXauL9rRPUugaYToBaXvUeUHuTJV1SA606nfIHjGw5VO0M4NdSHblZiEqgUZTipu2tfMuWLWPUqFHMnTuXsLAwZs+ezfLly4mMjMTHx6fQ9r/88gtPPfUU8+fPp2PHjpw+fZoxY8YwdOhQZs2aBagB0G+//camTfk3AGtra7y8vEpdrpSUFNzc3EhOTsbV1fXOL7SKWL7/Mv/97QgaDTzfrR7D2tfmkTm7iEvJAsDbxY7tU7rhMO8+SDwDA7+A0DGWLbSoHs79Az8NATs3ePEgfNlKDYxG/aHWnIm709//UQd4bP6oGowuGapO2TH5lHSEEOVSlvu3RZOgZ82axTPPPMPYsWNp2rQpc+fOxdHRkfnz5xe5/a5du+jUqRPDhw8nKCiI3r17M2zYMMLDw022s7a2xs/Pz/goS/AjinbsajLTVqljmbzUowH/7dOYWjUc+XZEKDZWam3Bi93r4xC7Tw1+bJzyJ0YUorIFd1MHAMxKhhXj1ODHvQ4EdbF0yURJDLVsJ/9UR8sG0yk7hKhEFguAsrOziYiIoGfPnvmF0Wrp2bMnu3fvLnKfjh07EhERYQx4zp8/z5o1a+jfv7/JdmfOnCEgIIC6desyYsQIoqKiijqcUVZWFikpKSaP6iorV8fExQcYsyCc03HqbO0Hom7w7E8RZOXq6d7Yhxe75ycph9apwXcjQ/lPz4YMbV87v+mh+cPqWCRCmINWC63zbqbn8gbTLClXStwdAlqrzVy6bHWsKJAu78JsLJYDlJCQgE6nw9fX12S5r69vsfk6w4cPJyEhgfvvvx9FUcjNzeW5557jjTfeMG4TFhbGwoULadSoETExMbz77rt07tyZY8eO4eJS9A155syZhfKG7lnXTqvfnm7XS+NyuJrIeMvou+/8eYLVR2MA2Hk2gS4NvPknMh5FgSBPRz4fVBftlXCoHWbcp3tNhe65R+H4ofwpGqQruzC3VsNh6wdqb0KNNm9sHnHXazMK1kxRnwfeZzplhxCV6J76erR161Y++OADvv32Ww4cOMCKFStYvXo17733nnGbfv368dhjj9GyZUv69OnDmjVrSEpK4tdffy32uFOnTiU5Odn4uHz5sjkup+IlX4XvusD33dXJG4tz4k/4oVf+oHp5ft13mSXhUWg06hg9OTqFzafU4OeRNrVY+Xwn3NZNgvm98wMdRVFzL34bqzY95N4E7yaVnwQqxK3caqpjDwE06KPOpC7ufi0eU6crAan9EWZlsRogLy8vrKysiIuLM1keFxeHn59fkfu89dZbjBw5knHj1MnuWrRoQXp6OuPHj+fNN99EW0R1t7u7Ow0bNuTs2bPFlsXOzg47uyowzsShxWoAknsTTqwynTunoH3fqz/PbIDrF8AjmFOxKUz7Q83xmdyzIZO612fN0VhWH41mePs63N/ASx0fJXKtum/4/9Suw1G7If4EWNurXX+tbOD+/5R9YDshKkLv99UeXd2mWrokorQc3GHg7LwJf0uYEFeICmaxGiBbW1tCQ0PZvHmzcZler2fz5s106FD0vDsZGRmFghwrK3WOn+I6s6WlpXHu3Dn8/av4t0G9Hg4UmLKguCkCrp/P7/YKxrl2Fu26RHaunq4NvZn4QH00Gg0DWvrz7YhQNfiBvC6qee/zpR3qlAeG87R4DMb8DSNXqhNXCmEJ3g3h0fkymOa9JmQo9P9EBjwUZmXRJrDJkyfz/fffs2jRIk6ePMmECRNIT09n7Fh1hudRo0YxdWr+N7mBAwcyZ84cli5dyoULF9i4cSNvvfUWAwcONAZCU6ZMYdu2bVy8eJFdu3YxZMgQrKysGDasHKPN3kvOb1FHxbVzVUe5vbwX4ovIpTIESYaZnA8tRp+bw8YTak3cuM7BaLVF1N7odfkTExr23f0VHF+lPpfu7kIIIe4hFh0I8YknnuDatWu8/fbbxMbG0qpVK9atW2dMjI6KijKp8Zk2bRoajYZp06Zx9epVvL29GThwIP/3f/9n3ObKlSsMGzaMxMREvL29uf/++9mzZw/e3uUYbfZeYqiJCRmq5gJFrlYnseyT/96gy80faKz/p+rovKkxnN+9koQ0J1zsrQkL9ix8bICzmyHlqhr8DPgMfnsKIhaq63yaqjN0CyGEEPcIiw6EeLe6ZwZC1OWqQ/dnpcJ3XdURW5/bASnR8Mvj6iSCz/wDWrV2jIs7YNUEcPSCySdh87uw+2tOu3emd+wEBrUK4IuhrYs+19IRcOpvuO956DUDPm+mzhcE0PcjuO8581yzEEIIUYyy3L8tPhWGKCdFUXtyRR/IXxbQWp0XyLuJOlt0arQ6Iu6tWuUNNNZmNOz+mnpJO/FhKL2btin6XOmJ+fM4tRmlJjq3Gq5OlmhlBy0fr/DLE0IIISrTPdUNXhSQGpMf/Fjbg707dHlVfW1lDd1eB1sXdV3Bh3ttstuMY/6OC+xL9yLDrz1W6Blqs52ujYppJryyD/S54NUof16e9uPVWbU7TwZHj0q/XCGEEKIiSQ3QvSrmsPrTpyk8X8TI2aGji51Zfc6mM3y+6TQAE9w78BrhPGm3HWebYuJhw7kCCjSPuQbA87vKW3ohhBDCoqQG6F5lCEr8Q8q02430bL7/97zx9YKkVqQoDvjkxsDF7UXvVM5zCSGEEHcrCYDuVcagpFWZdpu77RxpWbk08Xdl2fj7CPLzYrNNV3VlcWMHSQAkhBCiipEmsHtVOYKSuJRMFu2+CMB/+zQkrK4n617ughLtAvPWwcm/IOO6aU5PegKkXFGf+7WooMILIYQQliUB0L0o7Zo6Jg8a8Gte6t0+33iazBw9oXVq8EAjH+NyTUBrNZCKOQxftVF7dtUOg0cX5gdaHvXA/i4eEkAIIYQoA2kCuxcZghLP+mBX9Az3mTk6Ii5dJ0enB+CPQ1dZuk+d5PXVPo3Q3DpX133Pqz9v3oC0WDjxh5oTJM1fQgghqiCpAboXxRxSf5YQlLz713GWhF+mib8rz3apy9QVRwGY9EB9wuoWMdpzyycgsL06i/zub+DIUohYBIpOXR/QqmKvQQghhLAgCYDuRbeplcnO1fP34RgATsak8PKyQwB0buDFf3o1LPqYGg141FWfd3heDYBO/a2OL1TCuYQQQoh7kTSB3YuM4/K0KnL1nvOJpGbl4uVsx6BWAQDUdHfgi6GtsSpqotNb+YeoD102pMery/xaVkDBhRBCiLuD1ADdazKuQ9Il9XkxQcmGE7EA9Grqy8yHWzDxgfr4utjj5mhT+vO0GQ2rJ6vP3WvLaM9CCCGqFKkButfEHlF/1ggCB/dCq/V6hQ3H1UlKezfzBaChr0vZgh+AFo+CjaP6XJq/hBBCVDESAN1rzm5WfwYUPWv74StJxKdm4WxnTcd6RSQ7l5a9G7R4TH1ep1P5jyOEEELchaQJ7F6Smw2Hl6jPDcHJLTacUGt/ujXyxs7a6s7O1+8jaNgHGvS+s+MIIYQQdxkJgO4lp9dB+jVw9i0yKMnR6Vl/TM3/6d3M787PZ+MAjQfc+XGEEEKIu4w0gd1LDixSf7YaAVamOT2HLicx8KsdnE9Ix85aS7dG3hYooBBCCHFvkBqge0XS5fz8n9ZPGhenZ+Xy6YZIFu66iKJADUcbPnykJa72ZUx6FkIIIaoRCYDuZooCx36HlGi4Eg4oENQZPOsBsOtsAv/97QhXk24CMKR1TaYNaIKns50FCy2EEELc/SQAuptd2A6/P226rM1oQJ3Z/alF+8jM0VOrhgP/N6QFXRtKs5cQQghRGhIA3c2uhKs/PetDrXbgVguaDQHg63/Okpmjp1WgO788E4ajrXyUQgghRGnJXfNuFn1I/Rk6FjpOMi6+fD2DJeFRALzWt7EEP0IIIUQZSS+wu1lM3qjPt4zEPHvTGXL1Cp0beNHhTgY7FEIIIaopCYDuVhnXIVmt5cGvhXHx2fhUVh68AsCU3o0sUTIhhBDinicB0N3KMON7jWCTOb8W7bqEXoGeTXwJCXQvclchhBBClEwCoLuVIQAKaGVcdDNbx6pDVwEY0zHI/GUSQgghqggJgO5WhgCoQP7PmqMxpGbmUquGw51NdCqEEEJUcxIA3a2KCICW7bsMwBNtA9FqNZYolRBCCFEllDkACgoKYsaMGURFRVVGeQRAZjJcP6c+91MDoHPX0gi/eB2tBh5rG2jBwgkhhBD3vjIHQC+//DIrVqygbt269OrVi6VLl5KVlVUZZau+Yo+qP90CwUlt6jLU/jzQyAc/N3tLlUwIIYSoEsoVAB06dIjw8HCaNGnCCy+8gL+/P5MmTeLAgQOVUcbq55bmL0VR+OtwNACPt5PaHyGEEOJOlTsHqE2bNnz55ZdER0czffp0/ve//9GuXTtatWrF/PnzURSlIstZvdwSAJ1PSCcmORNbKy1dGsh8X0IIIcSdKvccCjk5OaxcuZIFCxawceNG7rvvPp5++mmuXLnCG2+8waZNm/jll18qsqzVx/UL6k+vhoA66ztAmzruONhaWapUQgghRJVR5gDowIEDLFiwgCVLlqDVahk1ahSff/45jRs3Nm4zZMgQ2rVrV6EFrVbSYtWfLv4A7DqXCECnel6WKpEQQghRpZQ5AGrXrh29evVizpw5DB48GBsbm0LbBAcHM3To0AopYLWjKJAapz538UWvV9h9Xg2AOtaXAEgIIYSoCGUOgM6fP0+dOnVK3MbJyYkFCxaUu1DVWmYS6PJ61Tn7cSImhaSMHJxsrWhZy82iRRNCCCGqijInQcfHx7N3795Cy/fu3cv+/fsrpFDVmqH2x94NbOzZmZf/E1bXExsrGbdSCCGEqAhlvqNOnDiRy5cvF1p+9epVJk6cWCGFqtYM+T/OfgDszMv/kakvhBBCiIpT5gDoxIkTtGnTptDy1q1bc+LEiQopVLVWIP8nO1fPvgvXAegk+T9CCCFEhSlzAGRnZ0dcXFyh5TExMVhbl7tXvTAo0APsQNQNbubo8HSypZGvi2XLJYQQQlQhZQ6AevfuzdSpU0lOTjYuS0pK4o033qBXr14VWrhqyVAD5OzLphPq8y4NvWXyUyGEEKIClbnK5tNPP6VLly7UqVOH1q1bA3Do0CF8fX356aefKryA1U5qDACKsy8bdqgBUJ9mvpYskRBCCFHllDkAqlmzJkeOHGHx4sUcPnwYBwcHxo4dy7Bhw4ocE0iUUZoa9ETr3Yi6noGdtZYuDWX6CyGEEKIilStpx8nJifHjx1d0WaqntHg4tBjaPg32rpCq5gDtiVODyc4NvHC0ldwqIYQQoiKV+8564sQJoqKiyM7ONln+0EMP3XGhqpW//wOn/obcbOj2mrEGaF2Uurp3Mz8LFk4IIYSomso1EvSQIUM4evQoGo3GOOu7RqMm6ep0uootYVWWGgeRa9XnV/dDVhpkpwGwK84arQZ6NPaxYAGFEEKIqqnMvcBeeuklgoODiY+Px9HRkePHj7N9+3batm3L1q1bK6GIVdihxaDkBYwxh421PzlWDqTjQNsgDzyd7SxYQCGEEKJqKnMN0O7du/nnn3/w8vJCq9Wi1Wq5//77mTlzJi+++CIHDx6sjHJWPYoCB37Mf50WBzGHAEjU1ACgd1Pp/SWEEEJUhjLXAOl0Olxc1EH5vLy8iI6OBqBOnTpERkZWbOmqsos74MYFsHWGGkHqstPrAbiao0562rGejP4shBBCVIYy1wA1b96cw4cPExwcTFhYGB9//DG2trbMmzePunXrVkYZqyZD7U+LRyEnE25chDMbAYjRu+Fka0UjPxn9WQghhKgMZa4BmjZtGnq9HoAZM2Zw4cIFOnfuzJo1a/jyyy8rvIBVki4XTv6pPm89CvxD1Oc31Xm/4hV3QgLdsZLRn4UQQohKUeYaoD59+hif169fn1OnTnH9+nVq1Khh7AkmbiMnHXIz1ed+zUGXZbI6TqlBaJ0aFiiYEEIIUT2UqQYoJycHa2trjh07ZrLcw8NDgp+yyC0Q8FjZgl8Lk9XxijttaksAJIQQQlSWMgVANjY21K5dW8b6uVOGAMjKDjQasHMBz/rG1fG407q2u2XKJoQQQlQDZc4BevPNN3njjTe4fv16ZZSnejAEQNb2+csMeUCAnXsA7o62Zi6UEEIIUX2UOQfo66+/5uzZswQEBFCnTh2cnJxM1h84cKDCCldlGfJ/rAsMcugfAsd+B6BWYLAFCiWEEEJUH2UOgAYPHlwJxahmSqgBylJsaFK3tgUKJYQQQlQfZQ6Apk+fXhnlqF4Mvb6s85u5cgPacVypz3FdbULreFioYEIIIUT1UO7Z4MUdMDaB5dcAnU/WMyhrBs521hzxcbZQwYQQQojqocwBkFarLbHLu/QQKwVjE1h+DtD5a+kA1PN2QisDIAohhBCVqswB0MqVK01e5+TkcPDgQRYtWsS7775bYQWr0gp2g89zKVENgOp4OhW1hxBCCCEqUJkDoEGDBhVa9uijj9KsWTOWLVvG008/XSEFq9KKqAG6mBcABXlJACSEEEJUtjKPA1Sc++67j82bN1fU4aq2InKALiZkABDk6WiJEgkhhBDVSoUEQDdv3uTLL7+kZs2aZd73m2++ISgoCHt7e8LCwggPDy9x+9mzZ9OoUSMcHBwIDAzkP//5D5mZmXd0TLOTGiAhhBDCosrcBHbrpKeKopCamoqjoyM///xzmY61bNkyJk+ezNy5cwkLC2P27Nn06dOHyMhIfHx8Cm3/yy+/8PrrrzN//nw6duzI6dOnGTNmDBqNhlmzZpXrmBahMw2AMnN0xCSrQVyQ5AAJIYQQla7MAdDnn39uEgBptVq8vb0JCwujRo2yTeA5a9YsnnnmGcaOHQvA3LlzWb16NfPnz+f1118vtP2uXbvo1KkTw4cPByAoKIhhw4axd+/ech/TIm4ZCfpSotr85WpvTQ1HG0uVSgghhKg2yhwAjRkzpkJOnJ2dTUREBFOnTjUu02q19OzZk927dxe5T8eOHfn5558JDw+nffv2nD9/njVr1jBy5MhyHxMgKyuLrKz8GdpTUlLu9PJKdstI0AWbv0oaYkAIIYQQFaPMOUALFixg+fLlhZYvX76cRYsWlfo4CQkJ6HQ6fH19TZb7+voSGxtb5D7Dhw9nxowZ3H///djY2FCvXj26devGG2+8Ue5jAsycORM3NzfjIzAwsNTXUS7GbvDqSNAXE/ICIGn+EkIIIcyizAHQzJkz8fLyKrTcx8eHDz74oEIKVZytW7fywQcf8O2333LgwAFWrFjB6tWree+99+7ouFOnTiU5Odn4uHz5cgWVuBiFaoCkB5gQQghhTmVuAouKiiI4uPBs5XXq1CEqKqrUx/Hy8sLKyoq4uDiT5XFxcfj5+RW5z1tvvcXIkSMZN24cAC1atCA9PZ3x48fz5ptvluuYAHZ2dtjZ2RW7vsLd0g3eUAMkgyAKIYQQ5lHmGiAfHx+OHDlSaPnhw4fx9PQs9XFsbW0JDQ01GTtIr9ezefNmOnToUOQ+GRkZaLWmRbaysgLU3mjlOaZF3NIN/pJ0gRdCCCHMqsw1QMOGDePFF1/ExcWFLl26ALBt2zZeeuklhg4dWqZjTZ48mdGjR9O2bVvat2/P7NmzSU9PN/bgGjVqFDVr1mTmzJkADBw4kFmzZtG6dWvCwsI4e/Ysb731FgMHDjQGQrc75l2hQDf4zBwd0cYu8NIEJoQQQphDmQOg9957j4sXL9KjRw+srdXd9Xo9o0aNKnMO0BNPPMG1a9d4++23iY2NpVWrVqxbt86YxBwVFWVS4zNt2jQ0Gg3Tpk3j6tWreHt7M3DgQP7v//6v1Me8KxSoAYq6rub/uNhb4+Fka8FCCSGEENWHRlEUpTw7njlzhkOHDuHg4ECLFi2oU6dORZfNYlJSUnBzcyM5ORlXV9eKP8FPQ+DcPzDkOzZYd2P8TxG0qOnGXy/cX/HnEkIIIaqJsty/y1wDZNCgQQMaNGhQ3t2rt9xs9aeVrUyBIYQQQlhAmZOgH3nkET766KNCyz/++GMee+yxCilUlVegF9jl6zcBqOMh+T9CCCGEuZQ5ANq+fTv9+/cvtLxfv35s3769QgpV5RXIAUq+mQNADcn/EUIIIcymzAFQWloatraFb9Y2NjaVP4VEVVGgBig1Uw2AXOzK3RophBBCiDIqcwDUokULli1bVmj50qVLadq0aYUUqsor0A0+LSsXUHuBCSGEEMI8ynzXfeutt3j44Yc5d+4c3bt3B2Dz5s388ssv/PbbbxVewCqpQBNYaqbaDd5ZAiAhhBDCbMp81x04cCCrVq3igw8+4LfffsPBwYGQkBD++ecfPDw8KqOMVY9JE5haA+QsTWBCCCGE2ZTrrjtgwAAGDBgAqH3ulyxZwpQpU4iIiECn01VoAaukAt3g85vAbCxYICGEEKJ6KXMOkMH27dsZPXo0AQEBfPbZZ3Tv3p09e/ZUZNmqJkUx1gApkgMkhBBCWESZ7rqxsbEsXLiQH374gZSUFB5//HGysrJYtWqVJECXli4HUAffvqnYoNOrz6UJTAghhDCfUtcADRw4kEaNGnHkyBFmz55NdHQ0X331VWWWrWoy5P8AabnqBK5aDTjaWlmqREIIIUS1U+pqh7Vr1/Liiy8yYcIEmQLjTuiyjU9Tc9X409nOGo1GY6kSCSGEENVOqWuAduzYQWpqKqGhoYSFhfH111+TkJBQmWWrmgw1QFa2pGbpAUmAFkIIIcyt1AHQfffdx/fff09MTAzPPvssS5cuJSAgAL1ez8aNG0lNTa3MclYdxjGA7EmTLvBCCCGERZS5F5iTkxNPPfUUO3bs4OjRo7zyyit8+OGH+Pj48NBDD1VGGauW3IKjQOdNgyE9wIQQQgizKnc3eIBGjRrx8ccfc+XKFZYsWVJRZarajE1gdvmDIEoAJIQQQpjVHQVABlZWVgwePJg///yzIg5XtZlMgyFNYEIIIYQlVEgAJMqgwDQYMgq0EEIIYRkSAJmboRu8ta2MAi2EEEJYiARA5mYyEaqaBC1NYEIIIYR5SQBkbkXkAEkNkBBCCGFeEgCZW8FxgLIkCVoIIYSwBAmAzK3gSNBSAySEEEJYhARA5lbESNDSC0wIIYQwLwmAzM2YBG0nTWBCCCGEhUgAZG7GbvB2pBh6gUkTmBBCCGFWEgCZW14NkGJlJ+MACSGEEBYiAZC55eUA5WhtURR1kYud5AAJIYQQ5iQBkLnlBUBZii0AVloN9jbyMQghhBDmJHdeczMEQFgBavOXRqOxZImEEEKIakcCIHPLywHK1KvNXtIDTAghhDA/CYDMLa8GKEORAEgIIYSwFAmAzE2nBkA39WoTmKsMgiiEEEKYnQRA5maoAdKpNT8yBpAQQghhfhIAmVteDlCqTq0BkiYwIYQQwvwkADK3vBqg9LwaIBkEUQghhDA/CYDMLS8ASsvJqwGSAEgIIYQwOwmAzC2vCSwlN28cIGkCE0IIIcxOAiBzy6sBMgZA0gtMCCGEMDsJgMwtrxt8co46+rMkQQshhBDmJwGQueXVAN3IVt96yQESQgghzE8CIHPLywFKylLfeukFJoQQQpifBEDmpMsFRQ/k1wC52EkOkBBCCGFuEgCZU17tD8D1rLwcIKkBEkIIIcxOAiBzysv/gfwASJrAhBBCCPOTAMic8mqAFK0N+ry33sHGypIlEkIIIaolCYDMKa8LPFa2xkW21vIRCCGEEOYmd19zymsCU6ztjYustRpLlUYIIYSotiQAMidDE5iVHQC2Vlo0GgmAhBBCCHOTAMiccrMB0OcFQDZWEvwIIYQQliABkDnl1QDp83KAbCT/RwghhLAIuQObU14OkF6bFwBZydsvhBBCWILcgc3JWAOUnwMkhBBCCPOTO7A56dQcIJ1WcoCEEEIIS5IAyJzyaoB0WnX+L2upARJCCCEsQu7A5pQXAOUaa4Dk7RdCCCEsQe7A5pTXDV6nUZOgbaUJTAghhLAICYDMyVgDpDaBSQ2QEEIIYRkyFbk5+TSBkOEkaJsAEgAJIYQQliIBkDk16geN+nH+wBXYfVgGQhRCCCEsRO7AFpCj0wOSAySEEEJYigRAFpCtUwBpAhNCCCEsRe7AFpCTq9YASQAkhBBCWMZdcQf+5ptvCAoKwt7enrCwMMLDw4vdtlu3bmg0mkKPAQMGGLcZM2ZMofV9+/Y1x6WUiqEJzFqawIQQQgiLsHgS9LJly5g8eTJz584lLCyM2bNn06dPHyIjI/Hx8Sm0/YoVK8jOzja+TkxMJCQkhMcee8xku759+7JgwQLjazs7u8q7iDLKzwG6K+JPIYQQotqx+B141qxZPPPMM4wdO5amTZsyd+5cHB0dmT9/fpHbe3h44OfnZ3xs3LgRR0fHQgGQnZ2dyXY1atQwx+WUiuQACSGEEJZl0TtwdnY2ERER9OzZ07hMq9XSs2dPdu/eXapj/PDDDwwdOhQnJyeT5Vu3bsXHx4dGjRoxYcIEEhMTiz1GVlYWKSkpJo/KZKgBkgBICCGEsAyL3oETEhLQ6XT4+vqaLPf19SU2Nva2+4eHh3Ps2DHGjRtnsrxv3778+OOPbN68mY8++oht27bRr18/dDpdkceZOXMmbm5uxkdgYGD5L6oUjEnQ1pIDJIQQQliCxXOA7sQPP/xAixYtaN++vcnyoUOHGp+3aNGCli1bUq9ePbZu3UqPHj0KHWfq1KlMnjzZ+DolJaVSgyDJARJCCCEsy6J3YC8vL6ysrIiLizNZHhcXh5+fX4n7pqens3TpUp5++unbnqdu3bp4eXlx9uzZItfb2dnh6upq8qhMOXrJARJCCCEsyaJ3YFtbW0JDQ9m8ebNxmV6vZ/PmzXTo0KHEfZcvX05WVhZPPvnkbc9z5coVEhMT8ff3v+MyVwQZB0gIIYSwLIvfgSdPnsz333/PokWLOHnyJBMmTCA9PZ2xY8cCMGrUKKZOnVpovx9++IHBgwfj6elpsjwtLY3//ve/7Nmzh4sXL7J582YGDRpE/fr16dOnj1mu6Xbyk6AlB0gIIYSwBIvnAD3xxBNcu3aNt99+m9jYWFq1asW6deuMidFRUVFotaZxWmRkJDt27GDDhg2FjmdlZcWRI0dYtGgRSUlJBAQE0Lt3b9577727ZiygnLxu8LYyGaoQQghhERpFURRLF+Juk5KSgpubG8nJyZWSD/TMj/vZeCKOD4a0YHhY7Qo/vhBCCFEdleX+LVUQFiBNYEIIIYRlSQBkAcZu8NIEJoQQQliE3IEtICdXusELIYQQliR3YAvIlqkwhBBCCIuSO7AF5OolB0gIIYSwJAmALMDQBCZTYQghhBCWIXdgCzD2ApMkaCGEEMIi5A5sAZIDJIQQQliW3IEtQMYBEkIIISxLAiALMEyFITVAQgghhGXIHdgCZDZ4IYQQwrLkDmwB2dIEJoQQQliUBEAWYJwKQ2qAhBBCCIuQO7CZ6fQKejUFSJrAhBBCCAuRO7CZGWp/QMYBEkIIISxF7sBmZhIASQ6QEEIIYRESAJmZoQs8gI1W3n4hhBDCEuQObGaGGiBrrQatVmqAhBBCCEuQAMjMsmUMICGEEMLi5C5sZsYaIMn/EUIIISxGAiAzM+QAyRhAQgghhOXIXdjMcmQmeCGEEMLi5C5sZsZpMKylCUwIIYSwFAmAzEwmQhVCCCEsT+7CZparlxwgIYQQwtLkLmxm2ZIDJIQQQlic3IXNLL8JTHKAhBBCCEuRAMjMDN3gpQZICCGEsBy5C5uZoRu8rcwEL4QQQliM3IXNLLvAXGBCCCGEsAwJgMxMBkIUQgghLE/uwmZmTIKWJjAhhBDCYuQubGYyF5gQQghheXIXNrMcvXSDF0IIISxNAiAzy8mVbvBCCCGEpcld2MwkCVoIIYSwPLkLm5mMAySEEEJYntyFzSx/LjDJARJCCCEsRQIgM5MmMCGEEMLy5C5sZpIELYQQQlietaULUN3kSBOYEKKK0+l05OTkWLoYogqysbHBysqqQo4lAZCZZUsTmBCiilIUhdjYWJKSkixdFFGFubu74+fnh0ZzZxUJEgCZmeQACSGqKkPw4+Pjg6Oj4x3foIQoSFEUMjIyiI+PB8Df3/+OjicBkJnlylQYQogqSKfTGYMfT09PSxdHVFEODg4AxMfH4+Pjc0fNYXIXNjNjE5i1fDMSQlQdhpwfR0dHC5dEVHWG37E7zTOTAMjMpAlMCFGVSbOXqGwV9Tsmd2EzM8wGLwGQEEJUTUFBQcyePbvU22/duhWNRmOR5PGFCxfi7u5u9vPeDeQubGbGqTAkABJCiLtCt27dePnllyvsePv27WP8+PGl3r5jx47ExMTg5uZWYWWoTGUN8O5WkgRtZtm50gQmhBD3GkVR0Ol0WFvf/rbp7e1dpmPb2tri5+dX3qKJcpK7sJkZaoCsZSBEIYSwuDFjxrBt2za++OILNBoNGo2GixcvGpul1q5dS2hoKHZ2duzYsYNz584xaNAgfH19cXZ2pl27dmzatMnkmLfWkGg0Gv73v/8xZMgQHB0dadCgAX/++adx/a1NYIZmqfXr19OkSROcnZ3p27cvMTExxn1yc3N58cUXcXd3x9PTk9dee43Ro0czePDgEq934cKF1K5dG0dHR4YMGUJiYqLJ+ttdX7du3bh06RL/+c9/jO8XQGJiIsOGDaNmzZo4OjrSokULlixZUpaPwuwkADIzyQESQlQXiqKQkZ1rkYeiKKUq4xdffEGHDh145plniImJISYmhsDAQOP6119/nQ8//JCTJ0/SsmVL0tLS6N+/P5s3b+bgwYP07duXgQMHEhUVVeJ53n33XR5//HGOHDlC//79GTFiBNevXy92+4yMDD799FN++ukntm/fTlRUFFOmTDGu/+ijj1i8eDELFixg586dpKSksGrVqhLLsHfvXp5++mkmTZrEoUOHeOCBB3j//fdNtrnd9a1YsYJatWoxY8YM4/sFkJmZSWhoKKtXr+bYsWOMHz+ekSNHEh4eXmKZLEmawMxMcoCEENXFzRwdTd9eb5Fzn5jRB0fb29/i3NzcsLW1xdHRschmqBkzZtCrVy/jaw8PD0JCQoyv33vvPVauXMmff/7JpEmTij3PmDFjGDZsGAAffPABX375JeHh4fTt27fI7XNycpg7dy716tUDYNKkScyYMcO4/quvvmLq1KkMGTIEgK+//po1a9aUeK1ffPEFffv25dVXXwWgYcOG7Nq1i3Xr1hm3CQkJKfH6PDw8sLKywsXFxeT9qlmzpkmA9sILL7B+/Xp+/fVX2rdvX2K5LEXuwmaWI+MACSHEPaNt27Ymr9PS0pgyZQpNmjTB3d0dZ2dnTp48edsaoJYtWxqfOzk54erqahzRuCiOjo7G4AfUUY8N2ycnJxMXF2cSWFhZWREaGlpiGU6ePElYWJjJsg4dOlTI9el0Ot577z1atGiBh4cHzs7OrF+//rb7WZLUAJmZNIEJIaoLBxsrTszoY7FzVwQnJyeT11OmTGHjxo18+umn1K9fHwcHBx599FGys7NLPI6NjY3Ja41Gg16vL9P2pW3WuxPlvb5PPvmEL774gtmzZ9OiRQucnJx4+eWXb7ufJUkAZGbSBCaEqC40Gk2pmqEszdbWFp1OV6ptd+7cyZgxY4xNT2lpaVy8eLESS1eYm5sbvr6+7Nu3jy5dugBqDcyBAwdo1apVsfs1adKEvXv3mizbs2ePyevSXF9R79fOnTsZNGgQTz75JAB6vZ7Tp0/TtGnT8lyiWchd2MxkJGghhLi7BAUFsXfvXi5evEhCQkKJNTMNGjRgxYoVHDp0iMOHDzN8+PASt68sL7zwAjNnzuSPP/4gMjKSl156iRs3bpQ4SvKLL77IunXr+PTTTzlz5gxff/21Sf4PlO76goKC2L59O1evXiUhIcG438aNG9m1axcnT57k2WefJS4uruIvvALJXdiMFEUp0AQmOUBCCHE3mDJlClZWVjRt2hRvb+8S81ZmzZpFjRo16NixIwMHDqRPnz60adPGjKVVvfbaawwbNoxRo0bRoUMHnJ2d6dOnD/b29sXuc9999/H999/zxRdfEBISwoYNG5g2bZrJNqW5vhkzZnDx4kXq1atnHPNo2rRptGnThj59+tCtWzf8/Pxu2yXf0jSKORoV7zEpKSm4ubmRnJyMq6trhR03O1dPw2lrATjyTm9c7W1us4cQQtwbMjMzuXDhAsHBwSXehEXl0Ov1NGnShMcff5z33nvP0sWpVCX9rpXl/n33N85WIYbmLwAbrVS+CSGEKJ9Lly6xYcMGunbtSlZWFl9//TUXLlxg+PDhli7aPUPuwmZkEgBJE5gQQohy0mq1LFy4kHbt2tGpUyeOHj3Kpk2baNKkiaWLds+QGiAzys4LgDQasNJKACSEEKJ8AgMD2blzp6WLcU+TGiAzKjgGUEmZ+kIIIYSoXBIAmVGujAEkhBBC3BXuijvxN998Q1BQEPb29oSFhZU4eVq3bt2MM9AWfAwYMMC4jaIovP322/j7++Pg4EDPnj05c+aMOS6lRPljAEntjxBCCGFJFg+Ali1bxuTJk5k+fToHDhwgJCSEPn36FDtHyooVK4wz0MbExHDs2DGsrKx47LHHjNt8/PHHfPnll8ydO5e9e/fi5OREnz59yMzMNNdlFSk7V6bBEEIIIe4GFr8Tz5o1i2eeeYaxY8fStGlT5s6di6OjI/Pnzy9yew8PD/z8/IyPjRs34ujoaAyAFEVh9uzZTJs2jUGDBtGyZUt+/PFHoqOjWbVqlRmvrDAZBVoIIYS4O1j0TpydnU1ERAQ9e/Y0LtNqtfTs2ZPdu3eX6hg//PADQ4cONU5Yd+HCBWJjY02O6ebmRlhYWLHHzMrKIiUlxeRRGYzzgFlLACSEEEJYkkXvxAkJCeh0Onx9fU2W+/r6Ehsbe9v9w8PDOXbsGOPGjTMuM+xXlmPOnDkTNzc34yMwMLCsl1Iq2ZIDJIQQVVJQUBCzZ882vtZoNCW2Oly8eBGNRsOhQ4fu6LwVdZzyGDNmzF0/3UVJ7umqiB9++IEWLVrQvn37OzrO1KlTSU5ONj4uX75cQSU0ZegGby2jQAshRJUWExNDv379KvSYRQUcgYGBxMTE0Lx58wo9V2WwZLBWFIveib28vLCysio0Y2xcXBx+fn4l7puens7SpUt5+umnTZYb9ivLMe3s7HB1dTV5VIac3LwaIGkCE0KIKs3Pzw87O7tKP4+VlRV+fn5YW8u4xmVl0Tuxra0toaGhbN682bhMr9ezefNmOnToUOK+y5cvJysriyeffNJkeXBwMH5+fibHTElJYe/evbc9ZmUz5gBJE5gQQtwV5s2bR0BAAHq93mT5oEGDeOqppwA4d+4cgwYNwtfXF2dnZ9q1a8emTZtKPO6tTWDh4eG0bt0ae3t72rZty8GDB0221+l0PP300wQHB+Pg4ECjRo344osvjOvfeecdFi1axB9//GEc/mXr1q1F1qps27aN9u3bY2dnh7+/P6+//jq5ubnG9d26dePFF1/k1VdfNXYseuedd0q8Hp1Ox+TJk3F3d8fT05NXX32VW+dSX7duHffff79xmwcffJBz584Z1wcHBwPQunVrNBoN3bp1A2Dfvn306tULLy8v3Nzc6Nq1KwcOHCixPBXB4lURkydP5vvvv2fRokWcPHmSCRMmkJ6eztixYwEYNWoUU6dOLbTfDz/8wODBg/H09DRZrtFoePnll3n//ff5888/OXr0KKNGjSIgIMDibZXZ0gtMCFGdKApkp1vmccvNuTiPPfYYiYmJbNmyxbjs+vXrrFu3jhEjRgCQlpZG//792bx5MwcPHqRv374MHDiQqKioUp0jLS2NBx98kKZNmxIREcE777zDlClTTLbR6/XUqlWL5cuXc+LECd5++23eeOMNfv31VwCmTJnC448/Tt++fY3DwHTs2LHQua5evUr//v1p164dhw8fZs6cOfzwww+8//77JtstWrQIJycn9u7dy8cff8yMGTPYuHFjsdfw2WefsXDhQubPn8+OHTu4fv06K1euNNkmPT2dyZMns3//fjZv3oxWq2XIkCHG4NIwxt+mTZuIiYlhxYoVAKSmpjJ69Gh27NjBnj17aNCgAf379yc1NbVU7295WbzO7IknnuDatWu8/fbbxMbG0qpVK9atW2dMYo6KikJ7S85MZGQkO3bsYMOGDUUe89VXXyU9PZ3x48eTlJTE/fffz7p167C3t6/06ylJrk7GARJCVCM5GfBBgGXO/UY02DrddrMaNWrQr18/fvnlF3r06AHAb7/9hpeXFw888AAAISEhhISEGPd57733WLlyJX/++SeTJk267Tl++eUX9Ho9P/zwA/b29jRr1owrV64wYcIE4zY2Nja8++67xtfBwcHs3r2bX3/9lccffxxnZ2ccHBzIysoqMUXk22+/JTAwkK+//hqNRkPjxo2Jjo7mtdde4+233zbeT1u2bMn06dMBaNCgAV9//TWbN2+mV69eRR539uzZTJ06lYcffhiAuXPnsn79epNtHnnkEZPX8+fPx9vbmxMnTtC8eXO8vb0B8PT0NLmG7t27m+w3b9483N3d2bZtGw8++GCx13qn7oo78aRJk7h06RJZWVns3buXsLAw47qtW7eycOFCk+0bNWqEoijFflAajYYZM2YQGxtLZmYmmzZtomHDhpV5CaUi4wAJIcTdZ8SIEfz+++9kZWUBsHjxYoYOHWoMFtLS0pgyZQpNmjTB3d0dZ2dnTp48WeoaoJMnT9KyZUuTL+FFpWR88803hIaG4u3tjbOzM/PmzSv1OQqeq0OHDibzTXbq1Im0tDSuXLliXNayZUuT/fz9/YsdgDg5OZmYmBiTe7O1tTVt27Y12e7MmTMMGzaMunXr4urqSlBQEMBtryEuLo5nnnmGBg0a4ObmhqurK2lpaWW+9rKyeA1QdZI/DpDkAAkhqgEbR7UmxlLnLqWBAweiKAqrV6+mXbt2/Pvvv3z++efG9VOmTGHjxo18+umn1K9fHwcHBx599FGys7MrrLhLly5lypQpfPbZZ3To0AEXFxc++eQT9u7dW2HnKMjGxsbktUajKZQHVVYDBw6kTp06fP/998a8qubNm9/2fRo9ejSJiYl88cUX1KlTBzs7Ozp06FCh729RJAAyo2xpAhNCVCcaTamaoSzN3t6ehx9+mMWLF3P27FkaNWpEmzZtjOt37tzJmDFjGDJkCKDWCF28eLHUx2/SpAk//fQTmZmZxlqgPXv2mGyzc+dOOnbsyPPPP29cVjCBGNSOQzqd7rbn+v3331EUxVgLtHPnTlxcXKhVq1apy1yQm5sb/v7+7N27ly5dugCQm5tLRESE8X1KTEwkMjKS77//ns6dOwOwY8eOQuUHCl3Dzp07+fbbb+nfvz8Aly9fJiEhoVxlLQu5E5uRNIEJIcTdacSIEaxevZr58+cbk58NGjRowIoVKzh06BCHDx9m+PDhZaotGT58OBqNhmeeeYYTJ06wZs0aPv3000Ln2L9/P+vXr+f06dO89dZb7Nu3z2SboKAgjhw5QmRkJAkJCeTk5BQ61/PPP8/ly5d54YUXOHXqFH/88QfTp09n8uTJhfJpy+Kll17iww8/ZNWqVZw6dYrnn3+epKQk4/oaNWrg6enJvHnzOHv2LP/88w+TJ082OYaPjw8ODg6sW7eOuLg4kpOTjdf+008/cfLkSfbu3cuIESNwcHAod1lLS+7EZqQB7G202Mk4QEIIcVfp3r07Hh4eREZGMnz4cJN1s2bNokaNGnTs2JGBAwfSp08fkxqi23F2duavv/7i6NGjtG7dmjfffJOPPvrIZJtnn32Whx9+mCeeeIKwsDASExNNaoMAnnnmGRo1akTbtm3x9vZm586dhc5Vs2ZN1qxZQ3h4OCEhITz33HM8/fTTTJs2rQzvRmGvvPIKI0eOZPTo0cYmOkONGKjTWC1dupSIiAiaN2/Of/7zHz755BOTY1hbW/Pll1/y3XffERAQwKBBgwC1V/eNGzdo06YNI0eO5MUXX8THx+eOylsaGuXWjvyClJQU3NzcSE5OrrRBEYUQoirJzMzkwoULBAcHW7zHrajaSvpdK8v9W6oihBBCCFHtSAAkhBBCiGpHAiAhhBBCVDsSAAkhhBCi2pEASAghhBDVjgRAQgghKox0LBaVraJ+xyQAEkIIcccMUytkZGRYuCSiqjP8jt06nUdZyVQYQggh7piVlRXu7u7GCTUdHR1NJuQU4k4pikJGRgbx8fG4u7tjZWV1R8eTAEgIIUSF8PPzAyh2VnEhKoK7u7vxd+1OSAAkhBCiQmg0Gvz9/fHx8Slynioh7pSNjc0d1/wYSAAkhBCiQllZWVXYTUqIyiJJ0EIIIYSodiQAEkIIIUS1IwGQEEIIIaodyQEqgmGQpZSUFAuXRAghhBClZbhvl2awRAmAipCamgpAYGCghUsihBBCiLJKTU3Fzc2txG00ioxbXoheryc6OhoXF5cKH8grJSWFwMBALl++jKura4Ue+25V3a65ul0vyDXLNVddcs331jUrikJqaioBAQFotSVn+UgNUBG0Wi21atWq1HO4urrec79Yd6q6XXN1u16Qa64u5Jqrh3v1mm9X82MgSdBCCCGEqHYkABJCCCFEtSMBkJnZ2dkxffp07OzsLF0Us6lu11zdrhfkmqsLuebqobpcsyRBCyGEEKLakRogIYQQQlQ7EgAJIYQQotqRAEgIIYQQ1Y4EQEIIIYSodiQAMqNvvvmGoKAg7O3tCQsLIzw83NJFqjAzZ86kXbt2uLi44OPjw+DBg4mMjDTZplu3bmg0GpPHc889Z6ES37l33nmn0PU0btzYuD4zM5OJEyfi6emJs7MzjzzyCHFxcRYs8Z0LCgoqdM0ajYaJEycCVeMz3r59OwMHDiQgIACNRsOqVatM1iuKwttvv42/vz8ODg707NmTM2fOmGxz/fp1RowYgaurK+7u7jz99NOkpaWZ8SrKpqRrzsnJ4bXXXqNFixY4OTkREBDAqFGjiI6ONjlGUb8bH374oZmvpPRu9zmPGTOm0PX07dvXZJt76XO+3fUW9Xet0Wj45JNPjNvca5/x7UgAZCbLli1j8uTJTJ8+nQMHDhASEkKfPn2Ij4+3dNEqxLZt25g4cSJ79uxh48aN5OTk0Lt3b9LT0022e+aZZ4iJiTE+Pv74YwuVuGI0a9bM5Hp27NhhXPef//yHv/76i+XLl7Nt2zaio6N5+OGHLVjaO7dv3z6T6924cSMAjz32mHGbe/0zTk9PJyQkhG+++abI9R9//DFffvklc+fOZe/evTg5OdGnTx8yMzON24wYMYLjx4+zceNG/v77b7Zv38748ePNdQllVtI1Z2RkcODAAd566y0OHDjAihUriIyM5KGHHiq07YwZM0w++xdeeMEcxS+X233OAH379jW5niVLlpisv5c+59tdb8HrjImJYf78+Wg0Gh555BGT7e6lz/i2FGEW7du3VyZOnGh8rdPplICAAGXmzJkWLFXliY+PVwBl27ZtxmVdu3ZVXnrpJcsVqoJNnz5dCQkJKXJdUlKSYmNjoyxfvty47OTJkwqg7N6920wlrHwvvfSSUq9ePUWv1yuKUvU+Y0BZuXKl8bVer1f8/PyUTz75xLgsKSlJsbOzU5YsWaIoiqKcOHFCAZR9+/YZt1m7dq2i0WiUq1evmq3s5XXrNRclPDxcAZRLly4Zl9WpU0f5/PPPK7dwlaSoax49erQyaNCgYve5lz/n0nzGgwYNUrp3726y7F7+jIsiNUBmkJ2dTUREBD179jQu02q19OzZk927d1uwZJUnOTkZAA8PD5PlixcvxsvLi+bNmzN16lQyMjIsUbwKc+bMGQICAqhbty4jRowgKioKgIiICHJyckw+88aNG1O7du0q85lnZ2fz888/89RTT5lMGlzVPuOCLly4QGxsrMnn6ubmRlhYmPFz3b17N+7u7rRt29a4Tc+ePdFqtezdu9fsZa4MycnJaDQa3N3dTZZ/+OGHeHp60rp1az755BNyc3MtU8AKsnXrVnx8fGjUqBETJkwgMTHRuK4qf85xcXGsXr2ap59+utC6qvQZy2SoZpCQkIBOp8PX19dkua+vL6dOnbJQqSqPXq/n5ZdfplOnTjRv3ty4fPjw4dSpU4eAgACOHDnCa6+9RmRkJCtWrLBgacsvLCyMhQsX0qhRI2JiYnj33Xfp3Lkzx44dIzY2Fltb20I3CF9fX2JjYy1T4Aq2atUqkpKSGDNmjHFZVfuMb2X47Ir6Wzasi42NxcfHx2S9tbU1Hh4eVeKzz8zM5LXXXmPYsGEmE2W++OKLtGnTBg8PD3bt2sXUqVOJiYlh1qxZFixt+fXt25eHH36Y4OBgzp07xxtvvEG/fv3YvXs3VlZWVfpzXrRoES4uLoWa7KvaZywBkKhwEydO5NixYyb5MIBJ23iLFi3w9/enR48enDt3jnr16pm7mHesX79+xuctW7YkLCyMOnXq8Ouvv+Lg4GDBkpnHDz/8QL9+/QgICDAuq2qfsTCVk5PD448/jqIozJkzx2Td5MmTjc9btmyJra0tzz77LDNnzrwnp1QYOnSo8XmLFi1o2bIl9erVY+vWrfTo0cOCJat88+fPZ8SIEdjb25ssr2qfsTSBmYGXlxdWVlaFegDFxcXh5+dnoVJVjkmTJvH333+zZcsWatWqVeK2YWFhAJw9e9YcRat07u7uNGzYkLNnz+Ln50d2djZJSUkm21SVz/zSpUts2rSJcePGlbhdVfuMDZ9dSX/Lfn5+hTo35Obmcv369Xv6szcEP5cuXWLjxo0mtT9FCQsLIzc3l4sXL5qngJWsbt26eHl5GX+Xq+rn/O+//xIZGXnbv2249z9jCYDMwNbWltDQUDZv3mxcptfr2bx5Mx06dLBgySqOoihMmjSJlStX8s8//xAcHHzbfQ4dOgSAv79/JZfOPNLS0jh37hz+/v6EhoZiY2Nj8plHRkYSFRVVJT7zBQsW4OPjw4ABA0rcrqp9xsHBwfj5+Zl8rikpKezdu9f4uXbo0IGkpCQiIiKM2/zzzz/o9XpjQHivMQQ/Z86cYdOmTXh6et52n0OHDqHVags1E92rrly5QmJiovF3uSp+zqDW7IaGhhISEnLbbe/5z9jSWdjVxdKlSxU7Oztl4cKFyokTJ5Tx48cr7u7uSmxsrKWLViEmTJiguLm5KVu3blViYmKMj4yMDEVRFOXs2bPKjBkzlP379ysXLlxQ/vjjD6Vu3bpKly5dLFzy8nvllVeUrVu3KhcuXFB27typ9OzZU/Hy8lLi4+MVRVGU5557Tqldu7byzz//KPv371c6dOigdOjQwcKlvnM6nU6pXbu28tprr5ksryqfcWpqqnLw4EHl4MGDCqDMmjVLOXjwoLHH04cffqi4u7srf/zxh3LkyBFl0KBBSnBwsHLz5k3jMfr27au0bt1a2bt3r7Jjxw6lQYMGyrBhwyx1SbdV0jVnZ2crDz30kFKrVi3l0KFDJn/fWVlZiqIoyq5du5TPP/9cOXTokHLu3Dnl559/Vry9vZVRo0ZZ+MqKV9I1p6amKlOmTFF2796tXLhwQdm0aZPSpk0bpUGDBkpmZqbxGPfS53y732tFUZTk5GTF0dFRmTNnTqH978XP+HYkADKjr776Sqldu7Zia2urtG/fXtmzZ4+li1RhgCIfCxYsUBRFUaKiopQuXbooHh4eip2dnVK/fn3lv//9r5KcnGzZgt+BJ554QvH391dsbW2VmjVrKk888YRy9uxZ4/qbN28qzz//vFKjRg3F0dFRGTJkiBITE2PBEleM9evXK4ASGRlpsryqfMZbtmwp8nd59OjRiqKoXeHfeustxdfXV7Gzs1N69OhR6L1ITExUhg0bpjg7Oyuurq7K2LFjldTUVAtcTemUdM0XLlwo9u97y5YtiqIoSkREhBIWFqa4ubkp9vb2SpMmTZQPPvjAJFi425R0zRkZGUrv3r0Vb29vxcbGRqlTp47yzDPPFPrCei99zrf7vVYURfnuu+8UBwcHJSkpqdD+9+JnfDsaRVGUSq1iEkIIIYS4y0gOkBBCCCGqHQmAhBBCCFHtSAAkhBBCiGpHAiAhhBBCVDsSAAkhhBCi2pEASAghhBDVjgRAQgghhKh2JAASQohiaDQaVq1aZeliCCEqgQRAQoi70pgxY9BoNIUeffv2tXTRhBBVgLWlCyCEEMXp27cvCxYsMFlmZ2dnodIIIaoSqQESQty17Ozs8PPzM3nUqFEDUJun5syZQ79+/XBwcKBu3br89ttvJvsfPXqU7t274+DggKenJ+PHjyctLc1km/nz59OsWTPs7Ozw9/dn0qRJJusTEhIYMmQIjo6ONGjQgD///NO47saNG4wYMQJvb28cHBxo0KBBoYBNCHF3kgBICHHPeuutt3jkkUc4fPgwI0aMYOjQoZw8eRKA9PR0+vTpQ40aNdi3bx/Lly9n06ZNJgHOnDlzmDhxIuPHj+fo0aP8+eef1K9f3+Qc7777Lo8//jhHjhyhf//+jBgxguvXrxvPf+LECdauXcvJkyeZM2cOXl5e5nsDhBDlZ+nZWIUQoiijR49WrKysFCcnJ5PH//3f/ymKoiiA8txzz5nsExYWpkyYMEFRFEWZN2+eUqNGDSUtLc24fvXq1YpWqzXO6h0QEKC8+eabxZYBUKZNm2Z8nZaWpgDK2rVrFUVRlIEDBypjx46tmAsWQpiV5AAJIe5aDzzwAHPmzDFZ5uHhYXzeoUMHk3UdOnTg0KFDAJw8eZKQkBCcnJyM6zt16oRerycyMhKNRkN0dDQ9evQosQwtW7Y0PndycsLV1ZX4+HgAJkyYwCOPPMKBAwfo3bs3gwcPpmPHjuW6ViGEeUkAJIS4azk5ORVqkqooDg4OpdrOxsbG5LVGo0Gv1wPQr18/Ll26xJo1a9i4cSM9evRg4sSJfPrppxVeXiFExZIcICHEPWvPnj2FXjdp0gSAJk2acPjwYdLT043rd+7ciVarpVGjRri4uBAUFMTmzZvvqAze3t6MHj2an3/+mdmzZzNv3rw7Op4QwjykBkgIcdfKysoiNjbWZJm1tbUx0Xj58uW0bduW+++/n8WLFxMeHs4PP/wAwIgRI5g+fTqjR4/mnXfe4dq1a7zwwguMHDkSX19fAN555x2ee+45fHx86NevH6mpqezcuZMXXnihVOV7++23CQ0NpVmzZmRlZfH3338bAzAhxN1NAiAhxF1r3bp1+Pv7myxr1KgRp06dAtQeWkuXLuX555/H39+fJUuW0LRpUwAcHR1Zv349L730Eu3atcPR0ZFHHnmEWbNmGY81evRoMjMz+fzzz5kyZQpeXl48+uijpS6fra0tU6dO5eLFizg4ONC5c2eWLl1aAVcuhKhsGkVRFEsXQgghykqj0bBy5UoGDx5s6aIIIe5BkgMkhBBCiGpHAiAhhBBCVDuSAySEuCdJ670Q4k5IDZAQQgghqh0JgIQQQghR7UgAJIQQQohqRwIgIYQQQlQ7EgAJIYQQotqRAEiI/2+3DgQAAAAABPlbD3JRBMCOAAEAOwIEAOwIEACwEy7+s7Xqv+NXAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot loss of best model\n",
        "- 16-8-1"
      ],
      "metadata": {
        "id": "zosyiL9pPWeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Training loss vs Validation loss')\n",
        "plt.legend(['training data', 'validation data'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "r160-DZsPTlb",
        "outputId": "46f46dde-b3c3-49b8-e8df-bcd64210c043"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load best model from checkpoint file for manual predictions (GRAD student only work)\n",
        "- Requirement: **build your own function/method that serves as a prediction model**\n",
        "- Verify predictions are the same"
      ],
      "metadata": {
        "id": "ZAGig919Wh8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.saving import load_model\n",
        "best_model = load_model('model.keras')"
      ],
      "metadata": {
        "id": "wTXz2XCnVau5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check it is the 16-8-1 model\n",
        "best_model.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dpyoeoclVg7Y",
        "outputId": "e00fe1e0-3faa-4595-b99a-b70236925fda"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'16x8x1_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the summary\n",
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu9weFyNYQYy",
        "outputId": "8e6aabb1-6245-4b85-9afa-4c9fb2821609"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"16x8x1_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_23 (Dense)            (None, 16)                192       \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 337 (1.32 KB)\n",
            "Trainable params: 337 (1.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gives us the shape of the weights (11,16) for the weight kernel and (16,) for the bias\n",
        "# which makes sense since the layer is 16 neurons and input is 11 features\n",
        "best_model.layers[0].weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhfc6jDUYYRW",
        "outputId": "43261f69-8a62-422f-e6a0-017cce9aca47"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_23/kernel:0' shape=(11, 16) dtype=float32, numpy=\n",
              " array([[-0.32, -0.19,  0.33, -0.92, -0.14,  0.05,  0.09,  0.13,  0.15,\n",
              "          0.41, -0.13,  0.52,  0.00, -0.54, -0.26, -0.16],\n",
              "        [-0.34,  0.46,  0.46,  0.02,  0.43,  0.21, -0.10, -0.10,  0.15,\n",
              "         -0.34,  0.32, -0.61,  0.21, -0.29, -0.15, -0.28],\n",
              "        [-0.62,  0.46, -0.71, -0.16, -0.10,  0.15, -0.03,  1.01,  0.01,\n",
              "          0.15, -0.11,  0.19, -0.81,  0.02, -0.29,  0.33],\n",
              "        [-0.47,  0.05, -0.26, -0.15,  0.12,  0.07,  0.47,  0.07,  0.20,\n",
              "         -0.15, -0.91, -0.11,  0.26, -0.26,  0.05, -0.39],\n",
              "        [ 0.01, -0.06, -0.72, -0.66, -0.44,  0.62,  0.11, -0.42, -0.02,\n",
              "          0.07,  0.02,  0.40, -0.08,  0.11, -0.31, -0.80],\n",
              "        [-0.42,  0.33, -0.07,  0.81, -0.05, -0.14, -0.34, -0.07, -0.18,\n",
              "         -0.45, -0.23,  0.08, -0.24,  0.46, -0.77,  0.69],\n",
              "        [ 0.18,  0.04,  0.64,  0.17,  0.06,  0.03,  0.25, -0.29,  0.53,\n",
              "         -0.52, -0.18,  0.38, -0.26, -0.17, -0.31, -0.18],\n",
              "        [ 0.41,  0.13,  1.12, -0.28,  0.39, -0.11,  0.56,  0.34, -0.52,\n",
              "         -0.07, -0.10,  0.29,  0.89, -0.30, -0.32, -0.35],\n",
              "        [ 0.22, -0.07, -0.56, -0.69, -0.10,  0.19, -0.46,  0.22,  0.30,\n",
              "         -0.32, -0.49, -1.03, -0.43,  0.45, -0.06, -0.16],\n",
              "        [ 0.16,  0.72,  0.10,  0.04,  0.08,  0.36,  0.34,  0.39,  0.75,\n",
              "          0.13,  0.93, -0.74, -0.89, -0.32,  0.19, -0.84],\n",
              "        [ 0.17,  1.03, -0.50, -0.22,  0.84,  0.18,  0.24,  0.31, -0.11,\n",
              "         -0.73, -0.20, -0.74, -0.34, -0.83, -0.54, -0.23]], dtype=float32)>,\n",
              " <tf.Variable 'dense_23/bias:0' shape=(16,) dtype=float32, numpy=\n",
              " array([-0.39,  0.25,  0.03,  0.19,  0.21,  0.39,  0.15,  0.04, -0.02,\n",
              "         0.14, -0.17,  0.17, -0.13,  0.36,  0.16, -0.07], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import activations"
      ],
      "metadata": {
        "id": "F9A-G3XbUH20"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing cell to see what various methods do\n",
        "len(best_model.layers[0].get_weights()[0].T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JIq9qDLsOs8",
        "outputId": "444da796-3744-4b7f-e3d3-b1d4557ee4a2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from Keras docs: output = activation(dot(input, kernel) + bias)\n",
        "# used in hidden layers\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "# used in output layer\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def my_prediction_function(model, data):\n",
        "  # this will be mutated between layers and end up as output\n",
        "  output = data\n",
        "\n",
        "  # iterate over the layers\n",
        "  for keras_layer in model.layers:\n",
        "    # transpose the weights and bias of current layer\n",
        "    weights = keras_layer.get_weights()[0].T\n",
        "    bias_list = keras_layer.get_weights()[1].T\n",
        "\n",
        "    temporary_values = []\n",
        "    for i in range(len(output)):\n",
        "      neuron_outputs = [] # list to store neuron outputs\n",
        "      for j in range(len(weights)):\n",
        "        pre_activation = np.dot(output[i], weights[j]) + bias_list[j]\n",
        "\n",
        "        # apply activatoin functions\n",
        "        if keras_layer.activation == activations.sigmoid:\n",
        "          activated_output = sigmoid(pre_activation)\n",
        "        elif keras_layer.activation == activations.relu:\n",
        "          activated_output = relu(pre_activation)\n",
        "        else:\n",
        "          activated_output = pre_activation # just linear without activation\n",
        "\n",
        "        neuron_outputs.append(activated_output)\n",
        "\n",
        "      temporary_values.append(neuron_outputs)\n",
        "    # input for the next layer\n",
        "    output = np.array(temporary_values)\n",
        "  return output\n",
        "\n"
      ],
      "metadata": {
        "id": "cFqaQn6uXhtA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the manual prediction function"
      ],
      "metadata": {
        "id": "lgYIabNni3Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manually using the weights and bias from the model to predict\n",
        "my_prediction_function(best_model, XVALID)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oYvLHaIuggV",
        "outputId": "77967ea7-6688-4ec5-b2d8-0cba89b2f6cc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.94],\n",
              "       [ 0.18],\n",
              "       [ 0.78],\n",
              "       [ 1.00],\n",
              "       [ 0.95]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual model predictions\n",
        "predictions = best_model.predict(XVALID)\n",
        "predictions[:5].T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx2iy_05N5Do",
        "outputId": "800090d3-5bba-4507-d974-2f5ab0bb1aab"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.94,  0.18,  0.78,  1.00,  0.95]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual values from dataset\n",
        "true_values = np_data[:, -1]\n",
        "true_values[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fh_rhNZZQ2l",
        "outputId": "65c3fb03-2964-4603-9e8d-af66c444bc38"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.00,  1.00,  1.00,  1.00,  1.00])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XVALID.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Sc9_1xyMqU",
        "outputId": "462b69b8-1a07-474b-9715-9d986239f462"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4 - Feature importance and reduction\n",
        "- train models where each model only receives one feature at a time"
      ],
      "metadata": {
        "id": "CGxdSJAYw2pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "print(XTRAIN.shape, YTRAIN.shape)\n",
        "print(XVALID.shape, YVALID.shape)\n",
        "print(best_model.name)\n",
        "print(XTRAIN[:, 1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7hkH7hzxYNF",
        "outputId": "1eb2993b-402f-477e-86ad-5a6549c7c58e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(952, 11) (952,)\n",
            "(238, 11) (238,)\n",
            "16x8x1_model\n",
            "(952,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol', 'fasting blood sugar',\n",
        "# 'resting ecg', 'max heart rate', 'exercise angina', 'oldpeak', 'ST slope', 'target']"
      ],
      "metadata": {
        "id": "imZlHPi51K8u"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model checkpoint callback"
      ],
      "metadata": {
        "id": "oSEsldAQ2GnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint(filepath = 'feature_model.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')"
      ],
      "metadata": {
        "id": "JpxHL6MD15FR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Age model - 16x8x1\n",
        "- val_accuracy: 0.6471\n",
        "- val_loss: 0.6520\n",
        "- val_precision: 0.6567\n",
        "- val_recall: 0.6984"
      ],
      "metadata": {
        "id": "u7M5AKv-2Kvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_model = Sequential(name=\"age_model\")\n",
        "age_model.add(Input(shape=(1,)))\n",
        "age_model.add(Dense(16, activation='relu'))\n",
        "age_model.add(Dense(8, activation='relu'))\n",
        "age_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "age_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "age_history = age_model.fit(XTRAIN[:, 0], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 0], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QWuoZwk80sQP",
        "outputId": "21d951d8-99de-4903-8c16-b1d162082c32"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6933 - accuracy: 0.3750 - precision: 0.7560 - recall: 0.8699\n",
            "Epoch 1: val_loss improved from inf to 0.67230, saving model to feature_model.keras\n",
            "30/30 [==============================] - 1s 11ms/step - loss: 0.6700 - accuracy: 0.6334 - precision: 0.6846 - recall: 0.7075 - val_loss: 0.6723 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6842 - accuracy: 0.6875 - precision: 0.7778 - recall: 0.7000\n",
            "Epoch 2: val_loss improved from 0.67230 to 0.67065, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6707 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6255 - accuracy: 0.6875 - precision: 0.7368 - recall: 0.7368\n",
            "Epoch 3: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6707 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6521 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.6667\n",
            "Epoch 4: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6711 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6076 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.7143\n",
            "Epoch 5: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6524 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6717 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6370 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.6667\n",
            "Epoch 6: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6518 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6725 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6864 - accuracy: 0.5625 - precision: 0.5882 - recall: 0.5882\n",
            "Epoch 7: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6728 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5826 - accuracy: 0.7188 - precision: 0.7368 - recall: 0.7778\n",
            "Epoch 8: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6730 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6529 - accuracy: 0.6250 - precision: 0.5625 - recall: 0.6429\n",
            "Epoch 9: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6728 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6701 - accuracy: 0.6562 - precision: 0.7647 - recall: 0.6500\n",
            "Epoch 10: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6728 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.5625 - precision: 0.6500 - recall: 0.6500\n",
            "Epoch 11: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6507 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6730 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6281 - accuracy: 0.6875 - precision: 0.6500 - recall: 0.8125\n",
            "Epoch 12: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6730 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6938 - accuracy: 0.5938 - precision: 0.6316 - recall: 0.6667\n",
            "Epoch 13: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6724 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6342 - accuracy: 0.6250 - precision: 0.6364 - recall: 0.7778\n",
            "Epoch 14: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6721 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5767 - accuracy: 0.6875 - precision: 0.8095 - recall: 0.7391\n",
            "Epoch 15: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6718 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6236 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 16: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6719 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6382 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.7059\n",
            "Epoch 17: val_loss did not improve from 0.67065\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.6355 - precision: 0.6429 - recall: 0.6727 - val_loss: 0.6713 - val_accuracy: 0.6092 - val_precision: 0.6519 - val_recall: 0.6567\n",
            "Epoch 17: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sex\n",
        "- val_accuracy: 0.6849\n",
        "- val_loss: 0.6165\n",
        "- val_precision: 0.6441\n",
        "- val_recall: 0.9048"
      ],
      "metadata": {
        "id": "a9fxyA9g3CxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sex_model = Sequential(name=\"sex_model\")\n",
        "sex_model.add(Input(shape=(1,)))\n",
        "sex_model.add(Dense(16, activation='relu'))\n",
        "sex_model.add(Dense(8, activation='relu'))\n",
        "sex_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "sex_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "sex_history = sex_model.fit(XTRAIN[:, 1], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 1], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WZn62eeP26TE",
        "outputId": "58cfa307-a17b-439b-d47f-9b605e382cb8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 17s - loss: 0.6515 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.6928\n",
            "Epoch 1: val_loss improved from 0.67065 to 0.65250, saving model to feature_model.keras\n",
            "30/30 [==============================] - 1s 12ms/step - loss: 0.6651 - accuracy: 0.6439 - precision: 0.6139 - recall: 0.8442 - val_loss: 0.6525 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6528 - accuracy: 0.6562 - precision: 0.6296 - recall: 0.9444\n",
            "Epoch 2: val_loss improved from 0.65250 to 0.64654, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6520 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6465 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6585 - accuracy: 0.6250 - precision: 0.6000 - recall: 0.8824\n",
            "Epoch 3: val_loss improved from 0.64654 to 0.64363, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6436 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6478 - accuracy: 0.5938 - precision: 0.4500 - recall: 0.8182\n",
            "Epoch 4: val_loss improved from 0.64363 to 0.64354, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6435 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6435 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5796 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.9524\n",
            "Epoch 5: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6443 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5990 - accuracy: 0.6875 - precision: 0.6154 - recall: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6446 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6090 - accuracy: 0.7188 - precision: 0.7000 - recall: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6454 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6251 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.8235\n",
            "Epoch 8: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6461 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6460 - accuracy: 0.5625 - precision: 0.4615 - recall: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6458 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6353 - accuracy: 0.5938 - precision: 0.4783 - recall: 0.9167\n",
            "Epoch 10: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6458 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6620 - accuracy: 0.5625 - precision: 0.5000 - recall: 1.0000\n",
            "Epoch 11: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6449 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7082 - accuracy: 0.5625 - precision: 0.5714 - recall: 0.8889\n",
            "Epoch 12: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6462 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6357 - accuracy: 0.6562 - precision: 0.6296 - recall: 0.9444\n",
            "Epoch 13: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6459 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6584 - accuracy: 0.6250 - precision: 0.6000 - recall: 0.8824\n",
            "Epoch 14: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6461 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6498 - accuracy: 0.6562 - precision: 0.6552 - recall: 0.9500\n",
            "Epoch 15: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6466 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6554 - accuracy: 0.5938 - precision: 0.5385 - recall: 0.9333\n",
            "Epoch 16: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6471 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6529 - accuracy: 0.6562 - precision: 0.6538 - recall: 0.8947\n",
            "Epoch 17: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6465 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6088 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.9565\n",
            "Epoch 18: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6460 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5630 - accuracy: 0.6875 - precision: 0.5455 - recall: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.6439 - precision: 0.6068 - recall: 0.8949 - val_loss: 0.6468 - val_accuracy: 0.6597 - val_precision: 0.6480 - val_recall: 0.8657\n",
            "Epoch 19: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chest pain type\n",
        "- val_accuracy: 0.7605\n",
        "- val_loss: 0.5380\n",
        "- val_precision: 0.7556\n",
        "- val_recall: 0.8095"
      ],
      "metadata": {
        "id": "U-c9evac3GqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chest_model = Sequential(name=\"chest_model\")\n",
        "chest_model.add(Input(shape=(1,)))\n",
        "chest_model.add(Dense(16, activation='relu'))\n",
        "chest_model.add(Dense(8, activation='relu'))\n",
        "chest_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "chest_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "chest_history = chest_model.fit(XTRAIN[:, 2], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 2], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y8gPTMGb26zQ",
        "outputId": "c52b1be5-5673-4920-a8fd-56031e8e1f6e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7031 - accuracy: 0.5312 - precision: 0.6480 - recall: 0.7785\n",
            "Epoch 1: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6821 - accuracy: 0.4800 - precision: 0.6480 - recall: 0.1844 - val_loss: 0.6759 - val_accuracy: 0.4370 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6744 - accuracy: 0.5312 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 2: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.6008 - precision: 0.7700 - recall: 0.3313 - val_loss: 0.6644 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6369 - accuracy: 0.8125 - precision: 0.7692 - recall: 0.7692\n",
            "Epoch 3: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6575 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6547 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6609 - accuracy: 0.6250 - precision: 0.5882 - recall: 0.6667\n",
            "Epoch 4: val_loss did not improve from 0.64354\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6485 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6457 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6555 - accuracy: 0.6562 - precision: 0.6429 - recall: 0.6000\n",
            "Epoch 5: val_loss improved from 0.64354 to 0.63860, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6407 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6386 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6144 - accuracy: 0.7188 - precision: 0.7368 - recall: 0.7778\n",
            "Epoch 6: val_loss improved from 0.63860 to 0.63261, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6350 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6326 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7041 - accuracy: 0.5938 - precision: 0.5882 - recall: 0.6250\n",
            "Epoch 7: val_loss improved from 0.63261 to 0.62736, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6301 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6274 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6464 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 8: val_loss improved from 0.62736 to 0.62239, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6224 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7024 - accuracy: 0.7188 - precision: 0.8125 - recall: 0.6842\n",
            "Epoch 9: val_loss improved from 0.62239 to 0.61756, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6220 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6176 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6330 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.8182\n",
            "Epoch 10: val_loss improved from 0.61756 to 0.61356, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6186 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6136 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5962 - accuracy: 0.7812 - precision: 0.7000 - recall: 0.9333\n",
            "Epoch 11: val_loss improved from 0.61356 to 0.61003, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6158 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6100 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5016 - accuracy: 0.8125 - precision: 0.6667 - recall: 0.9091\n",
            "Epoch 12: val_loss improved from 0.61003 to 0.60653, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6132 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6065 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6521 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 13: val_loss improved from 0.60653 to 0.60308, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6106 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6031 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5823 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 14: val_loss improved from 0.60308 to 0.60017, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6084 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.6002 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6096 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 15: val_loss improved from 0.60017 to 0.59709, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6061 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5971 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5970 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 16: val_loss improved from 0.59709 to 0.59401, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6040 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5940 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6552 - accuracy: 0.7500 - precision: 0.7895 - recall: 0.7895\n",
            "Epoch 17: val_loss improved from 0.59401 to 0.59053, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6015 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5905 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5892 - accuracy: 0.7812 - precision: 0.9412 - recall: 0.7273\n",
            "Epoch 18: val_loss improved from 0.59053 to 0.58765, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5994 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5877 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5800 - accuracy: 0.7188 - precision: 0.5625 - recall: 0.8182\n",
            "Epoch 19: val_loss improved from 0.58765 to 0.58486, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5973 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5849 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6855 - accuracy: 0.6875 - precision: 0.7059 - recall: 0.7059\n",
            "Epoch 20: val_loss improved from 0.58486 to 0.58217, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5955 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5822 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5566 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 21: val_loss improved from 0.58217 to 0.57966, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5797 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6434 - accuracy: 0.7500 - precision: 0.8125 - recall: 0.7222\n",
            "Epoch 22: val_loss improved from 0.57966 to 0.57720, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5919 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5772 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6276 - accuracy: 0.7188 - precision: 0.7333 - recall: 0.6875\n",
            "Epoch 23: val_loss improved from 0.57720 to 0.57454, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5902 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5745 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4975 - accuracy: 0.8750 - precision: 0.9412 - recall: 0.8421\n",
            "Epoch 24: val_loss improved from 0.57454 to 0.57245, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5725 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5171 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 25: val_loss improved from 0.57245 to 0.57011, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5870 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5701 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6447 - accuracy: 0.6250 - precision: 0.6190 - recall: 0.7647\n",
            "Epoch 26: val_loss improved from 0.57011 to 0.56754, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5854 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5675 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4892 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 27: val_loss improved from 0.56754 to 0.56538, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5654 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6700 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.7500\n",
            "Epoch 28: val_loss improved from 0.56538 to 0.56300, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5825 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5630 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5305 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 29: val_loss improved from 0.56300 to 0.56101, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5610 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6706 - accuracy: 0.7188 - precision: 0.8125 - recall: 0.6842\n",
            "Epoch 30: val_loss improved from 0.56101 to 0.55925, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5799 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5593 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6593 - accuracy: 0.6250 - precision: 0.6087 - recall: 0.8235\n",
            "Epoch 31: val_loss improved from 0.55925 to 0.55720, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5786 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5572 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5589 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 32: val_loss improved from 0.55720 to 0.55541, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5774 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5554 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5333 - accuracy: 0.8125 - precision: 0.9286 - recall: 0.7222\n",
            "Epoch 33: val_loss improved from 0.55541 to 0.55380, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5764 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5538 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5212 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 34: val_loss improved from 0.55380 to 0.55231, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5754 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5523 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5528 - accuracy: 0.7188 - precision: 0.6471 - recall: 0.7857\n",
            "Epoch 35: val_loss improved from 0.55231 to 0.55082, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5745 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5508 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7500\n",
            "Epoch 36: val_loss improved from 0.55082 to 0.54910, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5736 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5491 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5106 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.8182\n",
            "Epoch 37: val_loss improved from 0.54910 to 0.54759, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5476 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5282 - accuracy: 0.7812 - precision: 0.8824 - recall: 0.7500\n",
            "Epoch 38: val_loss improved from 0.54759 to 0.54606, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5716 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5461 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5270 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 39: val_loss improved from 0.54606 to 0.54466, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5706 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5447 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4863 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8421\n",
            "Epoch 40: val_loss improved from 0.54466 to 0.54289, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5698 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5429 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6465 - accuracy: 0.6562 - precision: 0.7647 - recall: 0.6500\n",
            "Epoch 41: val_loss improved from 0.54289 to 0.54142, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5689 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5414 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6865 - accuracy: 0.6250 - precision: 0.7222 - recall: 0.6500\n",
            "Epoch 42: val_loss improved from 0.54142 to 0.54022, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5681 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5402 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5325 - accuracy: 0.7812 - precision: 0.6667 - recall: 1.0000\n",
            "Epoch 43: val_loss improved from 0.54022 to 0.53906, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5674 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5391 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4459 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 44: val_loss improved from 0.53906 to 0.53781, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5668 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5378 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6076 - accuracy: 0.6562 - precision: 0.5625 - recall: 0.6923\n",
            "Epoch 45: val_loss improved from 0.53781 to 0.53666, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5661 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5367 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5835 - accuracy: 0.7500 - precision: 0.6316 - recall: 0.9231\n",
            "Epoch 46: val_loss improved from 0.53666 to 0.53560, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5654 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5356 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5996 - accuracy: 0.7500 - precision: 0.8125 - recall: 0.7222\n",
            "Epoch 47: val_loss improved from 0.53560 to 0.53457, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5649 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5346 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5230 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 48: val_loss improved from 0.53457 to 0.53340, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5642 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5334 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5261 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.7895\n",
            "Epoch 49: val_loss improved from 0.53340 to 0.53248, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5325 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5884 - accuracy: 0.7188 - precision: 0.6818 - recall: 0.8824\n",
            "Epoch 50: val_loss improved from 0.53248 to 0.53147, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5632 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5315 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5358 - accuracy: 0.8125 - precision: 0.8636 - recall: 0.8636\n",
            "Epoch 51: val_loss improved from 0.53147 to 0.53065, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5625 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5306 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5145 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 52: val_loss improved from 0.53065 to 0.52984, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5619 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5298 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4554 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 53: val_loss improved from 0.52984 to 0.52882, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5616 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5288 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5658 - accuracy: 0.7812 - precision: 1.0000 - recall: 0.6957\n",
            "Epoch 54: val_loss did not improve from 0.52882\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5303 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4982 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.9375\n",
            "Epoch 55: val_loss improved from 0.52882 to 0.52701, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5608 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5270 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5991 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 56: val_loss improved from 0.52701 to 0.52633, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5604 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5263 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5553 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 57: val_loss improved from 0.52633 to 0.52556, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5600 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5256 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5425 - accuracy: 0.7500 - precision: 0.8421 - recall: 0.7619\n",
            "Epoch 58: val_loss improved from 0.52556 to 0.52502, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5250 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5871 - accuracy: 0.7188 - precision: 0.8000 - recall: 0.6667\n",
            "Epoch 59: val_loss improved from 0.52502 to 0.52416, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5592 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5242 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5143 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 60: val_loss improved from 0.52416 to 0.52346, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5235 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5336 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 61: val_loss did not improve from 0.52346\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5244 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4372 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 62: val_loss improved from 0.52346 to 0.52208, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5221 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5191 - accuracy: 0.7812 - precision: 0.9333 - recall: 0.7000\n",
            "Epoch 63: val_loss improved from 0.52208 to 0.52163, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5579 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5216 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4539 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 64: val_loss improved from 0.52163 to 0.52106, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5578 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5211 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5944 - accuracy: 0.6875 - precision: 0.5714 - recall: 0.6667\n",
            "Epoch 65: val_loss improved from 0.52106 to 0.52056, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5574 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5206 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5360 - accuracy: 0.7812 - precision: 0.7222 - recall: 0.8667\n",
            "Epoch 66: val_loss improved from 0.52056 to 0.52020, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5572 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5202 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5781 - accuracy: 0.7500 - precision: 0.8125 - recall: 0.7222\n",
            "Epoch 67: val_loss improved from 0.52020 to 0.51953, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5570 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5195 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6178 - accuracy: 0.7188 - precision: 0.6429 - recall: 0.6923\n",
            "Epoch 68: val_loss improved from 0.51953 to 0.51901, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5567 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5190 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6032 - accuracy: 0.6875 - precision: 0.8333 - recall: 0.6818\n",
            "Epoch 69: val_loss improved from 0.51901 to 0.51859, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5565 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5186 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4549 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8182\n",
            "Epoch 70: val_loss improved from 0.51859 to 0.51804, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5560 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5180 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6327 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.6667\n",
            "Epoch 71: val_loss improved from 0.51804 to 0.51789, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5559 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5179 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4914 - accuracy: 0.8125 - precision: 0.9048 - recall: 0.8261\n",
            "Epoch 72: val_loss improved from 0.51789 to 0.51733, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5555 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5173 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4543 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 73: val_loss improved from 0.51733 to 0.51660, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5166 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5219 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 74: val_loss improved from 0.51660 to 0.51634, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5553 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5163 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6408 - accuracy: 0.6875 - precision: 0.6364 - recall: 0.8750\n",
            "Epoch 75: val_loss improved from 0.51634 to 0.51593, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5552 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5159 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6066 - accuracy: 0.6875 - precision: 0.7333 - recall: 0.6471\n",
            "Epoch 76: val_loss improved from 0.51593 to 0.51551, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5549 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5155 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6796 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.7059\n",
            "Epoch 77: val_loss improved from 0.51551 to 0.51498, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5549 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5150 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4967 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 78: val_loss improved from 0.51498 to 0.51482, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5544 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5148 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5539 - accuracy: 0.7500 - precision: 0.9231 - recall: 0.6316\n",
            "Epoch 79: val_loss improved from 0.51482 to 0.51425, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5544 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5142 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6711 - accuracy: 0.6250 - precision: 0.5500 - recall: 0.7857\n",
            "Epoch 80: val_loss improved from 0.51425 to 0.51402, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5545 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5140 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4576 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 81: val_loss improved from 0.51402 to 0.51361, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5540 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5136 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5900 - accuracy: 0.6875 - precision: 0.5833 - recall: 0.5833\n",
            "Epoch 82: val_loss improved from 0.51361 to 0.51348, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5541 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5135 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6421 - accuracy: 0.6875 - precision: 0.6429 - recall: 0.6429\n",
            "Epoch 83: val_loss improved from 0.51348 to 0.51314, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5541 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5131 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5818 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 84: val_loss improved from 0.51314 to 0.51279, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5539 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5128 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5213 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 85: val_loss improved from 0.51279 to 0.51241, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5537 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5124 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7701 - accuracy: 0.5625 - precision: 0.5789 - recall: 0.6471\n",
            "Epoch 86: val_loss improved from 0.51241 to 0.51218, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5122 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5081 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 87: val_loss improved from 0.51218 to 0.51194, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5119 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5109 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.7895\n",
            "Epoch 88: val_loss improved from 0.51194 to 0.51160, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5116 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5673 - accuracy: 0.7500 - precision: 0.6471 - recall: 0.8462\n",
            "Epoch 89: val_loss improved from 0.51160 to 0.51143, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5534 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5114 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5333 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 90: val_loss improved from 0.51143 to 0.51122, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5530 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5112 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5151 - accuracy: 0.7812 - precision: 0.7222 - recall: 0.8667\n",
            "Epoch 91: val_loss improved from 0.51122 to 0.51110, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5111 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4106 - accuracy: 0.8750 - precision: 0.9091 - recall: 0.7692\n",
            "Epoch 92: val_loss improved from 0.51110 to 0.51097, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5530 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5110 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3752 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8667\n",
            "Epoch 93: val_loss improved from 0.51097 to 0.51065, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5528 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5107 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4444 - accuracy: 0.8438 - precision: 0.7333 - recall: 0.9167\n",
            "Epoch 94: val_loss improved from 0.51065 to 0.51044, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5529 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5104 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5778 - accuracy: 0.7188 - precision: 0.6429 - recall: 0.6923\n",
            "Epoch 95: val_loss improved from 0.51044 to 0.51025, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5529 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5103 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4553 - accuracy: 0.8438 - precision: 0.9286 - recall: 0.7647\n",
            "Epoch 96: val_loss did not improve from 0.51025\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5528 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5104 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5628 - accuracy: 0.7500 - precision: 0.7692 - recall: 0.6667\n",
            "Epoch 97: val_loss improved from 0.51025 to 0.50981, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5527 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5098 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5281 - accuracy: 0.7500 - precision: 0.8125 - recall: 0.7222\n",
            "Epoch 98: val_loss improved from 0.50981 to 0.50967, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5526 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5097 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5209 - accuracy: 0.7812 - precision: 0.8421 - recall: 0.8000\n",
            "Epoch 99: val_loss improved from 0.50967 to 0.50946, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5525 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5095 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5146 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 100: val_loss improved from 0.50946 to 0.50927, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5527 - accuracy: 0.7500 - precision: 0.7586 - recall: 0.7616 - val_loss: 0.5093 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resting bp\n",
        "- val_accuracy: 0.5882\n",
        "- val_loss: 0.6704\n",
        "- val_precision: 0.6400\n",
        "- val_recall: 0.5079"
      ],
      "metadata": {
        "id": "5WAj1a-o3JiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bp_model = Sequential(name=\"bp_model\")\n",
        "bp_model.add(Input(shape=(1,)))\n",
        "bp_model.add(Dense(16, activation='relu'))\n",
        "bp_model.add(Dense(8, activation='relu'))\n",
        "bp_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "bp_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "bp_history = bp_model.fit(XTRAIN[:, 3], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 3], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vc5Z7Pyn27FC",
        "outputId": "2928bca5-1a5c-46d9-d422-e9d3b7a6f8d0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 17s - loss: 0.7028 - accuracy: 0.4375 - precision: 0.8281 - recall: 0.6974\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6957 - accuracy: 0.5221 - precision: 0.6722 - recall: 0.3847 - val_loss: 0.6925 - val_accuracy: 0.5546 - val_precision: 0.6014 - val_recall: 0.6194\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6885 - accuracy: 0.5312 - precision: 0.4762 - recall: 0.7143\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5242 - precision: 0.5371 - recall: 0.6141 - val_loss: 0.6872 - val_accuracy: 0.5546 - val_precision: 0.6014 - val_recall: 0.6194\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6689 - accuracy: 0.6875 - precision: 0.6923 - recall: 0.6000\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5284 - precision: 0.5402 - recall: 0.6242 - val_loss: 0.6834 - val_accuracy: 0.5630 - val_precision: 0.6071 - val_recall: 0.6343\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6639 - accuracy: 0.6875 - precision: 0.7368 - recall: 0.7368\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5315 - precision: 0.5423 - recall: 0.6343 - val_loss: 0.6813 - val_accuracy: 0.5630 - val_precision: 0.6071 - val_recall: 0.6343\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7059 - accuracy: 0.5000 - precision: 0.6000 - recall: 0.6000\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6854 - accuracy: 0.5315 - precision: 0.5420 - recall: 0.6384 - val_loss: 0.6804 - val_accuracy: 0.5630 - val_precision: 0.6071 - val_recall: 0.6343\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6537 - accuracy: 0.6875 - precision: 0.5556 - recall: 0.8333\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5305 - precision: 0.5423 - recall: 0.6222 - val_loss: 0.6798 - val_accuracy: 0.5630 - val_precision: 0.6071 - val_recall: 0.6343\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6754 - accuracy: 0.5938 - precision: 0.6000 - recall: 0.7059\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5315 - precision: 0.5423 - recall: 0.6343 - val_loss: 0.6790 - val_accuracy: 0.5546 - val_precision: 0.6014 - val_recall: 0.6194\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7053 - accuracy: 0.4375 - precision: 0.4286 - recall: 0.3750\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5294 - precision: 0.5422 - recall: 0.6101 - val_loss: 0.6786 - val_accuracy: 0.5546 - val_precision: 0.6014 - val_recall: 0.6194\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6748 - accuracy: 0.5312 - precision: 0.5238 - recall: 0.6875\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.5378 - precision: 0.5524 - recall: 0.5859 - val_loss: 0.6782 - val_accuracy: 0.5546 - val_precision: 0.6014 - val_recall: 0.6194\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6924 - accuracy: 0.4688 - precision: 0.5238 - recall: 0.6111\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.5347 - precision: 0.5476 - recall: 0.6040 - val_loss: 0.6782 - val_accuracy: 0.5630 - val_precision: 0.6389 - val_recall: 0.5149\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6431 - accuracy: 0.7500 - precision: 0.7692 - recall: 0.6667\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.5525 - precision: 0.5720 - recall: 0.5535 - val_loss: 0.6781 - val_accuracy: 0.5630 - val_precision: 0.6389 - val_recall: 0.5149\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6738 - accuracy: 0.5938 - precision: 0.6000 - recall: 0.4000\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.5536 - precision: 0.5785 - recall: 0.5212 - val_loss: 0.6776 - val_accuracy: 0.5630 - val_precision: 0.6389 - val_recall: 0.5149\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6892 - accuracy: 0.4062 - precision: 0.4545 - recall: 0.2778\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5494 - precision: 0.5750 - recall: 0.5111 - val_loss: 0.6774 - val_accuracy: 0.5630 - val_precision: 0.6389 - val_recall: 0.5149\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7225 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.5000\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6839 - accuracy: 0.5525 - precision: 0.5789 - recall: 0.5111 - val_loss: 0.6774 - val_accuracy: 0.5546 - val_precision: 0.6321 - val_recall: 0.5000\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6972 - accuracy: 0.3750 - precision: 0.4737 - recall: 0.4737\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5494 - precision: 0.5778 - recall: 0.4949 - val_loss: 0.6772 - val_accuracy: 0.5546 - val_precision: 0.6321 - val_recall: 0.5000\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6490 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.5546 - precision: 0.5839 - recall: 0.4990 - val_loss: 0.6773 - val_accuracy: 0.5546 - val_precision: 0.6321 - val_recall: 0.5000\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7221 - accuracy: 0.4062 - precision: 0.2500 - recall: 0.2308\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5536 - precision: 0.5810 - recall: 0.5071 - val_loss: 0.6777 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7182 - accuracy: 0.4375 - precision: 0.3000 - recall: 0.2143\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5578 - precision: 0.5873 - recall: 0.5030 - val_loss: 0.6778 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6714 - accuracy: 0.6562 - precision: 0.7273 - recall: 0.5000\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.5546 - precision: 0.5851 - recall: 0.4929 - val_loss: 0.6776 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.5000 - precision: 0.7273 - recall: 0.3810\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6835 - accuracy: 0.5546 - precision: 0.5851 - recall: 0.4929 - val_loss: 0.6775 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6642 - accuracy: 0.5938 - precision: 0.8182 - recall: 0.4500\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5546 - precision: 0.5851 - recall: 0.4929 - val_loss: 0.6780 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6766 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5536 - precision: 0.5845 - recall: 0.4889 - val_loss: 0.6777 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6527 - accuracy: 0.5938 - precision: 0.6875 - recall: 0.5789\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5546 - precision: 0.5855 - recall: 0.4909 - val_loss: 0.6786 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6523 - accuracy: 0.5938 - precision: 0.6923 - recall: 0.5000\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5536 - precision: 0.5845 - recall: 0.4889 - val_loss: 0.6788 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6673 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.5000\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.5557 - precision: 0.5874 - recall: 0.4889 - val_loss: 0.6789 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6847 - accuracy: 0.4688 - precision: 0.6875 - recall: 0.4783\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6835 - accuracy: 0.5546 - precision: 0.5864 - recall: 0.4869 - val_loss: 0.6782 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7011 - accuracy: 0.4688 - precision: 0.4000 - recall: 0.4286\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5546 - precision: 0.5860 - recall: 0.4889 - val_loss: 0.6782 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7300 - accuracy: 0.4375 - precision: 0.2500 - recall: 0.2500\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5546 - precision: 0.5864 - recall: 0.4869 - val_loss: 0.6779 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6904 - accuracy: 0.6250 - precision: 0.5000 - recall: 0.4167\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.5546 - precision: 0.5864 - recall: 0.4869 - val_loss: 0.6778 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6866 - accuracy: 0.4688 - precision: 0.4615 - recall: 0.3750\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5546 - precision: 0.5864 - recall: 0.4869 - val_loss: 0.6779 - val_accuracy: 0.5546 - val_precision: 0.6429 - val_recall: 0.4701\n",
            "Epoch 30: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cholesterol\n",
        "- val_accuracy: 0.5798\n",
        "- val_loss: 0.6686\n",
        "- val_precision: 0.6806\n",
        "- val_recall: 0.3889"
      ],
      "metadata": {
        "id": "V7MTHUql3MIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ch_model = Sequential(name=\"ch_model\")\n",
        "ch_model.add(Input(shape=(1,)))\n",
        "ch_model.add(Dense(16, activation='relu'))\n",
        "ch_model.add(Dense(8, activation='relu'))\n",
        "ch_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "ch_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "ch_history = ch_model.fit(XTRAIN[:, 4], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 4], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "STbMoRCR27Wa",
        "outputId": "e8a23abb-d4bf-4fec-86e7-cff9e32fdae9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7700 - accuracy: 0.4688 - precision: 0.6429 - recall: 0.4172\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7238 - accuracy: 0.4559 - precision: 0.5117 - recall: 0.1733 - val_loss: 0.7158 - val_accuracy: 0.3782 - val_precision: 0.3750 - val_recall: 0.1567\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7136 - accuracy: 0.2812 - precision: 0.2857 - recall: 0.1000\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7003 - accuracy: 0.4149 - precision: 0.4176 - recall: 0.3172 - val_loss: 0.6960 - val_accuracy: 0.5630 - val_precision: 0.6172 - val_recall: 0.5896\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7064 - accuracy: 0.4688 - precision: 0.7143 - recall: 0.4348\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.4863 - precision: 0.5049 - recall: 0.6202 - val_loss: 0.6852 - val_accuracy: 0.5210 - val_precision: 0.5658 - val_recall: 0.6418\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6936 - accuracy: 0.3125 - precision: 0.3182 - recall: 0.5000\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.4790 - precision: 0.4993 - recall: 0.6768 - val_loss: 0.6784 - val_accuracy: 0.5252 - val_precision: 0.5568 - val_recall: 0.7687\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7100 - accuracy: 0.3750 - precision: 0.3333 - recall: 0.6667\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6831 - accuracy: 0.5147 - precision: 0.5194 - recall: 0.8909 - val_loss: 0.6736 - val_accuracy: 0.5504 - val_precision: 0.5678 - val_recall: 0.8433\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6952 - accuracy: 0.4688 - precision: 0.4333 - recall: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6801 - accuracy: 0.5189 - precision: 0.5209 - recall: 0.9313 - val_loss: 0.6682 - val_accuracy: 0.5462 - val_precision: 0.5575 - val_recall: 0.9403\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6958 - accuracy: 0.5000 - precision: 0.4839 - recall: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.5168 - precision: 0.5186 - recall: 0.9838 - val_loss: 0.6624 - val_accuracy: 0.5588 - val_precision: 0.5617 - val_recall: 0.9851\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7009 - accuracy: 0.3750 - precision: 0.3871 - recall: 0.9231\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6739 - accuracy: 0.5179 - precision: 0.5190 - recall: 0.9939 - val_loss: 0.6569 - val_accuracy: 0.5630 - val_precision: 0.5636 - val_recall: 0.9925\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6835 - accuracy: 0.3750 - precision: 0.3750 - recall: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5200 - precision: 0.5200 - recall: 1.0000 - val_loss: 0.6518 - val_accuracy: 0.5630 - val_precision: 0.5630 - val_recall: 1.0000\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6731 - accuracy: 0.5000 - precision: 0.5000 - recall: 1.0000\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.5200 - precision: 0.5200 - recall: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.5672 - val_precision: 0.5677 - val_recall: 0.9701\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6575 - accuracy: 0.5000 - precision: 0.5000 - recall: 1.0000\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6655 - accuracy: 0.5525 - precision: 0.5434 - recall: 0.8727 - val_loss: 0.6419 - val_accuracy: 0.5672 - val_precision: 0.5756 - val_recall: 0.8806\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6958 - accuracy: 0.3750 - precision: 0.3846 - recall: 0.7143\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6630 - accuracy: 0.5683 - precision: 0.5585 - recall: 0.8101 - val_loss: 0.6371 - val_accuracy: 0.5672 - val_precision: 0.5866 - val_recall: 0.7836\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6374 - accuracy: 0.5000 - precision: 0.5652 - recall: 0.6842\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.5809 - precision: 0.5805 - recall: 0.6990 - val_loss: 0.6333 - val_accuracy: 0.5966 - val_precision: 0.6187 - val_recall: 0.7388\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6637 - accuracy: 0.6875 - precision: 0.7059 - recall: 0.7059\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5693 - precision: 0.5737 - recall: 0.6687 - val_loss: 0.6299 - val_accuracy: 0.6176 - val_precision: 0.6617 - val_recall: 0.6567\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6673 - accuracy: 0.4375 - precision: 0.5789 - recall: 0.5238\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.5819 - precision: 0.5896 - recall: 0.6444 - val_loss: 0.6275 - val_accuracy: 0.6387 - val_precision: 0.7000 - val_recall: 0.6269\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6114 - accuracy: 0.5312 - precision: 0.5500 - recall: 0.6471\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.5788 - precision: 0.5996 - recall: 0.5717 - val_loss: 0.6250 - val_accuracy: 0.6345 - val_precision: 0.7043 - val_recall: 0.6045\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6145 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.7500\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.5840 - precision: 0.6143 - recall: 0.5374 - val_loss: 0.6229 - val_accuracy: 0.6429 - val_precision: 0.7248 - val_recall: 0.5896\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6707 - accuracy: 0.6250 - precision: 0.6429 - recall: 0.5625\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6532 - accuracy: 0.5777 - precision: 0.6126 - recall: 0.5111 - val_loss: 0.6217 - val_accuracy: 0.6471 - val_precision: 0.7404 - val_recall: 0.5746\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6863 - accuracy: 0.5938 - precision: 0.5455 - recall: 0.4286\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6522 - accuracy: 0.5861 - precision: 0.6194 - recall: 0.5293 - val_loss: 0.6199 - val_accuracy: 0.6513 - val_precision: 0.7429 - val_recall: 0.5821\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6742 - accuracy: 0.6250 - precision: 0.7273 - recall: 0.4706\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6512 - accuracy: 0.5987 - precision: 0.6348 - recall: 0.5374 - val_loss: 0.6181 - val_accuracy: 0.6597 - val_precision: 0.7624 - val_recall: 0.5746\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5674 - accuracy: 0.6250 - precision: 0.7059 - recall: 0.6316\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.5956 - precision: 0.6410 - recall: 0.5051 - val_loss: 0.6168 - val_accuracy: 0.6597 - val_precision: 0.7624 - val_recall: 0.5746\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6540 - accuracy: 0.5312 - precision: 0.5625 - recall: 0.5294\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.5830 - precision: 0.6354 - recall: 0.4646 - val_loss: 0.6153 - val_accuracy: 0.6597 - val_precision: 0.7573 - val_recall: 0.5821\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6031 - accuracy: 0.5312 - precision: 0.6667 - recall: 0.5000\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.5935 - precision: 0.6239 - recall: 0.5495 - val_loss: 0.6156 - val_accuracy: 0.6513 - val_precision: 0.7576 - val_recall: 0.5597\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6630 - accuracy: 0.5938 - precision: 0.5556 - recall: 0.3571\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6487 - accuracy: 0.5945 - precision: 0.6416 - recall: 0.4990 - val_loss: 0.6146 - val_accuracy: 0.6471 - val_precision: 0.7500 - val_recall: 0.5597\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6059 - accuracy: 0.5312 - precision: 0.8182 - recall: 0.4091\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6483 - accuracy: 0.5998 - precision: 0.6418 - recall: 0.5212 - val_loss: 0.6140 - val_accuracy: 0.6471 - val_precision: 0.7500 - val_recall: 0.5597\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5958 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6477 - accuracy: 0.6019 - precision: 0.6457 - recall: 0.5192 - val_loss: 0.6143 - val_accuracy: 0.6387 - val_precision: 0.7449 - val_recall: 0.5448\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6625 - accuracy: 0.5625 - precision: 0.6364 - recall: 0.4118\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6473 - accuracy: 0.5935 - precision: 0.6525 - recall: 0.4667 - val_loss: 0.6131 - val_accuracy: 0.6471 - val_precision: 0.7500 - val_recall: 0.5597\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6852 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.2500\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.5998 - precision: 0.6454 - recall: 0.5111 - val_loss: 0.6125 - val_accuracy: 0.6345 - val_precision: 0.7327 - val_recall: 0.5522\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6329 - accuracy: 0.5938 - precision: 0.6250 - recall: 0.5882\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.5956 - precision: 0.6303 - recall: 0.5374 - val_loss: 0.6125 - val_accuracy: 0.6345 - val_precision: 0.7327 - val_recall: 0.5522\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6660 - accuracy: 0.6250 - precision: 0.6250 - recall: 0.6250\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6462 - accuracy: 0.5977 - precision: 0.6327 - recall: 0.5394 - val_loss: 0.6121 - val_accuracy: 0.6345 - val_precision: 0.7423 - val_recall: 0.5373\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7117 - accuracy: 0.5000 - precision: 0.6000 - recall: 0.4737\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6459 - accuracy: 0.6050 - precision: 0.6578 - recall: 0.5010 - val_loss: 0.6112 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6420 - accuracy: 0.5938 - precision: 0.5833 - recall: 0.4667\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.6050 - precision: 0.6514 - recall: 0.5172 - val_loss: 0.6109 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6675 - accuracy: 0.5312 - precision: 0.5000 - recall: 0.4000\n",
            "Epoch 33: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6455 - accuracy: 0.6103 - precision: 0.6566 - recall: 0.5253 - val_loss: 0.6109 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5860 - accuracy: 0.7500 - precision: 0.8235 - recall: 0.7368\n",
            "Epoch 34: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6103 - precision: 0.6512 - recall: 0.5394 - val_loss: 0.6111 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6277 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 35: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.6071 - precision: 0.6486 - recall: 0.5333 - val_loss: 0.6115 - val_accuracy: 0.6345 - val_precision: 0.7327 - val_recall: 0.5522\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6359 - accuracy: 0.5938 - precision: 0.7273 - recall: 0.4444\n",
            "Epoch 36: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.6029 - precision: 0.6481 - recall: 0.5172 - val_loss: 0.6114 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6516 - accuracy: 0.6250 - precision: 0.7059 - recall: 0.6316\n",
            "Epoch 37: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6445 - accuracy: 0.6092 - precision: 0.6565 - recall: 0.5212 - val_loss: 0.6118 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6660 - accuracy: 0.5625 - precision: 0.6154 - recall: 0.4706\n",
            "Epoch 38: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6445 - accuracy: 0.6029 - precision: 0.6437 - recall: 0.5293 - val_loss: 0.6114 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5991 - accuracy: 0.7188 - precision: 1.0000 - recall: 0.5263\n",
            "Epoch 39: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6050 - precision: 0.6545 - recall: 0.5091 - val_loss: 0.6115 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6419 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 40: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.6050 - precision: 0.6604 - recall: 0.4949 - val_loss: 0.6106 - val_accuracy: 0.6218 - val_precision: 0.7037 - val_recall: 0.5672\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6263 - accuracy: 0.5312 - precision: 0.4118 - recall: 0.5833\n",
            "Epoch 41: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6050 - precision: 0.6514 - recall: 0.5172 - val_loss: 0.6106 - val_accuracy: 0.6218 - val_precision: 0.7075 - val_recall: 0.5597\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6193 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 42: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.6050 - precision: 0.6427 - recall: 0.5414 - val_loss: 0.6117 - val_accuracy: 0.6261 - val_precision: 0.7143 - val_recall: 0.5597\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6230 - accuracy: 0.5625 - precision: 0.4545 - recall: 0.3846\n",
            "Epoch 43: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6082 - precision: 0.6510 - recall: 0.5313 - val_loss: 0.6114 - val_accuracy: 0.6218 - val_precision: 0.7075 - val_recall: 0.5597\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6251 - accuracy: 0.6562 - precision: 0.7692 - recall: 0.5556\n",
            "Epoch 44: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6103 - precision: 0.6498 - recall: 0.5434 - val_loss: 0.6115 - val_accuracy: 0.6261 - val_precision: 0.7143 - val_recall: 0.5597\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6356 - accuracy: 0.6562 - precision: 0.7273 - recall: 0.5000\n",
            "Epoch 45: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6134 - precision: 0.6560 - recall: 0.5394 - val_loss: 0.6108 - val_accuracy: 0.6176 - val_precision: 0.7087 - val_recall: 0.5448\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6665 - accuracy: 0.5312 - precision: 0.6000 - recall: 0.5000\n",
            "Epoch 46: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6082 - precision: 0.6502 - recall: 0.5333 - val_loss: 0.6110 - val_accuracy: 0.6176 - val_precision: 0.7087 - val_recall: 0.5448\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6744 - accuracy: 0.6875 - precision: 0.6000 - recall: 0.6923\n",
            "Epoch 47: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.6082 - precision: 0.6517 - recall: 0.5293 - val_loss: 0.6112 - val_accuracy: 0.6176 - val_precision: 0.7087 - val_recall: 0.5448\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6044 - accuracy: 0.7500 - precision: 0.8182 - recall: 0.6000\n",
            "Epoch 48: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6432 - accuracy: 0.6082 - precision: 0.6556 - recall: 0.5192 - val_loss: 0.6106 - val_accuracy: 0.6261 - val_precision: 0.7143 - val_recall: 0.5597\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6388 - accuracy: 0.5312 - precision: 0.7333 - recall: 0.5000\n",
            "Epoch 49: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.6071 - precision: 0.6458 - recall: 0.5414 - val_loss: 0.6106 - val_accuracy: 0.6176 - val_precision: 0.7087 - val_recall: 0.5448\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6482 - accuracy: 0.6562 - precision: 0.8182 - recall: 0.5000\n",
            "Epoch 50: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 0.6113 - precision: 0.6559 - recall: 0.5313 - val_loss: 0.6102 - val_accuracy: 0.6261 - val_precision: 0.7143 - val_recall: 0.5597\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5918 - accuracy: 0.5938 - precision: 0.7273 - recall: 0.4444\n",
            "Epoch 51: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6431 - accuracy: 0.6061 - precision: 0.6442 - recall: 0.5414 - val_loss: 0.6114 - val_accuracy: 0.6218 - val_precision: 0.7157 - val_recall: 0.5448\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7454 - accuracy: 0.6250 - precision: 0.5625 - recall: 0.6429\n",
            "Epoch 52: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.6082 - precision: 0.6517 - recall: 0.5293 - val_loss: 0.6102 - val_accuracy: 0.6176 - val_precision: 0.7087 - val_recall: 0.5448\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7151 - accuracy: 0.6250 - precision: 0.5333 - recall: 0.6154\n",
            "Epoch 53: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.6071 - precision: 0.6509 - recall: 0.5273 - val_loss: 0.6105 - val_accuracy: 0.6261 - val_precision: 0.7143 - val_recall: 0.5597\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6595 - accuracy: 0.6875 - precision: 0.8571 - recall: 0.6000\n",
            "Epoch 54: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6426 - accuracy: 0.6124 - precision: 0.6522 - recall: 0.5455 - val_loss: 0.6123 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6476 - accuracy: 0.6250 - precision: 0.6154 - recall: 0.5333\n",
            "Epoch 55: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6427 - accuracy: 0.6092 - precision: 0.6541 - recall: 0.5273 - val_loss: 0.6120 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6259 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.6667\n",
            "Epoch 56: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.6113 - precision: 0.6485 - recall: 0.5515 - val_loss: 0.6128 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6342 - accuracy: 0.7188 - precision: 0.7273 - recall: 0.5714\n",
            "Epoch 57: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.6082 - precision: 0.6459 - recall: 0.5455 - val_loss: 0.6121 - val_accuracy: 0.6261 - val_precision: 0.7184 - val_recall: 0.5522\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6422 - accuracy: 0.7500 - precision: 0.7273 - recall: 0.6154\n",
            "Epoch 58: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6092 - precision: 0.6489 - recall: 0.5414 - val_loss: 0.6120 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6374 - accuracy: 0.5938 - precision: 0.7778 - recall: 0.3889\n",
            "Epoch 59: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6082 - precision: 0.6495 - recall: 0.5354 - val_loss: 0.6121 - val_accuracy: 0.6176 - val_precision: 0.7048 - val_recall: 0.5522\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6214 - accuracy: 0.5938 - precision: 0.7500 - recall: 0.4737\n",
            "Epoch 60: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6092 - precision: 0.6489 - recall: 0.5414 - val_loss: 0.6126 - val_accuracy: 0.6176 - val_precision: 0.7048 - val_recall: 0.5522\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7138 - accuracy: 0.5312 - precision: 0.5000 - recall: 0.4667\n",
            "Epoch 61: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6124 - precision: 0.6500 - recall: 0.5515 - val_loss: 0.6126 - val_accuracy: 0.6176 - val_precision: 0.7048 - val_recall: 0.5522\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6694 - accuracy: 0.5625 - precision: 0.4286 - recall: 0.2308\n",
            "Epoch 62: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6082 - precision: 0.6459 - recall: 0.5455 - val_loss: 0.6124 - val_accuracy: 0.6134 - val_precision: 0.6981 - val_recall: 0.5522\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6461 - accuracy: 0.5625 - precision: 0.5333 - recall: 0.5333\n",
            "Epoch 63: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6145 - precision: 0.6509 - recall: 0.5576 - val_loss: 0.6124 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6096 - accuracy: 0.6875 - precision: 0.7143 - recall: 0.6250\n",
            "Epoch 64: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6113 - precision: 0.6457 - recall: 0.5596 - val_loss: 0.6126 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6533 - accuracy: 0.5625 - precision: 0.6364 - recall: 0.4118\n",
            "Epoch 65: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6134 - precision: 0.6553 - recall: 0.5414 - val_loss: 0.6122 - val_accuracy: 0.6176 - val_precision: 0.7048 - val_recall: 0.5522\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6226 - accuracy: 0.5625 - precision: 0.5833 - recall: 0.4375\n",
            "Epoch 66: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6103 - precision: 0.6469 - recall: 0.5515 - val_loss: 0.6127 - val_accuracy: 0.6134 - val_precision: 0.6981 - val_recall: 0.5522\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7277 - accuracy: 0.4375 - precision: 0.4545 - recall: 0.2941\n",
            "Epoch 67: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6103 - precision: 0.6449 - recall: 0.5576 - val_loss: 0.6136 - val_accuracy: 0.6134 - val_precision: 0.6981 - val_recall: 0.5522\n",
            "Epoch 67: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fasting blood sugar\n",
        "- val_accuracy: 0.5924\n",
        "- val_loss: 0.6534\n",
        "- val_precision: 0.8222\n",
        "- val_recall: 0.2937"
      ],
      "metadata": {
        "id": "wVoYgv713QpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_model = Sequential(name=\"bs_model\")\n",
        "bs_model.add(Input(shape=(1,)))\n",
        "bs_model.add(Dense(16, activation='relu'))\n",
        "bs_model.add(Dense(8, activation='relu'))\n",
        "bs_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "bs_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "bs_history = bs_model.fit(XTRAIN[:, 5], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 5], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nfhyAFgS286h",
        "outputId": "2d181b91-22e2-45b0-c590-72297012b702"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6758 - accuracy: 0.6562 - precision: 0.6884 - recall: 0.6129\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5399 - precision: 0.5586 - recall: 0.7504 - val_loss: 0.6734 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6617 - accuracy: 0.5625 - precision: 1.0000 - recall: 0.3000\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6669 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6714 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.2500\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6637 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6600 - accuracy: 0.4688 - precision: 1.0000 - recall: 0.2273\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6618 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6747 - accuracy: 0.5000 - precision: 0.8000 - recall: 0.2105\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6609 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6966 - accuracy: 0.4375 - precision: 0.6667 - recall: 0.2000\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6700 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6602 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6702 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.4000\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6603 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6946 - accuracy: 0.5312 - precision: 0.6000 - recall: 0.1875\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6599 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6704 - accuracy: 0.5938 - precision: 0.7500 - recall: 0.2000\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6594 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6860 - accuracy: 0.5312 - precision: 0.6667 - recall: 0.2353\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6597 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6490 - accuracy: 0.5625 - precision: 1.0000 - recall: 0.2222\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6600 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6487 - accuracy: 0.6562 - precision: 0.7500 - recall: 0.4000\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6596 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7382 - accuracy: 0.5625 - precision: 0.4545 - recall: 0.3846\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6593 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6982 - accuracy: 0.3750 - precision: 0.7143 - recall: 0.2174\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6597 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6743 - accuracy: 0.5312 - precision: 0.7000 - recall: 0.3684\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6598 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7259 - accuracy: 0.5625 - precision: 0.2500 - recall: 0.0833\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6594 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6832 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.4615\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6591 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6247 - accuracy: 0.6250 - precision: 0.8182 - recall: 0.4737\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6595 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6938 - accuracy: 0.5938 - precision: 0.5714 - recall: 0.2857\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6599 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6297 - accuracy: 0.7500 - precision: 0.8333 - recall: 0.4167\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6594 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6887 - accuracy: 0.5625 - precision: 0.6250 - recall: 0.3125\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6596 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6375 - accuracy: 0.7188 - precision: 1.0000 - recall: 0.2500\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6595 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6378 - accuracy: 0.6250 - precision: 0.8571 - recall: 0.3529\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6595 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6280 - accuracy: 0.6250 - precision: 0.8750 - recall: 0.3889\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6592 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6512 - accuracy: 0.5938 - precision: 0.8333 - recall: 0.2941\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6593 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6236 - accuracy: 0.6562 - precision: 0.8750 - recall: 0.4118\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6599 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6868 - accuracy: 0.4062 - precision: 0.8000 - recall: 0.1818\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6597 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6407 - accuracy: 0.6562 - precision: 0.7778 - recall: 0.4375\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6593 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6459 - accuracy: 0.4375 - precision: 1.0000 - recall: 0.2500\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6597 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6583 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.3571\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6597 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6328 - accuracy: 0.6562 - precision: 0.8571 - recall: 0.3750\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6595 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7295 - accuracy: 0.5938 - precision: 0.3333 - recall: 0.1818\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5746 - precision: 0.7250 - recall: 0.2929 - val_loss: 0.6595 - val_accuracy: 0.5630 - val_precision: 0.7778 - val_recall: 0.3134\n",
            "Epoch 32: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resting ecg\n",
        "- val_accuracy: 0.5672\n",
        "- val_loss: 0.6859\n",
        "- val_precision: 0.6211\n",
        "- val_recall: 0.4683"
      ],
      "metadata": {
        "id": "ifPYyGGn3UE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ecg_model = Sequential(name=\"ecg_model\")\n",
        "ecg_model.add(Input(shape=(1,)))\n",
        "ecg_model.add(Dense(16, activation='relu'))\n",
        "ecg_model.add(Dense(8, activation='relu'))\n",
        "ecg_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "ecg_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "ecg_history = ecg_model.fit(XTRAIN[:, 6], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 6], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CX47IIjd29NC",
        "outputId": "bc5ec81e-24fc-4d52-c1c7-81ef843a0fcb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7514 - accuracy: 0.4062 - precision: 0.6857 - recall: 0.3221\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7073 - accuracy: 0.4632 - precision: 0.5437 - recall: 0.1383 - val_loss: 0.7013 - val_accuracy: 0.4370 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6966 - accuracy: 0.4062 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6975 - accuracy: 0.4926 - precision: 0.5306 - recall: 0.2101 - val_loss: 0.6941 - val_accuracy: 0.5168 - val_precision: 0.7879 - val_recall: 0.1940\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6941 - accuracy: 0.3438 - precision: 0.2857 - recall: 0.1111\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.5200 - precision: 0.6284 - recall: 0.1879 - val_loss: 0.6911 - val_accuracy: 0.5168 - val_precision: 0.7879 - val_recall: 0.1940\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6968 - accuracy: 0.4375 - precision: 0.3333 - recall: 0.1250\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5252 - precision: 0.5824 - recall: 0.3071 - val_loss: 0.6896 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6924 - accuracy: 0.4688 - precision: 0.5000 - recall: 0.3529\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6886 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6834 - accuracy: 0.6562 - precision: 0.5385 - recall: 0.5833\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6880 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6856 - accuracy: 0.6250 - precision: 0.5714 - recall: 0.3077\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6870 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6793 - accuracy: 0.6562 - precision: 0.6429 - recall: 0.6000\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6867 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6871 - accuracy: 0.5938 - precision: 0.5455 - recall: 0.4286\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6862 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6726 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.7059\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6860 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6948 - accuracy: 0.4688 - precision: 0.5714 - recall: 0.4211\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6858 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7002 - accuracy: 0.4375 - precision: 0.5000 - recall: 0.3333\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6856 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6755 - accuracy: 0.6250 - precision: 0.6923 - recall: 0.5294\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6853 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7191 - accuracy: 0.3125 - precision: 0.3333 - recall: 0.1579\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6850 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6710 - accuracy: 0.6562 - precision: 0.6875 - recall: 0.6471\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6849 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6697 - accuracy: 0.6250 - precision: 0.7692 - recall: 0.5263\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6846 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6856 - accuracy: 0.5000 - precision: 0.6250 - recall: 0.5000\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6846 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6900 - accuracy: 0.4688 - precision: 0.6250 - recall: 0.4762\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6843 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7008 - accuracy: 0.5000 - precision: 0.4706 - recall: 0.5333\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6843 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6861 - accuracy: 0.5625 - precision: 0.5833 - recall: 0.4375\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6840 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6956 - accuracy: 0.4062 - precision: 0.6364 - recall: 0.3182\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6840 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6863 - accuracy: 0.5312 - precision: 0.6154 - recall: 0.4444\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6837 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6891 - accuracy: 0.5625 - precision: 0.5455 - recall: 0.4000\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6835 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7140 - accuracy: 0.3750 - precision: 0.4375 - recall: 0.3889\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6837 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6899 - accuracy: 0.5625 - precision: 0.5294 - recall: 0.6000\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6834 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7045 - accuracy: 0.5312 - precision: 0.3000 - recall: 0.2727\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6833 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6858 - accuracy: 0.5312 - precision: 0.6154 - recall: 0.4444\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6832 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7007 - accuracy: 0.4688 - precision: 0.5000 - recall: 0.3529\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6831 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6795 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6830 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7053 - accuracy: 0.4375 - precision: 0.5000 - recall: 0.4444\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6828 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7088 - accuracy: 0.4375 - precision: 0.2500 - recall: 0.1429\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6826 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6648 - accuracy: 0.6250 - precision: 0.7857 - recall: 0.5500\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6823 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7097 - accuracy: 0.4375 - precision: 0.4118 - recall: 0.4667\n",
            "Epoch 33: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6824 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6744 - accuracy: 0.6250 - precision: 0.6429 - recall: 0.5625\n",
            "Epoch 34: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6824 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6561 - accuracy: 0.6562 - precision: 0.8462 - recall: 0.5500\n",
            "Epoch 35: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6823 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000 - precision: 0.5455 - recall: 0.3529\n",
            "Epoch 36: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6822 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7081 - accuracy: 0.4375 - precision: 0.4286 - recall: 0.3750\n",
            "Epoch 37: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6823 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7073 - accuracy: 0.4375 - precision: 0.4286 - recall: 0.3750\n",
            "Epoch 38: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6821 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6713 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.6667\n",
            "Epoch 39: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6819 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6994 - accuracy: 0.5312 - precision: 0.4545 - recall: 0.3571\n",
            "Epoch 40: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6819 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6827 - accuracy: 0.5312 - precision: 0.7000 - recall: 0.3684\n",
            "Epoch 41: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6814 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6717 - accuracy: 0.6250 - precision: 0.6471 - recall: 0.6471\n",
            "Epoch 42: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6818 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6865 - accuracy: 0.5000 - precision: 0.6471 - recall: 0.5238\n",
            "Epoch 43: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6819 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6708 - accuracy: 0.6562 - precision: 0.6429 - recall: 0.6000\n",
            "Epoch 44: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6817 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6919 - accuracy: 0.5312 - precision: 0.5294 - recall: 0.5625\n",
            "Epoch 45: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6815 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6882 - accuracy: 0.5312 - precision: 0.5333 - recall: 0.5000\n",
            "Epoch 46: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6812 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6922 - accuracy: 0.5000 - precision: 0.5833 - recall: 0.3889\n",
            "Epoch 47: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6814 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6967 - accuracy: 0.5625 - precision: 0.4737 - recall: 0.6923\n",
            "Epoch 48: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6812 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7093 - accuracy: 0.4688 - precision: 0.3333 - recall: 0.2143\n",
            "Epoch 49: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6811 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7025 - accuracy: 0.4062 - precision: 0.5385 - recall: 0.3500\n",
            "Epoch 50: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6814 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7162 - accuracy: 0.4062 - precision: 0.4375 - recall: 0.4118\n",
            "Epoch 51: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6813 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6926 - accuracy: 0.5625 - precision: 0.5000 - recall: 0.3571\n",
            "Epoch 52: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6812 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7013 - accuracy: 0.4688 - precision: 0.4286 - recall: 0.4000\n",
            "Epoch 53: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6812 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6791 - accuracy: 0.6250 - precision: 0.6154 - recall: 0.5333\n",
            "Epoch 54: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6808 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6921 - accuracy: 0.4688 - precision: 0.5714 - recall: 0.4211\n",
            "Epoch 55: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6807 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7101 - accuracy: 0.4062 - precision: 0.5000 - recall: 0.3158\n",
            "Epoch 56: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6805 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7148 - accuracy: 0.3438 - precision: 0.4615 - recall: 0.3000\n",
            "Epoch 57: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6806 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6828 - accuracy: 0.5312 - precision: 0.5714 - recall: 0.4706\n",
            "Epoch 58: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6806 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6516 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 59: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6808 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6649 - accuracy: 0.6875 - precision: 0.6923 - recall: 0.6000\n",
            "Epoch 60: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6805 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6788 - accuracy: 0.5938 - precision: 0.6923 - recall: 0.5000\n",
            "Epoch 61: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6807 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6762 - accuracy: 0.5938 - precision: 0.5333 - recall: 0.5714\n",
            "Epoch 62: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6802 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7240 - accuracy: 0.5000 - precision: 0.3889 - recall: 0.5833\n",
            "Epoch 63: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6801 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6630 - accuracy: 0.6562 - precision: 0.6316 - recall: 0.7500\n",
            "Epoch 64: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6803 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6720 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.5000\n",
            "Epoch 65: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6799 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6857 - accuracy: 0.4688 - precision: 0.6000 - recall: 0.4500\n",
            "Epoch 66: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6958 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.3750\n",
            "Epoch 67: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6800 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6772 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.2857\n",
            "Epoch 68: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6803 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7023 - accuracy: 0.5000 - precision: 0.4444 - recall: 0.2667\n",
            "Epoch 69: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6800 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6560 - accuracy: 0.6562 - precision: 0.7500 - recall: 0.6316\n",
            "Epoch 70: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6797 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6746 - accuracy: 0.5938 - precision: 0.7500 - recall: 0.4737\n",
            "Epoch 71: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6800 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6844 - accuracy: 0.5938 - precision: 0.5455 - recall: 0.4286\n",
            "Epoch 72: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6667 - accuracy: 0.6250 - precision: 0.8000 - recall: 0.4444\n",
            "Epoch 73: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6796 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6841 - accuracy: 0.5000 - precision: 0.6364 - recall: 0.3684\n",
            "Epoch 74: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6799 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6642 - accuracy: 0.6562 - precision: 0.7273 - recall: 0.5000\n",
            "Epoch 75: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6942 - accuracy: 0.5625 - precision: 0.4545 - recall: 0.3846\n",
            "Epoch 76: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6796 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6774 - accuracy: 0.6562 - precision: 0.5714 - recall: 0.6154\n",
            "Epoch 77: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6794 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6879 - accuracy: 0.4688 - precision: 0.5000 - recall: 0.5294\n",
            "Epoch 78: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6792 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6798 - accuracy: 0.5312 - precision: 0.7273 - recall: 0.4000\n",
            "Epoch 79: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6796 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6611 - accuracy: 0.6250 - precision: 0.6471 - recall: 0.6471\n",
            "Epoch 80: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6445 - accuracy: 0.6562 - precision: 0.8571 - recall: 0.5714\n",
            "Epoch 81: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6799 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6475 - accuracy: 0.7500 - precision: 0.8182 - recall: 0.6000\n",
            "Epoch 82: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6912 - accuracy: 0.5312 - precision: 0.5625 - recall: 0.5294\n",
            "Epoch 83: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6798 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6796 - accuracy: 0.5312 - precision: 0.6250 - recall: 0.5263\n",
            "Epoch 84: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6795 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6735 - accuracy: 0.5625 - precision: 0.6923 - recall: 0.4737\n",
            "Epoch 85: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6793 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7025 - accuracy: 0.5312 - precision: 0.3000 - recall: 0.2727\n",
            "Epoch 86: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6797 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6757 - accuracy: 0.5625 - precision: 0.6667 - recall: 0.4444\n",
            "Epoch 87: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6799 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6772 - accuracy: 0.5625 - precision: 0.6429 - recall: 0.5000\n",
            "Epoch 88: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6792 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7015 - accuracy: 0.4375 - precision: 0.4167 - recall: 0.3125\n",
            "Epoch 89: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6786 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6855 - accuracy: 0.5312 - precision: 0.5625 - recall: 0.5294\n",
            "Epoch 90: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6792 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7038 - accuracy: 0.4375 - precision: 0.5000 - recall: 0.3333\n",
            "Epoch 91: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6789 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6625 - accuracy: 0.6562 - precision: 0.6875 - recall: 0.6471\n",
            "Epoch 92: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6787 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6889 - accuracy: 0.5312 - precision: 0.6364 - recall: 0.3889\n",
            "Epoch 93: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6784 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6977 - accuracy: 0.4688 - precision: 0.5000 - recall: 0.4118\n",
            "Epoch 94: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6782 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6517 - accuracy: 0.6562 - precision: 0.8125 - recall: 0.6190\n",
            "Epoch 95: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6781 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6854 - accuracy: 0.6250 - precision: 0.5333 - recall: 0.6154\n",
            "Epoch 96: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6780 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6928 - accuracy: 0.3438 - precision: 0.7143 - recall: 0.2083\n",
            "Epoch 97: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6781 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6832 - accuracy: 0.5625 - precision: 0.6250 - recall: 0.5556\n",
            "Epoch 98: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6779 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6880 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.4375\n",
            "Epoch 99: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6782 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6858 - accuracy: 0.4375 - precision: 0.6364 - recall: 0.3333\n",
            "Epoch 100: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - precision: 0.5804 - recall: 0.4667 - val_loss: 0.6785 - val_accuracy: 0.5462 - val_precision: 0.6204 - val_recall: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## max heart rate\n",
        "- val_accuracy: 0.6345\n",
        "- val_loss: 0.6337\n",
        "- val_precision: 0.6696\n",
        "- val_recall: 0.6111"
      ],
      "metadata": {
        "id": "8jPwepk83Wth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heart_model = Sequential(name=\"heart_model\")\n",
        "heart_model.add(Input(shape=(1,)))\n",
        "heart_model.add(Dense(16, activation='relu'))\n",
        "heart_model.add(Dense(8, activation='relu'))\n",
        "heart_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "heart_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "heart_history = heart_model.fit(XTRAIN[:, 7], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 7], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vGrmoo5T29fQ",
        "outputId": "d1ed3592-669a-48e1-c5ec-d87316e14a57"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6554 - accuracy: 0.4375 - precision: 0.5786 - recall: 0.5473\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6514 - accuracy: 0.5809 - precision: 0.5742 - recall: 0.7504 - val_loss: 0.6579 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5956 - accuracy: 0.6875 - precision: 0.8235 - recall: 0.6667\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6702 - precision: 0.6806 - recall: 0.6889 - val_loss: 0.6558 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6072 - accuracy: 0.7188 - precision: 0.7059 - recall: 0.7500\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6331 - accuracy: 0.6712 - precision: 0.6857 - recall: 0.6788 - val_loss: 0.6558 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6084 - accuracy: 0.6250 - precision: 0.6190 - recall: 0.7647\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6301 - accuracy: 0.6786 - precision: 0.6989 - recall: 0.6707 - val_loss: 0.6557 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6713 - accuracy: 0.5938 - precision: 0.6500 - recall: 0.6842\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.6796 - precision: 0.7004 - recall: 0.6707 - val_loss: 0.6561 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6435 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.5882\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6266 - accuracy: 0.6775 - precision: 0.7000 - recall: 0.6646 - val_loss: 0.6564 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6423 - accuracy: 0.6875 - precision: 0.6154 - recall: 0.6154\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6250 - accuracy: 0.6786 - precision: 0.7006 - recall: 0.6667 - val_loss: 0.6567 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7151 - accuracy: 0.6250 - precision: 0.6111 - recall: 0.6875\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6237 - accuracy: 0.6765 - precision: 0.6994 - recall: 0.6626 - val_loss: 0.6567 - val_accuracy: 0.6639 - val_precision: 0.7143 - val_recall: 0.6716\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5458 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.6754 - precision: 0.6987 - recall: 0.6606 - val_loss: 0.6562 - val_accuracy: 0.6639 - val_precision: 0.7143 - val_recall: 0.6716\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6113 - accuracy: 0.6250 - precision: 0.7000 - recall: 0.7000\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.6775 - precision: 0.7017 - recall: 0.6606 - val_loss: 0.6562 - val_accuracy: 0.6303 - val_precision: 0.7018 - val_recall: 0.5970\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6304 - accuracy: 0.7500 - precision: 0.7273 - recall: 0.6154\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.6786 - precision: 0.7162 - recall: 0.6323 - val_loss: 0.6566 - val_accuracy: 0.6303 - val_precision: 0.7018 - val_recall: 0.5970\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5545 - accuracy: 0.7500 - precision: 0.8571 - recall: 0.6667\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6193 - accuracy: 0.6786 - precision: 0.7172 - recall: 0.6303 - val_loss: 0.6562 - val_accuracy: 0.6303 - val_precision: 0.7018 - val_recall: 0.5970\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5953 - accuracy: 0.5938 - precision: 0.6250 - recall: 0.5882\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6182 - accuracy: 0.6786 - precision: 0.7153 - recall: 0.6343 - val_loss: 0.6558 - val_accuracy: 0.6303 - val_precision: 0.7018 - val_recall: 0.5970\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6045 - accuracy: 0.7500 - precision: 0.8000 - recall: 0.7059\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.6786 - precision: 0.7172 - recall: 0.6303 - val_loss: 0.6561 - val_accuracy: 0.6303 - val_precision: 0.7018 - val_recall: 0.5970\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6143 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6164 - accuracy: 0.6775 - precision: 0.7176 - recall: 0.6263 - val_loss: 0.6559 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6385 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.6000\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6155 - accuracy: 0.6775 - precision: 0.7186 - recall: 0.6242 - val_loss: 0.6554 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6482 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.4615\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6148 - accuracy: 0.6775 - precision: 0.7186 - recall: 0.6242 - val_loss: 0.6554 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6406 - accuracy: 0.6562 - precision: 0.5714 - recall: 0.6154\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6140 - accuracy: 0.6796 - precision: 0.7220 - recall: 0.6242 - val_loss: 0.6554 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6718 - accuracy: 0.5625 - precision: 0.6429 - recall: 0.5000\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6133 - accuracy: 0.6828 - precision: 0.7271 - recall: 0.6242 - val_loss: 0.6561 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7122 - accuracy: 0.6562 - precision: 0.5000 - recall: 0.7273\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6128 - accuracy: 0.6796 - precision: 0.7220 - recall: 0.6242 - val_loss: 0.6557 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6319 - accuracy: 0.5625 - precision: 0.5625 - recall: 0.5625\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6121 - accuracy: 0.6807 - precision: 0.7258 - recall: 0.6202 - val_loss: 0.6557 - val_accuracy: 0.6345 - val_precision: 0.7080 - val_recall: 0.5970\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6659 - accuracy: 0.6250 - precision: 0.6923 - recall: 0.5294\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6116 - accuracy: 0.6796 - precision: 0.7220 - recall: 0.6242 - val_loss: 0.6558 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6613 - accuracy: 0.6250 - precision: 0.7143 - recall: 0.5556\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6796 - precision: 0.7220 - recall: 0.6242 - val_loss: 0.6551 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7023 - accuracy: 0.6250 - precision: 0.6429 - recall: 0.5625\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6103 - accuracy: 0.6849 - precision: 0.7338 - recall: 0.6182 - val_loss: 0.6548 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7932 - accuracy: 0.5625 - precision: 0.3846 - recall: 0.4545\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6101 - accuracy: 0.6838 - precision: 0.7321 - recall: 0.6182 - val_loss: 0.6545 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6033 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.6870 - precision: 0.7373 - recall: 0.6182 - val_loss: 0.6553 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6236 - accuracy: 0.6875 - precision: 0.8000 - recall: 0.6316\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.6838 - precision: 0.7321 - recall: 0.6182 - val_loss: 0.6546 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7058 - accuracy: 0.5938 - precision: 0.6000 - recall: 0.5625\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.6849 - precision: 0.7338 - recall: 0.6182 - val_loss: 0.6543 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5568 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.6429\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6084 - accuracy: 0.6849 - precision: 0.7338 - recall: 0.6182 - val_loss: 0.6544 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6086 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6079 - accuracy: 0.6870 - precision: 0.7373 - recall: 0.6182 - val_loss: 0.6545 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5889 - accuracy: 0.6875 - precision: 0.8000 - recall: 0.6316\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6546 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5452 - accuracy: 0.8125 - precision: 0.9286 - recall: 0.7222\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6072 - accuracy: 0.6870 - precision: 0.7362 - recall: 0.6202 - val_loss: 0.6541 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6418 - accuracy: 0.7188 - precision: 0.5625 - recall: 0.8182\n",
            "Epoch 33: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6067 - accuracy: 0.6912 - precision: 0.7445 - recall: 0.6182 - val_loss: 0.6538 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5750 - accuracy: 0.7188 - precision: 0.8235 - recall: 0.7000\n",
            "Epoch 34: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6064 - accuracy: 0.6859 - precision: 0.7356 - recall: 0.6182 - val_loss: 0.6530 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5702 - accuracy: 0.6875 - precision: 0.8462 - recall: 0.5789\n",
            "Epoch 35: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.6870 - precision: 0.7373 - recall: 0.6182 - val_loss: 0.6528 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6139 - accuracy: 0.6562 - precision: 0.7333 - recall: 0.6111\n",
            "Epoch 36: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.6901 - precision: 0.7427 - recall: 0.6182 - val_loss: 0.6522 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7058 - accuracy: 0.5625 - precision: 0.6667 - recall: 0.3529\n",
            "Epoch 37: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6055 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6520 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6619 - accuracy: 0.7812 - precision: 0.5556 - recall: 0.6250\n",
            "Epoch 38: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6922 - precision: 0.7463 - recall: 0.6182 - val_loss: 0.6526 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5587 - accuracy: 0.6562 - precision: 0.8000 - recall: 0.4706\n",
            "Epoch 39: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6048 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6524 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6384 - accuracy: 0.6250 - precision: 0.7143 - recall: 0.5556\n",
            "Epoch 40: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6522 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6449 - accuracy: 0.5625 - precision: 0.7333 - recall: 0.5238\n",
            "Epoch 41: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6516 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5866 - accuracy: 0.6875 - precision: 0.7333 - recall: 0.6471\n",
            "Epoch 42: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6522 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7788 - accuracy: 0.5312 - precision: 0.5333 - recall: 0.5000\n",
            "Epoch 43: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.6912 - precision: 0.7445 - recall: 0.6182 - val_loss: 0.6512 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6171 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.5882\n",
            "Epoch 44: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.6922 - precision: 0.7463 - recall: 0.6182 - val_loss: 0.6514 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5452 - accuracy: 0.8125 - precision: 0.9167 - recall: 0.6875\n",
            "Epoch 45: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6028 - accuracy: 0.6870 - precision: 0.7373 - recall: 0.6182 - val_loss: 0.6507 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5884 - accuracy: 0.7188 - precision: 0.8000 - recall: 0.5333\n",
            "Epoch 46: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6026 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6502 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6213 - accuracy: 0.5938 - precision: 0.6250 - recall: 0.5882\n",
            "Epoch 47: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6498 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6362 - accuracy: 0.5625 - precision: 0.5333 - recall: 0.5333\n",
            "Epoch 48: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6022 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6500 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5397 - accuracy: 0.7812 - precision: 0.8667 - recall: 0.7222\n",
            "Epoch 49: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6015 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6502 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6947 - accuracy: 0.5938 - precision: 0.5625 - recall: 0.6000\n",
            "Epoch 50: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6015 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6498 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5508 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.7273\n",
            "Epoch 51: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6013 - accuracy: 0.6880 - precision: 0.7391 - recall: 0.6182 - val_loss: 0.6496 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5433 - accuracy: 0.8125 - precision: 0.8462 - recall: 0.7333\n",
            "Epoch 52: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6009 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6488 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6811 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.6429\n",
            "Epoch 53: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6492 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6227 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.5882\n",
            "Epoch 54: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.6922 - precision: 0.7463 - recall: 0.6182 - val_loss: 0.6492 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6003 - accuracy: 0.6875 - precision: 0.6923 - recall: 0.6000\n",
            "Epoch 55: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6000 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6498 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7212 - accuracy: 0.5938 - precision: 0.5833 - recall: 0.4667\n",
            "Epoch 56: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6488 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6001 - accuracy: 0.6875 - precision: 0.6000 - recall: 0.8571\n",
            "Epoch 57: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5999 - accuracy: 0.6922 - precision: 0.7463 - recall: 0.6182 - val_loss: 0.6489 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4598 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 58: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6487 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6035 - accuracy: 0.5625 - precision: 0.7647 - recall: 0.5652\n",
            "Epoch 59: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6922 - precision: 0.7463 - recall: 0.6182 - val_loss: 0.6486 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6000 - accuracy: 0.6562 - precision: 0.6500 - recall: 0.7647\n",
            "Epoch 60: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6486 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5314 - accuracy: 0.7500 - precision: 0.8571 - recall: 0.6667\n",
            "Epoch 61: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6485 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6586 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.6154\n",
            "Epoch 62: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5987 - accuracy: 0.6891 - precision: 0.7409 - recall: 0.6182 - val_loss: 0.6477 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7908 - accuracy: 0.4375 - precision: 0.5333 - recall: 0.4211\n",
            "Epoch 63: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.6901 - precision: 0.7427 - recall: 0.6182 - val_loss: 0.6484 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5418 - accuracy: 0.7500 - precision: 0.9000 - recall: 0.5625\n",
            "Epoch 64: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5983 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6479 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6648 - accuracy: 0.5938 - precision: 0.5294 - recall: 0.6429\n",
            "Epoch 65: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6474 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6150 - accuracy: 0.6875 - precision: 0.7000 - recall: 0.5000\n",
            "Epoch 66: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6467 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6602 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.6250\n",
            "Epoch 67: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.6912 - precision: 0.7445 - recall: 0.6182 - val_loss: 0.6460 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6477 - accuracy: 0.5938 - precision: 0.6000 - recall: 0.4000\n",
            "Epoch 68: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6452 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5524 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 69: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5974 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6452 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6308 - accuracy: 0.6250 - precision: 0.7778 - recall: 0.6364\n",
            "Epoch 70: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.6912 - precision: 0.7445 - recall: 0.6182 - val_loss: 0.6464 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5629 - accuracy: 0.6875 - precision: 0.8889 - recall: 0.4706\n",
            "Epoch 71: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6459 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5840 - accuracy: 0.6562 - precision: 0.8571 - recall: 0.5714\n",
            "Epoch 72: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5968 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6452 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5775 - accuracy: 0.7188 - precision: 0.7000 - recall: 0.5385\n",
            "Epoch 73: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6456 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6497 - accuracy: 0.6250 - precision: 0.6154 - recall: 0.5333\n",
            "Epoch 74: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6451 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5978 - accuracy: 0.6250 - precision: 0.6923 - recall: 0.5294\n",
            "Epoch 75: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6444 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6264 - accuracy: 0.6562 - precision: 0.7500 - recall: 0.5294\n",
            "Epoch 76: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6440 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5758 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 77: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6446 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5715 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 78: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6440 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5551 - accuracy: 0.7188 - precision: 0.7857 - recall: 0.6471\n",
            "Epoch 79: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6434 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6044 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 80: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6429 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5472 - accuracy: 0.7812 - precision: 0.8333 - recall: 0.6667\n",
            "Epoch 81: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6922 - precision: 0.7475 - recall: 0.6162 - val_loss: 0.6429 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5473 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 82: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6425 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7312 - accuracy: 0.5938 - precision: 0.6250 - recall: 0.5882\n",
            "Epoch 83: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6428 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6795 - accuracy: 0.5625 - precision: 0.6923 - recall: 0.4737\n",
            "Epoch 84: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6425 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6003 - accuracy: 0.6562 - precision: 0.6875 - recall: 0.6471\n",
            "Epoch 85: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.6933 - precision: 0.7482 - recall: 0.6182 - val_loss: 0.6423 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5605 - accuracy: 0.7188 - precision: 0.8750 - recall: 0.6667\n",
            "Epoch 86: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6430 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6123 - accuracy: 0.7188 - precision: 0.8182 - recall: 0.5625\n",
            "Epoch 87: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5942 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6432 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5606 - accuracy: 0.7812 - precision: 0.8462 - recall: 0.6875\n",
            "Epoch 88: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5941 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6437 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6309 - accuracy: 0.6562 - precision: 0.5882 - recall: 0.7143\n",
            "Epoch 89: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6429 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6462 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 90: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6427 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5218 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.5000\n",
            "Epoch 91: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6431 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5638 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 92: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.6933 - precision: 0.7494 - recall: 0.6162 - val_loss: 0.6434 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5474 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.7692\n",
            "Epoch 93: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5936 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6431 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6456 - accuracy: 0.5625 - precision: 0.6667 - recall: 0.5263\n",
            "Epoch 94: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.6933 - precision: 0.7494 - recall: 0.6162 - val_loss: 0.6421 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7283 - accuracy: 0.5312 - precision: 0.6154 - recall: 0.4444\n",
            "Epoch 95: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6428 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6078 - accuracy: 0.7188 - precision: 0.7647 - recall: 0.7222\n",
            "Epoch 96: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5932 - accuracy: 0.6933 - precision: 0.7506 - recall: 0.6141 - val_loss: 0.6426 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5950 - accuracy: 0.6250 - precision: 0.7273 - recall: 0.4706\n",
            "Epoch 97: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5930 - accuracy: 0.6922 - precision: 0.7488 - recall: 0.6141 - val_loss: 0.6428 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6373 - accuracy: 0.6250 - precision: 0.7500 - recall: 0.6000\n",
            "Epoch 98: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5930 - accuracy: 0.6954 - precision: 0.7518 - recall: 0.6182 - val_loss: 0.6422 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6780 - accuracy: 0.6562 - precision: 0.7857 - recall: 0.5789\n",
            "Epoch 99: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6422 - val_accuracy: 0.6303 - val_precision: 0.7130 - val_recall: 0.5746\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6288 - accuracy: 0.7188 - precision: 0.6842 - recall: 0.8125\n",
            "Epoch 100: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5926 - accuracy: 0.6943 - precision: 0.7500 - recall: 0.6182 - val_loss: 0.6407 - val_accuracy: 0.6218 - val_precision: 0.7115 - val_recall: 0.5522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## exercise angina\n",
        "- val_accuracy: 0.7479\n",
        "- val_loss: 0.5573\n",
        "- val_precision: 0.8173\n",
        "- val_recall: 0.6746"
      ],
      "metadata": {
        "id": "TgHTK1fA3Z5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ang_model = Sequential(name=\"ang_model\")\n",
        "ang_model.add(Input(shape=(1,)))\n",
        "ang_model.add(Dense(16, activation='relu'))\n",
        "ang_model.add(Dense(8, activation='relu'))\n",
        "ang_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "ang_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "ang_history = ang_model.fit(XTRAIN[:, 8], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 8], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvwjTb8a29yF",
        "outputId": "add9d3d1-6cc2-45ae-f864-6467e389bfc3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6775 - accuracy: 0.4375 - precision: 0.6471 - recall: 0.5946\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6440 - accuracy: 0.6744 - precision: 0.6971 - recall: 0.6439 - val_loss: 0.5965 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6368 - accuracy: 0.6875 - precision: 0.7692 - recall: 0.5882\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5640 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7206 - accuracy: 0.5938 - precision: 0.5625 - recall: 0.6000\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6053 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5459 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6605 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.3571\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5374 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6471 - accuracy: 0.7188 - precision: 0.6875 - recall: 0.7333\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5308 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5862 - accuracy: 0.7500 - precision: 0.7857 - recall: 0.6875\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5898 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5268 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5699 - accuracy: 0.7188 - precision: 0.8462 - recall: 0.6111\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5866 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5248 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5827 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5846 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5210 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4915 - accuracy: 0.7812 - precision: 0.9333 - recall: 0.7000\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5199 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5853 - accuracy: 0.7188 - precision: 0.8571 - recall: 0.4286\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5176 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6909 - accuracy: 0.6250 - precision: 0.6364 - recall: 0.4667\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5822 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5166 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6355 - accuracy: 0.5938 - precision: 0.8571 - recall: 0.5217\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5184 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6206 - accuracy: 0.6562 - precision: 0.8182 - recall: 0.5000\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5182 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6613 - accuracy: 0.6875 - precision: 0.6154 - recall: 0.6154\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5176 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6519 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.5882\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5159 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7670 - accuracy: 0.5625 - precision: 0.5714 - recall: 0.5000\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5156 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5408 - accuracy: 0.7500 - precision: 0.9000 - recall: 0.5625\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5180 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5973 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7059\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5169 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5377 - accuracy: 0.7188 - precision: 0.9286 - recall: 0.6190\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5185 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7241 - accuracy: 0.5938 - precision: 0.5556 - recall: 0.3571\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5175 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6196 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5178 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7040 - accuracy: 0.6250 - precision: 0.5556 - recall: 0.3846\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5196 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5619 - accuracy: 0.7188 - precision: 0.8333 - recall: 0.7143\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5188 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4274 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.7273\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5175 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5474 - accuracy: 0.7500 - precision: 0.8462 - recall: 0.6471\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5188 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4836 - accuracy: 0.8125 - precision: 1.0000 - recall: 0.6000\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5173 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5558 - accuracy: 0.7188 - precision: 0.8667 - recall: 0.6500\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5174 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5893 - accuracy: 0.6875 - precision: 0.8462 - recall: 0.5789\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5155 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5888 - accuracy: 0.6875 - precision: 0.8889 - recall: 0.4706\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5154 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5659 - accuracy: 0.7500 - precision: 0.7857 - recall: 0.6875\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5169 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6026 - accuracy: 0.6875 - precision: 0.8571 - recall: 0.4000\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5162 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6671 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.3846\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5154 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5689 - accuracy: 0.6875 - precision: 0.8750 - recall: 0.6364\n",
            "Epoch 33: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5162 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6640 - accuracy: 0.6875 - precision: 0.6154 - recall: 0.6154\n",
            "Epoch 34: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5159 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6205 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 35: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5180 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5785 - accuracy: 0.6875 - precision: 0.9091 - recall: 0.5263\n",
            "Epoch 36: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5176 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5561 - accuracy: 0.7188 - precision: 0.8667 - recall: 0.6500\n",
            "Epoch 37: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5820 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5175 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5543 - accuracy: 0.6875 - precision: 1.0000 - recall: 0.5238\n",
            "Epoch 38: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5177 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6074 - accuracy: 0.6875 - precision: 0.7857 - recall: 0.6111\n",
            "Epoch 39: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5165 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5892 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 40: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5161 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5162 - accuracy: 0.7500 - precision: 1.0000 - recall: 0.5556\n",
            "Epoch 41: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5186 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6492 - accuracy: 0.6250 - precision: 0.7857 - recall: 0.5500\n",
            "Epoch 42: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5194 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7105 - accuracy: 0.5938 - precision: 0.6364 - recall: 0.4375\n",
            "Epoch 43: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5191 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6032 - accuracy: 0.6875 - precision: 0.8571 - recall: 0.4000\n",
            "Epoch 44: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7185 - precision: 0.8093 - recall: 0.6000 - val_loss: 0.5178 - val_accuracy: 0.7647 - val_precision: 0.9149 - val_recall: 0.6418\n",
            "Epoch 44: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## oldpeak\n",
        "- val_accuracy: 0.7437\n",
        "- val_loss: 0.5588\n",
        "- val_precision: 0.7519\n",
        "- val_recall: 0.7698"
      ],
      "metadata": {
        "id": "H8L-nCTH3eJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peak_model = Sequential(name=\"peak_model\")\n",
        "peak_model.add(Input(shape=(1,)))\n",
        "peak_model.add(Dense(16, activation='relu'))\n",
        "peak_model.add(Dense(8, activation='relu'))\n",
        "peak_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "peak_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "peak_history = peak_model.fit(XTRAIN[:, 9], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 9], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yGTy5h-J2-G1",
        "outputId": "b7054683-b53e-460f-c684-cb8771ffaafb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6871 - accuracy: 0.5000 - precision: 0.8095 - recall: 0.6800\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6791 - accuracy: 0.6912 - precision: 0.7329 - recall: 0.6979 - val_loss: 0.6706 - val_accuracy: 0.6891 - val_precision: 0.7830 - val_recall: 0.6194\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6713 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.7143\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6617 - accuracy: 0.7069 - precision: 0.7328 - recall: 0.6869 - val_loss: 0.6486 - val_accuracy: 0.6933 - val_precision: 0.7699 - val_recall: 0.6493\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6607 - accuracy: 0.5938 - precision: 0.6000 - recall: 0.5625\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.7080 - precision: 0.7294 - recall: 0.6970 - val_loss: 0.6290 - val_accuracy: 0.6933 - val_precision: 0.7699 - val_recall: 0.6493\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6317 - accuracy: 0.6875 - precision: 0.6667 - recall: 0.7500\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6252 - accuracy: 0.7069 - precision: 0.7288 - recall: 0.6949 - val_loss: 0.6138 - val_accuracy: 0.6933 - val_precision: 0.7699 - val_recall: 0.6493\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5781 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6134 - accuracy: 0.7080 - precision: 0.7294 - recall: 0.6970 - val_loss: 0.6035 - val_accuracy: 0.6933 - val_precision: 0.7699 - val_recall: 0.6493\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6885 - accuracy: 0.5312 - precision: 0.5556 - recall: 0.5882\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.7069 - precision: 0.7288 - recall: 0.6949 - val_loss: 0.5973 - val_accuracy: 0.6933 - val_precision: 0.7699 - val_recall: 0.6493\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6195 - accuracy: 0.7188 - precision: 0.6429 - recall: 0.6923\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6012 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5943 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6133 - accuracy: 0.6250 - precision: 0.5882 - recall: 0.6667\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5927 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6003 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.7778\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5924 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5527 - accuracy: 0.7500 - precision: 0.8235 - recall: 0.7368\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5928 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7328 - accuracy: 0.5000 - precision: 0.5833 - recall: 0.3889\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5952 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5933 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5759 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.7692\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5930 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7194 - accuracy: 0.5312 - precision: 0.5000 - recall: 0.5333\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5948 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5929 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6053 - accuracy: 0.6875 - precision: 0.6875 - recall: 0.6875\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5946 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5932 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6801 - accuracy: 0.6562 - precision: 0.6316 - recall: 0.7500\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5928 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6685 - accuracy: 0.5938 - precision: 0.6923 - recall: 0.5000\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5929 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5733 - accuracy: 0.7188 - precision: 0.8824 - recall: 0.6818\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5934 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4462 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5942 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5933 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6425 - accuracy: 0.6250 - precision: 0.7857 - recall: 0.5500\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5941 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5941 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4912 - accuracy: 0.7812 - precision: 0.9231 - recall: 0.6667\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5945 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5945 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.8571\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5941 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5939 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5519 - accuracy: 0.7188 - precision: 0.7333 - recall: 0.6875\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5936 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5686 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5940 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5935 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5729 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.7069 - precision: 0.7308 - recall: 0.6909 - val_loss: 0.5938 - val_accuracy: 0.6891 - val_precision: 0.7727 - val_recall: 0.6343\n",
            "Epoch 24: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ST slope\n",
        "- val_accuracy: 0.7731\n",
        "- val_loss: 0.5739\n",
        "- val_precision: 0.7687\n",
        "- val_recall: 0.8175"
      ],
      "metadata": {
        "id": "OS0h0MyH3gE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slop_model = Sequential(name=\"slop_model\")\n",
        "slop_model.add(Input(shape=(1,)))\n",
        "slop_model.add(Dense(16, activation='relu'))\n",
        "slop_model.add(Dense(8, activation='relu'))\n",
        "slop_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "slop_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "slop_history = slop_model.fit(XTRAIN[:, 10], YTRAIN, epochs=100, verbose=1, validation_data=(XVALID[:, 10], YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3wa7oibD2-v9",
        "outputId": "97fc9c45-b7e2-4658-a31b-9ed67581f92c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 17s - loss: 0.7017 - accuracy: 0.5625 - precision: 0.7254 - recall: 0.6776\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6764 - accuracy: 0.5200 - precision: 0.5461 - recall: 0.9221 - val_loss: 0.6529 - val_accuracy: 0.5630 - val_precision: 0.5630 - val_recall: 1.0000\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6457 - accuracy: 0.4688 - precision: 0.4688 - recall: 1.0000\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6426 - accuracy: 0.7826 - precision: 0.7667 - recall: 0.8364 - val_loss: 0.6317 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6400 - accuracy: 0.7812 - precision: 0.7857 - recall: 0.7333\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.7952 - precision: 0.7852 - recall: 0.8343 - val_loss: 0.6187 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7218 - accuracy: 0.6250 - precision: 0.4762 - recall: 0.9091\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6149 - accuracy: 0.7952 - precision: 0.7852 - recall: 0.8343 - val_loss: 0.6104 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6060 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.7952 - precision: 0.7852 - recall: 0.8343 - val_loss: 0.6061 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6588 - accuracy: 0.7500 - precision: 0.6842 - recall: 0.8667\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.6033 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5983 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6000 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.6009 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5623 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 8: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5988 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5908 - accuracy: 0.7188 - precision: 0.7273 - recall: 0.8421\n",
            "Epoch 9: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5969 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5343 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 10: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5903 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5950 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6196 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.9231\n",
            "Epoch 11: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5874 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5931 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5934 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 12: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5844 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5913 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6040 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 13: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5897 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5225 - accuracy: 0.8125 - precision: 0.9000 - recall: 0.8182\n",
            "Epoch 14: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5788 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5880 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5389 - accuracy: 0.8125 - precision: 0.8421 - recall: 0.8421\n",
            "Epoch 15: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5865 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5875 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.8333\n",
            "Epoch 16: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5849 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4681 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 17: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5709 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5836 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5770 - accuracy: 0.8125 - precision: 0.7222 - recall: 0.9286\n",
            "Epoch 18: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5688 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5824 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4031 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9412\n",
            "Epoch 19: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5810 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5958 - accuracy: 0.7188 - precision: 0.7059 - recall: 0.7500\n",
            "Epoch 20: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5640 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5798 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6473 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 21: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5785 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5669 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.7778\n",
            "Epoch 22: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5593 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5773 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4527 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5763 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5604 - accuracy: 0.7812 - precision: 0.7727 - recall: 0.8947\n",
            "Epoch 24: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5553 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5754 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4579 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 25: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5538 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5745 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5289 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 26: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5518 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5736 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7021 - accuracy: 0.6562 - precision: 0.7083 - recall: 0.8095\n",
            "Epoch 27: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5499 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5730 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6478 - accuracy: 0.7188 - precision: 0.5789 - recall: 0.9167\n",
            "Epoch 28: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5719 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5092 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 29: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5462 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5712 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6274 - accuracy: 0.7188 - precision: 0.6923 - recall: 0.6429\n",
            "Epoch 30: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5706 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6825 - accuracy: 0.6562 - precision: 0.6364 - recall: 0.8235\n",
            "Epoch 31: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5430 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5699 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5732 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 32: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5692 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5550 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.7692\n",
            "Epoch 33: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5401 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5686 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4190 - accuracy: 0.8750 - precision: 0.9474 - recall: 0.8571\n",
            "Epoch 34: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5386 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5680 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5321 - accuracy: 0.8438 - precision: 0.7895 - recall: 0.9375\n",
            "Epoch 35: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5676 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4659 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 36: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5671 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5388 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 37: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5667 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5535 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.9474\n",
            "Epoch 38: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5662 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6121 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 39: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5319 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5658 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4509 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 40: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5307 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5655 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6219 - accuracy: 0.6875 - precision: 0.7000 - recall: 0.7778\n",
            "Epoch 41: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5297 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5653 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5570 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.7778\n",
            "Epoch 42: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5286 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5649 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5134 - accuracy: 0.8125 - precision: 0.7333 - recall: 0.8462\n",
            "Epoch 43: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5647 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7376 - accuracy: 0.6250 - precision: 0.5455 - recall: 0.8571\n",
            "Epoch 44: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5647 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.8125 - precision: 0.7692 - recall: 0.7692\n",
            "Epoch 45: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5643 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6178 - accuracy: 0.7188 - precision: 0.6842 - recall: 0.8125\n",
            "Epoch 46: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5642 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5720 - accuracy: 0.7188 - precision: 0.8333 - recall: 0.7143\n",
            "Epoch 47: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5243 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5640 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5616 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 48: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5639 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4464 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.9130\n",
            "Epoch 49: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5638 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5606 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8824\n",
            "Epoch 50: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5637 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6047 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.8000\n",
            "Epoch 51: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5210 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5636 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5522 - accuracy: 0.7500 - precision: 0.7826 - recall: 0.8571\n",
            "Epoch 52: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5635 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4367 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 53: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5637 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5011 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 54: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5637 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5153 - accuracy: 0.7812 - precision: 0.8421 - recall: 0.8000\n",
            "Epoch 55: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5188 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5635 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5384 - accuracy: 0.7812 - precision: 0.7857 - recall: 0.7333\n",
            "Epoch 56: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5181 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5635 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5023 - accuracy: 0.8125 - precision: 0.7059 - recall: 0.9231\n",
            "Epoch 57: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5638 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5938 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 58: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5635 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4252 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.7500\n",
            "Epoch 59: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5167 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5636 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5723 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 60: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5160 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5636 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5678 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.8750\n",
            "Epoch 61: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5154 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5637 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5323 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 62: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5638 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5272 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8824\n",
            "Epoch 63: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5639 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4890 - accuracy: 0.8125 - precision: 0.7826 - recall: 0.9474\n",
            "Epoch 64: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5642 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5320 - accuracy: 0.7812 - precision: 0.7857 - recall: 0.7333\n",
            "Epoch 65: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5640 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4497 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 66: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5642 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4831 - accuracy: 0.8125 - precision: 0.9231 - recall: 0.7059\n",
            "Epoch 67: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5131 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5645 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6437 - accuracy: 0.6875 - precision: 0.6800 - recall: 0.8947\n",
            "Epoch 68: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5130 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5644 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4232 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5124 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5645 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6748 - accuracy: 0.6562 - precision: 0.6667 - recall: 0.7059\n",
            "Epoch 70: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5123 - accuracy: 0.7941 - precision: 0.7848 - recall: 0.8323 - val_loss: 0.5649 - val_accuracy: 0.7479 - val_precision: 0.7681 - val_recall: 0.7910\n",
            "Epoch 70: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot feature importance"
      ],
      "metadata": {
        "id": "UhEewrzc_TsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sl = slop_history.history['val_accuracy'][-1]\n",
        "pk = peak_history.history['val_accuracy'][-1]\n",
        "age = age_history.history['val_accuracy'][-1]\n",
        "sex = sex_history.history['val_accuracy'][-1]\n",
        "ecg = ecg_history.history['val_accuracy'][-1]\n",
        "ang = ang_history.history['val_accuracy'][-1]\n",
        "chest = chest_history.history['val_accuracy'][-1]\n",
        "bp = bp_history.history['val_accuracy'][-1]\n",
        "ch = ch_history.history['val_accuracy'][-1]\n",
        "bs = bs_history.history['val_accuracy'][-1]\n",
        "heart = heart_history.history['val_accuracy'][-1]\n",
        "def plot_feature_importance():\n",
        "  cols = ['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol', 'fasting blood sugar','resting ecg', 'max heart rate', 'exercise angina', 'oldpeak', 'ST slope']\n",
        "  vals = [age, sex, chest, bp, ch, bs, ecg, heart, ang, pk, sl]\n",
        "  plt.bar(cols, vals)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim(.5, 1)\n",
        "  plt.xlabel('Features')\n",
        "  plt.title('Feature importance')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()\n",
        "\n",
        "plot_feature_importance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "3RMkuDJlCuwe",
        "outputId": "13d2ab72-ad41-4287-8af9-fe191241695a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol', 'fasting blood sugar',\n",
        "# 'resting ecg', 'max heart rate', 'exercise angina', 'oldpeak', 'ST slope', 'target']\n"
      ],
      "metadata": {
        "id": "kK28a8R9FOIr"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove resting ecg\n",
        "- val_accuracy: 0.8235\n",
        "- val_loss: 0.3971\n",
        "- val_precision: 0.8043\n",
        "- val_recall: 0.8810"
      ],
      "metadata": {
        "id": "vAhZ-6tVK9LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ETRAIN = np.delete(XTRAIN, 6, axis=1 )\n",
        "EVALID = np.delete(XVALID, 6, axis=1 )\n",
        "less_ecg_model = Sequential(name=\"less_ecg_model\")\n",
        "less_ecg_model.add(Input(shape=(10,)))\n",
        "less_ecg_model.add(Dense(16, activation='relu'))\n",
        "less_ecg_model.add(Dense(8, activation='relu'))\n",
        "less_ecg_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_ecg_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_ecg_history = less_ecg_model.fit(ETRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(EVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y8_P7VTjF4BW",
        "outputId": "0d0be587-fb57-45f4-837b-871d92c16c94"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.8897 - accuracy: 0.2812 - precision: 0.7019 - recall: 0.7635\n",
            "Epoch 1: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7533 - accuracy: 0.4023 - precision: 0.5000 - recall: 0.6184 - val_loss: 0.7439 - val_accuracy: 0.4622 - val_precision: 0.5211 - val_recall: 0.5522\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6582 - accuracy: 0.5938 - precision: 0.6957 - recall: 0.7273\n",
            "Epoch 2: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6781 - accuracy: 0.6239 - precision: 0.6266 - recall: 0.6848 - val_loss: 0.6845 - val_accuracy: 0.6429 - val_precision: 0.6929 - val_recall: 0.6567\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6054 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 3: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6262 - accuracy: 0.7237 - precision: 0.7377 - recall: 0.7273 - val_loss: 0.6362 - val_accuracy: 0.7269 - val_precision: 0.7594 - val_recall: 0.7537\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5837 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 4: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5783 - accuracy: 0.7742 - precision: 0.7905 - recall: 0.7697 - val_loss: 0.5895 - val_accuracy: 0.7521 - val_precision: 0.7863 - val_recall: 0.7687\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5453 - accuracy: 0.7188 - precision: 0.6111 - recall: 0.8462\n",
            "Epoch 5: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.8067 - precision: 0.8246 - recall: 0.7980 - val_loss: 0.5473 - val_accuracy: 0.7731 - val_precision: 0.8077 - val_recall: 0.7836\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5027 - accuracy: 0.8750 - precision: 0.9524 - recall: 0.8696\n",
            "Epoch 6: val_loss did not improve from 0.50927\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4972 - accuracy: 0.8204 - precision: 0.8361 - recall: 0.8141 - val_loss: 0.5095 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4908 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 7: val_loss improved from 0.50927 to 0.47601, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4649 - accuracy: 0.8330 - precision: 0.8443 - recall: 0.8323 - val_loss: 0.4760 - val_accuracy: 0.8025 - val_precision: 0.8372 - val_recall: 0.8060\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4262 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 8: val_loss improved from 0.47601 to 0.44854, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4390 - accuracy: 0.8382 - precision: 0.8473 - recall: 0.8404 - val_loss: 0.4485 - val_accuracy: 0.8109 - val_precision: 0.8346 - val_recall: 0.8284\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6139 - accuracy: 0.6562 - precision: 0.5625 - recall: 0.6923\n",
            "Epoch 9: val_loss improved from 0.44854 to 0.42757, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8435 - precision: 0.8474 - recall: 0.8525 - val_loss: 0.4276 - val_accuracy: 0.8109 - val_precision: 0.8346 - val_recall: 0.8284\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3464 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 10: val_loss improved from 0.42757 to 0.41164, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4048 - accuracy: 0.8435 - precision: 0.8460 - recall: 0.8545 - val_loss: 0.4116 - val_accuracy: 0.8151 - val_precision: 0.8409 - val_recall: 0.8284\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3961 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 11: val_loss improved from 0.41164 to 0.40035, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3945 - accuracy: 0.8466 - precision: 0.8497 - recall: 0.8566 - val_loss: 0.4004 - val_accuracy: 0.8235 - val_precision: 0.8433 - val_recall: 0.8433\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4879 - accuracy: 0.8125 - precision: 0.9412 - recall: 0.7619\n",
            "Epoch 12: val_loss improved from 0.40035 to 0.39146, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3875 - accuracy: 0.8508 - precision: 0.8523 - recall: 0.8626 - val_loss: 0.3915 - val_accuracy: 0.8361 - val_precision: 0.8571 - val_recall: 0.8507\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3724 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 13: val_loss improved from 0.39146 to 0.38423, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3816 - accuracy: 0.8550 - precision: 0.8577 - recall: 0.8646 - val_loss: 0.3842 - val_accuracy: 0.8361 - val_precision: 0.8571 - val_recall: 0.8507\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3179 - accuracy: 0.8438 - precision: 0.7647 - recall: 0.9286\n",
            "Epoch 14: val_loss improved from 0.38423 to 0.37848, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8550 - precision: 0.8535 - recall: 0.8707 - val_loss: 0.3785 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2272 - accuracy: 0.9375 - precision: 0.9545 - recall: 0.9545\n",
            "Epoch 15: val_loss improved from 0.37848 to 0.37440, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8550 - precision: 0.8521 - recall: 0.8727 - val_loss: 0.3744 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3256 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 16: val_loss improved from 0.37440 to 0.37011, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3697 - accuracy: 0.8561 - precision: 0.8524 - recall: 0.8747 - val_loss: 0.3701 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4752 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 17: val_loss improved from 0.37011 to 0.36686, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3666 - accuracy: 0.8561 - precision: 0.8524 - recall: 0.8747 - val_loss: 0.3669 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4346 - accuracy: 0.8438 - precision: 0.7059 - recall: 1.0000\n",
            "Epoch 18: val_loss improved from 0.36686 to 0.36490, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8571 - precision: 0.8527 - recall: 0.8768 - val_loss: 0.3649 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2935 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 19: val_loss improved from 0.36490 to 0.36153, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8561 - precision: 0.8538 - recall: 0.8727 - val_loss: 0.3615 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2512 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 20: val_loss improved from 0.36153 to 0.35954, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8592 - precision: 0.8532 - recall: 0.8808 - val_loss: 0.3595 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5643 - accuracy: 0.7188 - precision: 0.7222 - recall: 0.7647\n",
            "Epoch 21: val_loss improved from 0.35954 to 0.35745, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8582 - precision: 0.8529 - recall: 0.8788 - val_loss: 0.3574 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3088 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 22: val_loss improved from 0.35745 to 0.35661, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3538 - accuracy: 0.8603 - precision: 0.8521 - recall: 0.8848 - val_loss: 0.3566 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5464 - accuracy: 0.7188 - precision: 0.5714 - recall: 0.7273\n",
            "Epoch 23: val_loss improved from 0.35661 to 0.35548, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3521 - accuracy: 0.8603 - precision: 0.8535 - recall: 0.8828 - val_loss: 0.3555 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4611 - accuracy: 0.7500 - precision: 0.5625 - recall: 0.9000\n",
            "Epoch 24: val_loss improved from 0.35548 to 0.35341, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3503 - accuracy: 0.8592 - precision: 0.8560 - recall: 0.8768 - val_loss: 0.3534 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4732 - accuracy: 0.7812 - precision: 0.7826 - recall: 0.9000\n",
            "Epoch 25: val_loss improved from 0.35341 to 0.35154, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.8592 - precision: 0.8546 - recall: 0.8788 - val_loss: 0.3515 - val_accuracy: 0.8655 - val_precision: 0.8864 - val_recall: 0.8731\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4620 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 26: val_loss improved from 0.35154 to 0.35055, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3468 - accuracy: 0.8624 - precision: 0.8555 - recall: 0.8848 - val_loss: 0.3506 - val_accuracy: 0.8571 - val_precision: 0.8846 - val_recall: 0.8582\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3739 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 27: val_loss improved from 0.35055 to 0.34894, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3448 - accuracy: 0.8634 - precision: 0.8614 - recall: 0.8788 - val_loss: 0.3489 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2053 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 28: val_loss improved from 0.34894 to 0.34794, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3432 - accuracy: 0.8613 - precision: 0.8580 - recall: 0.8788 - val_loss: 0.3479 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4080 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 29: val_loss improved from 0.34794 to 0.34734, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3416 - accuracy: 0.8634 - precision: 0.8600 - recall: 0.8808 - val_loss: 0.3473 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4051 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 30: val_loss improved from 0.34734 to 0.34584, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3398 - accuracy: 0.8634 - precision: 0.8571 - recall: 0.8848 - val_loss: 0.3458 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2448 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 31: val_loss improved from 0.34584 to 0.34372, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3383 - accuracy: 0.8634 - precision: 0.8600 - recall: 0.8808 - val_loss: 0.3437 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3078 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 32: val_loss improved from 0.34372 to 0.34323, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.8645 - precision: 0.8617 - recall: 0.8808 - val_loss: 0.3432 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3709 - accuracy: 0.8125 - precision: 0.7222 - recall: 0.9286\n",
            "Epoch 33: val_loss improved from 0.34323 to 0.34276, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3363 - accuracy: 0.8634 - precision: 0.8571 - recall: 0.8848 - val_loss: 0.3428 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1965 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.34276\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8613 - precision: 0.8552 - recall: 0.8828 - val_loss: 0.3430 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2444 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.34276\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8655 - precision: 0.8591 - recall: 0.8869 - val_loss: 0.3432 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2918 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 36: val_loss improved from 0.34276 to 0.34141, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3332 - accuracy: 0.8676 - precision: 0.8625 - recall: 0.8869 - val_loss: 0.3414 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3915 - accuracy: 0.7812 - precision: 0.8750 - recall: 0.7368\n",
            "Epoch 37: val_loss improved from 0.34141 to 0.34032, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.8645 - precision: 0.8588 - recall: 0.8848 - val_loss: 0.3403 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2161 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 38: val_loss improved from 0.34032 to 0.33938, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8666 - precision: 0.8608 - recall: 0.8869 - val_loss: 0.3394 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4006 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 39: val_loss improved from 0.33938 to 0.33863, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3308 - accuracy: 0.8687 - precision: 0.8613 - recall: 0.8909 - val_loss: 0.3386 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2821 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9375\n",
            "Epoch 40: val_loss improved from 0.33863 to 0.33730, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.8676 - precision: 0.8583 - recall: 0.8929 - val_loss: 0.3373 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4364 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 41: val_loss improved from 0.33730 to 0.33690, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3292 - accuracy: 0.8676 - precision: 0.8596 - recall: 0.8909 - val_loss: 0.3369 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.9474\n",
            "Epoch 42: val_loss improved from 0.33690 to 0.33627, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3283 - accuracy: 0.8687 - precision: 0.8613 - recall: 0.8909 - val_loss: 0.3363 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1597 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 43: val_loss improved from 0.33627 to 0.33580, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3274 - accuracy: 0.8697 - precision: 0.8602 - recall: 0.8949 - val_loss: 0.3358 - val_accuracy: 0.8487 - val_precision: 0.8712 - val_recall: 0.8582\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2849 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 44: val_loss did not improve from 0.33580\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8708 - precision: 0.8619 - recall: 0.8949 - val_loss: 0.3366 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2658 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 45: val_loss improved from 0.33580 to 0.33424, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3260 - accuracy: 0.8708 - precision: 0.8619 - recall: 0.8949 - val_loss: 0.3342 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2358 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 46: val_loss improved from 0.33424 to 0.33365, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.8708 - precision: 0.8619 - recall: 0.8949 - val_loss: 0.3337 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4621 - accuracy: 0.7500 - precision: 0.7368 - recall: 0.8235\n",
            "Epoch 47: val_loss improved from 0.33365 to 0.33194, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8718 - precision: 0.8635 - recall: 0.8949 - val_loss: 0.3319 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3552 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 48: val_loss improved from 0.33194 to 0.33168, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8718 - precision: 0.8635 - recall: 0.8949 - val_loss: 0.3317 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2841 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 49: val_loss improved from 0.33168 to 0.33137, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3227 - accuracy: 0.8708 - precision: 0.8619 - recall: 0.8949 - val_loss: 0.3314 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1919 - accuracy: 0.9375 - precision: 0.8667 - recall: 1.0000\n",
            "Epoch 50: val_loss improved from 0.33137 to 0.32961, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.8729 - precision: 0.8638 - recall: 0.8970 - val_loss: 0.3296 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.9231\n",
            "Epoch 51: val_loss improved from 0.32961 to 0.32905, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8729 - precision: 0.8638 - recall: 0.8970 - val_loss: 0.3291 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2501 - accuracy: 0.8750 - precision: 0.9333 - recall: 0.8235\n",
            "Epoch 52: val_loss improved from 0.32905 to 0.32839, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8718 - precision: 0.8621 - recall: 0.8970 - val_loss: 0.3284 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4361 - accuracy: 0.7188 - precision: 0.7059 - recall: 0.7500\n",
            "Epoch 53: val_loss improved from 0.32839 to 0.32754, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8718 - precision: 0.8635 - recall: 0.8949 - val_loss: 0.3275 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2162 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 54: val_loss did not improve from 0.32754\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8718 - precision: 0.8621 - recall: 0.8970 - val_loss: 0.3276 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3292 - accuracy: 0.8750 - precision: 0.9375 - recall: 0.8333\n",
            "Epoch 55: val_loss improved from 0.32754 to 0.32528, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8718 - precision: 0.8621 - recall: 0.8970 - val_loss: 0.3253 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3959 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 56: val_loss did not improve from 0.32528\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8718 - precision: 0.8607 - recall: 0.8990 - val_loss: 0.3261 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2520 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 57: val_loss improved from 0.32528 to 0.32495, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8708 - precision: 0.8605 - recall: 0.8970 - val_loss: 0.3249 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2082 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9000\n",
            "Epoch 58: val_loss improved from 0.32495 to 0.32332, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3162 - accuracy: 0.8739 - precision: 0.8655 - recall: 0.8970 - val_loss: 0.3233 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2036 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 59: val_loss did not improve from 0.32332\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8718 - precision: 0.8607 - recall: 0.8990 - val_loss: 0.3237 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3652 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 60: val_loss improved from 0.32332 to 0.32106, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8750 - precision: 0.8672 - recall: 0.8970 - val_loss: 0.3211 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4155 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 61: val_loss did not improve from 0.32106\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8739 - precision: 0.8655 - recall: 0.8970 - val_loss: 0.3231 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2529 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8333\n",
            "Epoch 62: val_loss did not improve from 0.32106\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.8718 - precision: 0.8635 - recall: 0.8949 - val_loss: 0.3220 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3102 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 63: val_loss improved from 0.32106 to 0.31993, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3124 - accuracy: 0.8761 - precision: 0.8703 - recall: 0.8949 - val_loss: 0.3199 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2434 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 64: val_loss did not improve from 0.31993\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3116 - accuracy: 0.8750 - precision: 0.8658 - recall: 0.8990 - val_loss: 0.3209 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2628 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 65: val_loss improved from 0.31993 to 0.31853, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3111 - accuracy: 0.8750 - precision: 0.8672 - recall: 0.8970 - val_loss: 0.3185 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2476 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 66: val_loss improved from 0.31853 to 0.31795, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8761 - precision: 0.8689 - recall: 0.8970 - val_loss: 0.3180 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3495 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 67: val_loss did not improve from 0.31795\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8771 - precision: 0.8706 - recall: 0.8970 - val_loss: 0.3184 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3525 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.9231\n",
            "Epoch 68: val_loss improved from 0.31795 to 0.31619, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3091 - accuracy: 0.8761 - precision: 0.8689 - recall: 0.8970 - val_loss: 0.3162 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2671 - accuracy: 0.9062 - precision: 0.8235 - recall: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.31619\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8750 - precision: 0.8658 - recall: 0.8990 - val_loss: 0.3171 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4043 - accuracy: 0.8125 - precision: 0.9333 - recall: 0.7368\n",
            "Epoch 70: val_loss did not improve from 0.31619\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.8792 - precision: 0.8711 - recall: 0.9010 - val_loss: 0.3168 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1955 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 71: val_loss improved from 0.31619 to 0.31414, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3067 - accuracy: 0.8771 - precision: 0.8720 - recall: 0.8949 - val_loss: 0.3141 - val_accuracy: 0.8571 - val_precision: 0.8788 - val_recall: 0.8657\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3319 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 72: val_loss did not improve from 0.31414\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8771 - precision: 0.8677 - recall: 0.9010 - val_loss: 0.3147 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5804 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 73: val_loss improved from 0.31414 to 0.31233, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3052 - accuracy: 0.8782 - precision: 0.8708 - recall: 0.8990 - val_loss: 0.3123 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3075 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 74: val_loss did not improve from 0.31233\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.8771 - precision: 0.8677 - recall: 0.9010 - val_loss: 0.3135 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1835 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 75: val_loss improved from 0.31233 to 0.31166, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3043 - accuracy: 0.8761 - precision: 0.8718 - recall: 0.8929 - val_loss: 0.3117 - val_accuracy: 0.8613 - val_precision: 0.8855 - val_recall: 0.8657\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3318 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 76: val_loss did not improve from 0.31166\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8750 - precision: 0.8715 - recall: 0.8909 - val_loss: 0.3131 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4384 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 77: val_loss improved from 0.31166 to 0.31151, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.8824 - precision: 0.8838 - recall: 0.8909 - val_loss: 0.3115 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2845 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 78: val_loss improved from 0.31151 to 0.30973, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3026 - accuracy: 0.8813 - precision: 0.8820 - recall: 0.8909 - val_loss: 0.3097 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1829 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 79: val_loss improved from 0.30973 to 0.30915, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3017 - accuracy: 0.8824 - precision: 0.8822 - recall: 0.8929 - val_loss: 0.3091 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2733 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 80: val_loss improved from 0.30915 to 0.30882, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3014 - accuracy: 0.8803 - precision: 0.8818 - recall: 0.8889 - val_loss: 0.3088 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1346 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9524\n",
            "Epoch 81: val_loss improved from 0.30882 to 0.30840, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3008 - accuracy: 0.8824 - precision: 0.8822 - recall: 0.8929 - val_loss: 0.3084 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2311 - accuracy: 0.9062 - precision: 0.8571 - recall: 1.0000\n",
            "Epoch 82: val_loss improved from 0.30840 to 0.30686, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2999 - accuracy: 0.8834 - precision: 0.8825 - recall: 0.8949 - val_loss: 0.3069 - val_accuracy: 0.8655 - val_precision: 0.8923 - val_recall: 0.8657\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3995 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 83: val_loss improved from 0.30686 to 0.30683, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2995 - accuracy: 0.8834 - precision: 0.8840 - recall: 0.8929 - val_loss: 0.3068 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2447 - accuracy: 0.9062 - precision: 0.9167 - recall: 0.9565\n",
            "Epoch 84: val_loss did not improve from 0.30683\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8845 - precision: 0.8858 - recall: 0.8929 - val_loss: 0.3074 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4268 - accuracy: 0.8750 - precision: 0.7857 - recall: 0.9167\n",
            "Epoch 85: val_loss did not improve from 0.30683\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2986 - accuracy: 0.8855 - precision: 0.8907 - recall: 0.8889 - val_loss: 0.3069 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3464 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8095\n",
            "Epoch 86: val_loss improved from 0.30683 to 0.30658, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2976 - accuracy: 0.8887 - precision: 0.8898 - recall: 0.8970 - val_loss: 0.3066 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3351 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 87: val_loss improved from 0.30658 to 0.30617, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2974 - accuracy: 0.8897 - precision: 0.8963 - recall: 0.8909 - val_loss: 0.3062 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4281 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.7692\n",
            "Epoch 88: val_loss did not improve from 0.30617\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8866 - precision: 0.8893 - recall: 0.8929 - val_loss: 0.3062 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3550 - accuracy: 0.8750 - precision: 0.7333 - recall: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.30617\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8887 - precision: 0.8929 - recall: 0.8929 - val_loss: 0.3078 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2343 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 90: val_loss improved from 0.30617 to 0.30606, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2957 - accuracy: 0.8918 - precision: 0.8968 - recall: 0.8949 - val_loss: 0.3061 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3120 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 91: val_loss improved from 0.30606 to 0.30461, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2949 - accuracy: 0.8887 - precision: 0.8929 - recall: 0.8929 - val_loss: 0.3046 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2366 - accuracy: 0.9375 - precision: 0.8667 - recall: 1.0000\n",
            "Epoch 92: val_loss improved from 0.30461 to 0.30407, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2947 - accuracy: 0.8866 - precision: 0.8893 - recall: 0.8929 - val_loss: 0.3041 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2614 - accuracy: 0.9062 - precision: 0.8750 - recall: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.30407\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8908 - precision: 0.8949 - recall: 0.8949 - val_loss: 0.3045 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2990 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 94: val_loss improved from 0.30407 to 0.30345, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2933 - accuracy: 0.8855 - precision: 0.8860 - recall: 0.8949 - val_loss: 0.3034 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3552 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 95: val_loss improved from 0.30345 to 0.30255, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2930 - accuracy: 0.8855 - precision: 0.8923 - recall: 0.8869 - val_loss: 0.3026 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2332 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 96: val_loss improved from 0.30255 to 0.30110, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2926 - accuracy: 0.8866 - precision: 0.8893 - recall: 0.8929 - val_loss: 0.3011 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3719 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 97: val_loss improved from 0.30110 to 0.30046, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2920 - accuracy: 0.8887 - precision: 0.8898 - recall: 0.8970 - val_loss: 0.3005 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3428 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 98: val_loss did not improve from 0.30046\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8876 - precision: 0.8896 - recall: 0.8949 - val_loss: 0.3018 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2273 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 99: val_loss did not improve from 0.30046\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8918 - precision: 0.8984 - recall: 0.8929 - val_loss: 0.3014 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2275 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8750\n",
            "Epoch 100: val_loss improved from 0.30046 to 0.29998, saving model to feature_model.keras\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.8897 - precision: 0.8947 - recall: 0.8929 - val_loss: 0.3000 - val_accuracy: 0.8613 - val_precision: 0.8976 - val_recall: 0.8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove fasting blood sugar\n",
        "- val_accuracy: 0.8067\n",
        "- val_loss: 0.4022\n",
        "- val_precision: 0.7941\n",
        "- val_recall: 0.8571"
      ],
      "metadata": {
        "id": "bI5KJkkvLJNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FTRAIN = np.delete(ETRAIN, 5, axis=1 )\n",
        "FVALID = np.delete(EVALID, 5, axis=1 )\n",
        "less_fasting_model = Sequential(name=\"less_fasting_model\")\n",
        "less_fasting_model.add(Input(shape=(9,)))\n",
        "less_fasting_model.add(Dense(16, activation='relu'))\n",
        "less_fasting_model.add(Dense(8, activation='relu'))\n",
        "less_fasting_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_fasting_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_fasting_history = less_fasting_model.fit(FTRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(FVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ypmw6P6HGZD5",
        "outputId": "d08c6c4f-55be-4543-d18d-1db4ade3c2b4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6756 - accuracy: 0.5000 - precision: 0.8366 - recall: 0.8421\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6008 - accuracy: 0.6733 - precision: 0.6908 - recall: 0.8203 - val_loss: 0.5715 - val_accuracy: 0.6975 - val_precision: 0.7067 - val_recall: 0.7910\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6084 - accuracy: 0.7188 - precision: 0.6316 - recall: 0.8571\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5281 - accuracy: 0.7742 - precision: 0.7545 - recall: 0.8384 - val_loss: 0.5145 - val_accuracy: 0.7605 - val_precision: 0.7895 - val_recall: 0.7836\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5077 - accuracy: 0.6875 - precision: 0.8125 - recall: 0.6500\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.8015 - precision: 0.7988 - recall: 0.8263 - val_loss: 0.4783 - val_accuracy: 0.8067 - val_precision: 0.8438 - val_recall: 0.8060\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6043 - accuracy: 0.6875 - precision: 0.6818 - recall: 0.8333\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4570 - accuracy: 0.8109 - precision: 0.8208 - recall: 0.8141 - val_loss: 0.4526 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4924 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8246 - precision: 0.8431 - recall: 0.8141 - val_loss: 0.4351 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4130 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.8256 - precision: 0.8434 - recall: 0.8162 - val_loss: 0.4223 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3207 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8319 - precision: 0.8454 - recall: 0.8283 - val_loss: 0.4138 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3778 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4070 - accuracy: 0.8372 - precision: 0.8512 - recall: 0.8323 - val_loss: 0.4057 - val_accuracy: 0.8235 - val_precision: 0.8710 - val_recall: 0.8060\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4901 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8393 - precision: 0.8562 - recall: 0.8303 - val_loss: 0.3965 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3159 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8393 - precision: 0.8504 - recall: 0.8384 - val_loss: 0.3910 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4095 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3910 - accuracy: 0.8372 - precision: 0.8427 - recall: 0.8444 - val_loss: 0.3878 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4372 - accuracy: 0.7500 - precision: 0.8333 - recall: 0.6250\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8414 - precision: 0.8525 - recall: 0.8404 - val_loss: 0.3820 - val_accuracy: 0.8403 - val_precision: 0.8934 - val_recall: 0.8134\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3824 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8424 - precision: 0.8513 - recall: 0.8444 - val_loss: 0.3765 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2669 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8403 - precision: 0.8479 - recall: 0.8444 - val_loss: 0.3733 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3529 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.7647\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8435 - precision: 0.8516 - recall: 0.8465 - val_loss: 0.3697 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3160 - accuracy: 0.8750 - precision: 0.9000 - recall: 0.9000\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8466 - precision: 0.8540 - recall: 0.8505 - val_loss: 0.3680 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2050 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8466 - precision: 0.8525 - recall: 0.8525 - val_loss: 0.3669 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2756 - accuracy: 0.9062 - precision: 0.9474 - recall: 0.9000\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8466 - precision: 0.8540 - recall: 0.8505 - val_loss: 0.3653 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6468 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.7778\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3651 - accuracy: 0.8456 - precision: 0.8522 - recall: 0.8505 - val_loss: 0.3652 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1499 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8508 - precision: 0.8624 - recall: 0.8485 - val_loss: 0.3633 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2650 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8498 - precision: 0.8592 - recall: 0.8505 - val_loss: 0.3614 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4516 - accuracy: 0.8125 - precision: 0.9474 - recall: 0.7826\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.8529 - precision: 0.8615 - recall: 0.8545 - val_loss: 0.3589 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3028 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3577 - accuracy: 0.8508 - precision: 0.8566 - recall: 0.8566 - val_loss: 0.3617 - val_accuracy: 0.8487 - val_precision: 0.9016 - val_recall: 0.8209\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1939 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8582 - precision: 0.8673 - recall: 0.8586 - val_loss: 0.3592 - val_accuracy: 0.8571 - val_precision: 0.9032 - val_recall: 0.8358\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4189 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8561 - precision: 0.8698 - recall: 0.8505 - val_loss: 0.3550 - val_accuracy: 0.8571 - val_precision: 0.9032 - val_recall: 0.8358\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3087 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8519 - precision: 0.8583 - recall: 0.8566 - val_loss: 0.3589 - val_accuracy: 0.8487 - val_precision: 0.9016 - val_recall: 0.8209\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3157 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8603 - precision: 0.8755 - recall: 0.8525 - val_loss: 0.3550 - val_accuracy: 0.8571 - val_precision: 0.9032 - val_recall: 0.8358\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2279 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9091\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8540 - precision: 0.8633 - recall: 0.8545 - val_loss: 0.3532 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1910 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8634 - precision: 0.8747 - recall: 0.8606 - val_loss: 0.3532 - val_accuracy: 0.8613 - val_precision: 0.9040 - val_recall: 0.8433\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2697 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8645 - precision: 0.8765 - recall: 0.8606 - val_loss: 0.3522 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3812 - accuracy: 0.8438 - precision: 0.9167 - recall: 0.7333\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.8634 - precision: 0.8763 - recall: 0.8586 - val_loss: 0.3494 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3988 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8634 - precision: 0.8747 - recall: 0.8606 - val_loss: 0.3498 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3848 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8603 - precision: 0.8709 - recall: 0.8586 - val_loss: 0.3479 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2611 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8666 - precision: 0.8802 - recall: 0.8606 - val_loss: 0.3474 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4215 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8645 - precision: 0.8765 - recall: 0.8606 - val_loss: 0.3461 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4542 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8645 - precision: 0.8735 - recall: 0.8646 - val_loss: 0.3437 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3560 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8634 - precision: 0.8717 - recall: 0.8646 - val_loss: 0.3455 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2600 - accuracy: 0.9062 - precision: 0.9444 - recall: 0.8947\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8655 - precision: 0.8737 - recall: 0.8667 - val_loss: 0.3436 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4621 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3327 - accuracy: 0.8645 - precision: 0.8750 - recall: 0.8626 - val_loss: 0.3433 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3557 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8676 - precision: 0.8727 - recall: 0.8727 - val_loss: 0.3463 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1989 - accuracy: 0.9688 - precision: 0.9231 - recall: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8708 - precision: 0.8796 - recall: 0.8707 - val_loss: 0.3424 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3013 - accuracy: 0.9062 - precision: 0.8500 - recall: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8687 - precision: 0.8745 - recall: 0.8727 - val_loss: 0.3412 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3739 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.7000\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8729 - precision: 0.8832 - recall: 0.8707 - val_loss: 0.3375 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3418 - accuracy: 0.8750 - precision: 0.9412 - recall: 0.8421\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8708 - precision: 0.8750 - recall: 0.8768 - val_loss: 0.3428 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2663 - accuracy: 0.8125 - precision: 0.9048 - recall: 0.8261\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8676 - precision: 0.8742 - recall: 0.8707 - val_loss: 0.3434 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2924 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 46: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8687 - precision: 0.8745 - recall: 0.8727 - val_loss: 0.3415 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2979 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 47: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8729 - precision: 0.8785 - recall: 0.8768 - val_loss: 0.3364 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4345 - accuracy: 0.8438 - precision: 0.7500 - recall: 0.8182\n",
            "Epoch 48: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8729 - precision: 0.8801 - recall: 0.8747 - val_loss: 0.3339 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2849 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 49: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8708 - precision: 0.8735 - recall: 0.8788 - val_loss: 0.3343 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2476 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 50: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8729 - precision: 0.8725 - recall: 0.8848 - val_loss: 0.3355 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1952 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9412\n",
            "Epoch 51: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8729 - precision: 0.8755 - recall: 0.8808 - val_loss: 0.3397 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2764 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8636\n",
            "Epoch 52: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8739 - precision: 0.8788 - recall: 0.8788 - val_loss: 0.3337 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8750 - precision: 0.9167 - recall: 0.7857\n",
            "Epoch 53: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8782 - precision: 0.8767 - recall: 0.8909 - val_loss: 0.3389 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2012 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9444\n",
            "Epoch 54: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3180 - accuracy: 0.8739 - precision: 0.8788 - recall: 0.8788 - val_loss: 0.3342 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2822 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 55: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8708 - precision: 0.8705 - recall: 0.8828 - val_loss: 0.3376 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1186 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8782 - precision: 0.8813 - recall: 0.8848 - val_loss: 0.3331 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1715 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8500\n",
            "Epoch 57: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3157 - accuracy: 0.8739 - precision: 0.8758 - recall: 0.8828 - val_loss: 0.3338 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3360 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 58: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3149 - accuracy: 0.8750 - precision: 0.8745 - recall: 0.8869 - val_loss: 0.3354 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4050 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 59: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8782 - precision: 0.8798 - recall: 0.8869 - val_loss: 0.3301 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3591 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.7727\n",
            "Epoch 60: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8792 - precision: 0.8770 - recall: 0.8929 - val_loss: 0.3340 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1847 - accuracy: 0.9688 - precision: 0.9375 - recall: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8729 - precision: 0.8740 - recall: 0.8828 - val_loss: 0.3318 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4559 - accuracy: 0.7812 - precision: 0.7368 - recall: 0.8750\n",
            "Epoch 62: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8761 - precision: 0.8808 - recall: 0.8808 - val_loss: 0.3275 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1694 - accuracy: 0.9688 - precision: 0.9500 - recall: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.8739 - precision: 0.8728 - recall: 0.8869 - val_loss: 0.3310 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2447 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 64: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8750 - precision: 0.8775 - recall: 0.8828 - val_loss: 0.3302 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2383 - accuracy: 0.9062 - precision: 0.9545 - recall: 0.9130\n",
            "Epoch 65: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3106 - accuracy: 0.8761 - precision: 0.8762 - recall: 0.8869 - val_loss: 0.3329 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2705 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8125\n",
            "Epoch 66: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3293 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3436 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.9524\n",
            "Epoch 67: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8761 - precision: 0.8778 - recall: 0.8848 - val_loss: 0.3341 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2359 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8824\n",
            "Epoch 68: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8761 - precision: 0.8793 - recall: 0.8828 - val_loss: 0.3297 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2159 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 69: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8803 - precision: 0.8848 - recall: 0.8848 - val_loss: 0.3242 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2573 - accuracy: 0.9062 - precision: 0.8500 - recall: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8739 - precision: 0.8758 - recall: 0.8828 - val_loss: 0.3265 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3571 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 71: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8782 - precision: 0.8828 - recall: 0.8828 - val_loss: 0.3260 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3833 - accuracy: 0.8438 - precision: 0.8571 - recall: 0.8000\n",
            "Epoch 72: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3068 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3253 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2805 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 73: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8750 - precision: 0.8760 - recall: 0.8848 - val_loss: 0.3293 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3438 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 74: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8739 - precision: 0.8728 - recall: 0.8869 - val_loss: 0.3316 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2954 - accuracy: 0.8750 - precision: 0.8182 - recall: 0.8182\n",
            "Epoch 75: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8824 - precision: 0.8884 - recall: 0.8848 - val_loss: 0.3266 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4761 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 76: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8824 - precision: 0.8838 - recall: 0.8909 - val_loss: 0.3340 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2373 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8462\n",
            "Epoch 77: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8750 - precision: 0.8760 - recall: 0.8848 - val_loss: 0.3303 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3859 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 78: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8782 - precision: 0.8813 - recall: 0.8848 - val_loss: 0.3281 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1870 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8667\n",
            "Epoch 79: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8824 - precision: 0.8900 - recall: 0.8828 - val_loss: 0.3231 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1710 - accuracy: 0.9688 - precision: 0.9231 - recall: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8782 - precision: 0.8828 - recall: 0.8828 - val_loss: 0.3242 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1231 - accuracy: 0.9688 - precision: 0.9474 - recall: 1.0000\n",
            "Epoch 81: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3017 - accuracy: 0.8824 - precision: 0.8838 - recall: 0.8909 - val_loss: 0.3223 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2521 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 82: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8782 - precision: 0.8782 - recall: 0.8889 - val_loss: 0.3264 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2227 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 83: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8792 - precision: 0.8831 - recall: 0.8848 - val_loss: 0.3296 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1552 - accuracy: 0.9688 - precision: 0.9231 - recall: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8824 - precision: 0.8900 - recall: 0.8828 - val_loss: 0.3210 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2813 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 85: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8834 - precision: 0.8840 - recall: 0.8929 - val_loss: 0.3253 - val_accuracy: 0.8571 - val_precision: 0.8968 - val_recall: 0.8433\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1920 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 86: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8834 - precision: 0.8887 - recall: 0.8869 - val_loss: 0.3266 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5627 - accuracy: 0.7812 - precision: 0.7826 - recall: 0.9000\n",
            "Epoch 87: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8845 - precision: 0.8921 - recall: 0.8848 - val_loss: 0.3211 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2107 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.8182\n",
            "Epoch 88: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.8813 - precision: 0.8805 - recall: 0.8929 - val_loss: 0.3258 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3023 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 89: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2979 - accuracy: 0.8855 - precision: 0.8923 - recall: 0.8869 - val_loss: 0.3203 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1007 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8845 - precision: 0.8842 - recall: 0.8949 - val_loss: 0.3245 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2186 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 91: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2979 - accuracy: 0.8855 - precision: 0.8845 - recall: 0.8970 - val_loss: 0.3259 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2822 - accuracy: 0.9375 - precision: 0.8667 - recall: 1.0000\n",
            "Epoch 92: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8855 - precision: 0.8939 - recall: 0.8848 - val_loss: 0.3233 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4443 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 93: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8866 - precision: 0.8925 - recall: 0.8889 - val_loss: 0.3212 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2723 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 94: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2962 - accuracy: 0.8845 - precision: 0.8842 - recall: 0.8949 - val_loss: 0.3222 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4216 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 95: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8876 - precision: 0.8896 - recall: 0.8949 - val_loss: 0.3199 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3245 - accuracy: 0.8750 - precision: 0.7500 - recall: 1.0000\n",
            "Epoch 96: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8866 - precision: 0.8893 - recall: 0.8929 - val_loss: 0.3198 - val_accuracy: 0.8613 - val_precision: 0.8915 - val_recall: 0.8582\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2522 - accuracy: 0.9062 - precision: 0.9545 - recall: 0.9130\n",
            "Epoch 97: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8897 - precision: 0.8884 - recall: 0.9010 - val_loss: 0.3210 - val_accuracy: 0.8571 - val_precision: 0.8906 - val_recall: 0.8507\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2854 - accuracy: 0.8750 - precision: 0.8182 - recall: 1.0000\n",
            "Epoch 98: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8897 - precision: 0.8900 - recall: 0.8990 - val_loss: 0.3236 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3441 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 99: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8887 - precision: 0.8945 - recall: 0.8909 - val_loss: 0.3232 - val_accuracy: 0.8529 - val_precision: 0.8898 - val_recall: 0.8433\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3373 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 100: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8908 - precision: 0.8934 - recall: 0.8970 - val_loss: 0.3271 - val_accuracy: 0.8529 - val_precision: 0.8960 - val_recall: 0.8358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Cholesterol\n",
        "- val_accuracy: 0.8445\n",
        "- val_loss: 0.3999\n",
        "- val_precision: 0.8296\n",
        "- val_recall: 0.8889"
      ],
      "metadata": {
        "id": "wAqwVg9uLXXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CTRAIN = np.delete(FTRAIN, 4, axis=1 )\n",
        "CVALID = np.delete(FVALID, 4, axis=1 )\n",
        "less_ch_model = Sequential(name=\"less_ch_model\")\n",
        "less_ch_model.add(Input(shape=(8,)))\n",
        "less_ch_model.add(Dense(16, activation='relu'))\n",
        "less_ch_model.add(Dense(8, activation='relu'))\n",
        "less_ch_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_ch_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_ch_history = less_ch_model.fit(CTRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(CVALID, YVALID), callbacks=[model_checkpoint, early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mZqiz5C5Gbv-",
        "outputId": "77bb76da-d0b4-411e-e532-24b47ad64f11"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 27s - loss: 0.7076 - accuracy: 0.5625 - precision: 0.8411 - recall: 0.8355\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6661 - accuracy: 0.6639 - precision: 0.6739 - recall: 0.8442 - val_loss: 0.6396 - val_accuracy: 0.6975 - val_precision: 0.7153 - val_recall: 0.7687\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6595 - accuracy: 0.6562 - precision: 0.6250 - recall: 0.8824\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.7794 - precision: 0.7568 - recall: 0.8485 - val_loss: 0.5883 - val_accuracy: 0.7521 - val_precision: 0.7778 - val_recall: 0.7836\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5530 - accuracy: 0.7812 - precision: 0.8421 - recall: 0.8000\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5606 - accuracy: 0.7994 - precision: 0.7846 - recall: 0.8465 - val_loss: 0.5401 - val_accuracy: 0.7605 - val_precision: 0.7984 - val_recall: 0.7687\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5301 - accuracy: 0.7500 - precision: 0.9231 - recall: 0.6316\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5167 - accuracy: 0.8099 - precision: 0.8091 - recall: 0.8303 - val_loss: 0.4986 - val_accuracy: 0.7731 - val_precision: 0.8175 - val_recall: 0.7687\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5657 - accuracy: 0.7500 - precision: 0.8000 - recall: 0.7059\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.8120 - precision: 0.8185 - recall: 0.8202 - val_loss: 0.4634 - val_accuracy: 0.7857 - val_precision: 0.8268 - val_recall: 0.7836\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4392 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4524 - accuracy: 0.8151 - precision: 0.8275 - recall: 0.8141 - val_loss: 0.4400 - val_accuracy: 0.7941 - val_precision: 0.8295 - val_recall: 0.7985\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5374 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8256 - precision: 0.8310 - recall: 0.8343 - val_loss: 0.4239 - val_accuracy: 0.7983 - val_precision: 0.8359 - val_recall: 0.7985\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4433 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8277 - precision: 0.8357 - recall: 0.8323 - val_loss: 0.4141 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4019 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4141 - accuracy: 0.8298 - precision: 0.8337 - recall: 0.8404 - val_loss: 0.4074 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3258 - accuracy: 0.9062 - precision: 0.9524 - recall: 0.9091\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4084 - accuracy: 0.8309 - precision: 0.8353 - recall: 0.8404 - val_loss: 0.4024 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3972 - accuracy: 0.8750 - precision: 0.9412 - recall: 0.8421\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8330 - precision: 0.8373 - recall: 0.8424 - val_loss: 0.3971 - val_accuracy: 0.8025 - val_precision: 0.8425 - val_recall: 0.7985\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3097 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8309 - precision: 0.8340 - recall: 0.8424 - val_loss: 0.3935 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3478 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8319 - precision: 0.8343 - recall: 0.8444 - val_loss: 0.3896 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1851 - accuracy: 0.9688 - precision: 0.9286 - recall: 1.0000\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8403 - precision: 0.8396 - recall: 0.8566 - val_loss: 0.3861 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4305 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8403 - precision: 0.8383 - recall: 0.8586 - val_loss: 0.3843 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5888 - accuracy: 0.7500 - precision: 0.8462 - recall: 0.6471\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8414 - precision: 0.8399 - recall: 0.8586 - val_loss: 0.3816 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1589 - accuracy: 0.9688 - precision: 0.9412 - recall: 1.0000\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8403 - precision: 0.8383 - recall: 0.8586 - val_loss: 0.3804 - val_accuracy: 0.8067 - val_precision: 0.8492 - val_recall: 0.7985\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3735 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8424 - precision: 0.8402 - recall: 0.8606 - val_loss: 0.3793 - val_accuracy: 0.8151 - val_precision: 0.8629 - val_recall: 0.7985\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4428 - accuracy: 0.8125 - precision: 0.8261 - recall: 0.9048\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8424 - precision: 0.8402 - recall: 0.8606 - val_loss: 0.3783 - val_accuracy: 0.8151 - val_precision: 0.8629 - val_recall: 0.7985\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3612 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8403 - precision: 0.8369 - recall: 0.8606 - val_loss: 0.3767 - val_accuracy: 0.8151 - val_precision: 0.8629 - val_recall: 0.7985\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2186 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8414 - precision: 0.8399 - recall: 0.8586 - val_loss: 0.3751 - val_accuracy: 0.8151 - val_precision: 0.8629 - val_recall: 0.7985\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4930 - accuracy: 0.7188 - precision: 0.6875 - recall: 0.7333\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8414 - precision: 0.8413 - recall: 0.8566 - val_loss: 0.3736 - val_accuracy: 0.8193 - val_precision: 0.8640 - val_recall: 0.8060\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3341 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.8421\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8403 - precision: 0.8410 - recall: 0.8545 - val_loss: 0.3735 - val_accuracy: 0.8193 - val_precision: 0.8640 - val_recall: 0.8060\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3887 - accuracy: 0.8125 - precision: 0.7647 - recall: 0.8667\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8435 - precision: 0.8406 - recall: 0.8626 - val_loss: 0.3744 - val_accuracy: 0.8193 - val_precision: 0.8640 - val_recall: 0.8060\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4301 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8414 - precision: 0.8426 - recall: 0.8545 - val_loss: 0.3719 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3237 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8435 - precision: 0.8419 - recall: 0.8606 - val_loss: 0.3732 - val_accuracy: 0.8193 - val_precision: 0.8640 - val_recall: 0.8060\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3641 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8456 - precision: 0.8466 - recall: 0.8586 - val_loss: 0.3713 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5698 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8466 - precision: 0.8469 - recall: 0.8606 - val_loss: 0.3691 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4388 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8445 - precision: 0.8436 - recall: 0.8606 - val_loss: 0.3700 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3314 - accuracy: 0.8750 - precision: 0.8000 - recall: 0.9231\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8435 - precision: 0.8446 - recall: 0.8566 - val_loss: 0.3681 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4300 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8466 - precision: 0.8483 - recall: 0.8586 - val_loss: 0.3674 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1858 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8462\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8445 - precision: 0.8449 - recall: 0.8586 - val_loss: 0.3675 - val_accuracy: 0.8235 - val_precision: 0.8651 - val_recall: 0.8134\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3997 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8466 - precision: 0.8469 - recall: 0.8606 - val_loss: 0.3662 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2806 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8456 - precision: 0.8452 - recall: 0.8606 - val_loss: 0.3667 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3620 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3608 - accuracy: 0.8466 - precision: 0.8469 - recall: 0.8606 - val_loss: 0.3656 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5134 - accuracy: 0.7188 - precision: 0.6500 - recall: 0.8667\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8477 - precision: 0.8500 - recall: 0.8586 - val_loss: 0.3643 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2825 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8466 - precision: 0.8455 - recall: 0.8626 - val_loss: 0.3660 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2977 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8508 - precision: 0.8537 - recall: 0.8606 - val_loss: 0.3648 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4192 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3573 - accuracy: 0.8456 - precision: 0.8452 - recall: 0.8606 - val_loss: 0.3669 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3571 - accuracy: 0.8750 - precision: 0.7778 - recall: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8487 - precision: 0.8531 - recall: 0.8566 - val_loss: 0.3634 - val_accuracy: 0.8277 - val_precision: 0.8661 - val_recall: 0.8209\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4374 - accuracy: 0.7500 - precision: 0.9333 - recall: 0.6667\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8498 - precision: 0.8520 - recall: 0.8606 - val_loss: 0.3623 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3730 - accuracy: 0.8125 - precision: 0.8889 - recall: 0.8000\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8498 - precision: 0.8534 - recall: 0.8586 - val_loss: 0.3630 - val_accuracy: 0.8361 - val_precision: 0.8800 - val_recall: 0.8209\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2944 - accuracy: 0.8438 - precision: 1.0000 - recall: 0.6875\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8508 - precision: 0.8523 - recall: 0.8626 - val_loss: 0.3636 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3083 - accuracy: 0.7812 - precision: 0.7059 - recall: 0.8571\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8487 - precision: 0.8517 - recall: 0.8586 - val_loss: 0.3634 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2792 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8235\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8508 - precision: 0.8537 - recall: 0.8606 - val_loss: 0.3623 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4086 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 46: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8487 - precision: 0.8531 - recall: 0.8566 - val_loss: 0.3597 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2492 - accuracy: 0.9375 - precision: 0.8182 - recall: 1.0000\n",
            "Epoch 47: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8508 - precision: 0.8551 - recall: 0.8586 - val_loss: 0.3590 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3478 - accuracy: 0.8750 - precision: 0.7143 - recall: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8529 - precision: 0.8586 - recall: 0.8586 - val_loss: 0.3582 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3392 - accuracy: 0.8750 - precision: 0.7778 - recall: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3493 - accuracy: 0.8561 - precision: 0.8609 - recall: 0.8626 - val_loss: 0.3587 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2615 - accuracy: 0.8438 - precision: 0.9444 - recall: 0.8095\n",
            "Epoch 50: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3603 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3246 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 51: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8571 - precision: 0.8641 - recall: 0.8606 - val_loss: 0.3586 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4429 - accuracy: 0.8438 - precision: 0.8636 - recall: 0.9048\n",
            "Epoch 52: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8529 - precision: 0.8557 - recall: 0.8626 - val_loss: 0.3595 - val_accuracy: 0.8445 - val_precision: 0.8943 - val_recall: 0.8209\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2922 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 53: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8529 - precision: 0.8615 - recall: 0.8545 - val_loss: 0.3574 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4474 - accuracy: 0.8125 - precision: 0.6667 - recall: 0.8000\n",
            "Epoch 54: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8561 - precision: 0.8653 - recall: 0.8566 - val_loss: 0.3562 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3619 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 55: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3559 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2354 - accuracy: 0.9062 - precision: 0.8571 - recall: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.8571 - precision: 0.8656 - recall: 0.8586 - val_loss: 0.3563 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1842 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8500\n",
            "Epoch 57: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8582 - precision: 0.8659 - recall: 0.8606 - val_loss: 0.3563 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3208 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 58: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8582 - precision: 0.8673 - recall: 0.8586 - val_loss: 0.3546 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2713 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 59: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8571 - precision: 0.8641 - recall: 0.8606 - val_loss: 0.3571 - val_accuracy: 0.8529 - val_precision: 0.9024 - val_recall: 0.8284\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4045 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 60: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8540 - precision: 0.8633 - recall: 0.8545 - val_loss: 0.3529 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3476 - accuracy: 0.8438 - precision: 0.9474 - recall: 0.8182\n",
            "Epoch 61: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8571 - precision: 0.8641 - recall: 0.8606 - val_loss: 0.3526 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4391 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 62: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8561 - precision: 0.8609 - recall: 0.8626 - val_loss: 0.3547 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1728 - accuracy: 0.9375 - precision: 0.8889 - recall: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8561 - precision: 0.8653 - recall: 0.8566 - val_loss: 0.3512 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2532 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 64: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3514 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2149 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 65: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8561 - precision: 0.8623 - recall: 0.8606 - val_loss: 0.3524 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4711 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 66: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3381 - accuracy: 0.8592 - precision: 0.8661 - recall: 0.8626 - val_loss: 0.3505 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3114 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 67: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8592 - precision: 0.8661 - recall: 0.8626 - val_loss: 0.3508 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3505 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 68: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8592 - precision: 0.8632 - recall: 0.8667 - val_loss: 0.3522 - val_accuracy: 0.8487 - val_precision: 0.8952 - val_recall: 0.8284\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5184 - accuracy: 0.7812 - precision: 0.6875 - recall: 0.8462\n",
            "Epoch 69: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8582 - precision: 0.8629 - recall: 0.8646 - val_loss: 0.3499 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4794 - accuracy: 0.6875 - precision: 0.5714 - recall: 0.6667\n",
            "Epoch 70: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8592 - precision: 0.8632 - recall: 0.8667 - val_loss: 0.3508 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3122 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 71: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.3507 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2537 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 72: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8571 - precision: 0.8612 - recall: 0.8646 - val_loss: 0.3502 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2500 - accuracy: 0.9688 - precision: 0.9231 - recall: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8592 - precision: 0.8632 - recall: 0.8667 - val_loss: 0.3510 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5062 - accuracy: 0.7812 - precision: 0.7647 - recall: 0.8125\n",
            "Epoch 74: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8582 - precision: 0.8659 - recall: 0.8606 - val_loss: 0.3474 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2249 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 75: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3333 - accuracy: 0.8603 - precision: 0.8664 - recall: 0.8646 - val_loss: 0.3488 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4863 - accuracy: 0.7500 - precision: 0.8182 - recall: 0.6000\n",
            "Epoch 76: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3462 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5354 - accuracy: 0.7500 - precision: 0.7059 - recall: 0.8000\n",
            "Epoch 77: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3479 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3761 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.7692\n",
            "Epoch 78: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8571 - precision: 0.8656 - recall: 0.8586 - val_loss: 0.3470 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4175 - accuracy: 0.8438 - precision: 0.9333 - recall: 0.7778\n",
            "Epoch 79: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8592 - precision: 0.8661 - recall: 0.8626 - val_loss: 0.3480 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3323 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 80: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3477 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4026 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 81: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8571 - precision: 0.8656 - recall: 0.8586 - val_loss: 0.3485 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2400 - accuracy: 0.8750 - precision: 0.7500 - recall: 0.9000\n",
            "Epoch 82: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8561 - precision: 0.8653 - recall: 0.8566 - val_loss: 0.3452 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3442 - accuracy: 0.8438 - precision: 0.8889 - recall: 0.6667\n",
            "Epoch 83: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8603 - precision: 0.8694 - recall: 0.8606 - val_loss: 0.3440 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2208 - accuracy: 0.9375 - precision: 0.9048 - recall: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8613 - precision: 0.8697 - recall: 0.8626 - val_loss: 0.3482 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3470 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 85: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8592 - precision: 0.8691 - recall: 0.8586 - val_loss: 0.3478 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4167 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 86: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8603 - precision: 0.8694 - recall: 0.8606 - val_loss: 0.3469 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5456 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 87: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8634 - precision: 0.8717 - recall: 0.8646 - val_loss: 0.3456 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3365 - accuracy: 0.8750 - precision: 0.9474 - recall: 0.8571\n",
            "Epoch 88: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8603 - precision: 0.8694 - recall: 0.8606 - val_loss: 0.3467 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2592 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 89: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8645 - precision: 0.8704 - recall: 0.8687 - val_loss: 0.3461 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2413 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 90: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8603 - precision: 0.8694 - recall: 0.8606 - val_loss: 0.3449 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2778 - accuracy: 0.8750 - precision: 0.8000 - recall: 1.0000\n",
            "Epoch 91: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8603 - precision: 0.8694 - recall: 0.8606 - val_loss: 0.3424 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2644 - accuracy: 0.9375 - precision: 0.9474 - recall: 0.9474\n",
            "Epoch 92: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8645 - precision: 0.8720 - recall: 0.8667 - val_loss: 0.3432 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5062 - accuracy: 0.7812 - precision: 0.8421 - recall: 0.8000\n",
            "Epoch 93: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8666 - precision: 0.8710 - recall: 0.8727 - val_loss: 0.3451 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4474 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 94: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8624 - precision: 0.8730 - recall: 0.8606 - val_loss: 0.3414 - val_accuracy: 0.8487 - val_precision: 0.8889 - val_recall: 0.8358\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2240 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 95: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8655 - precision: 0.8722 - recall: 0.8687 - val_loss: 0.3415 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5754 - accuracy: 0.8125 - precision: 0.6923 - recall: 0.8182\n",
            "Epoch 96: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8666 - precision: 0.8755 - recall: 0.8667 - val_loss: 0.3406 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2937 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 97: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8676 - precision: 0.8742 - recall: 0.8707 - val_loss: 0.3439 - val_accuracy: 0.8403 - val_precision: 0.8871 - val_recall: 0.8209\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2035 - accuracy: 0.9375 - precision: 0.9231 - recall: 0.9231\n",
            "Epoch 98: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8666 - precision: 0.8817 - recall: 0.8586 - val_loss: 0.3402 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3035 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 99: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8697 - precision: 0.8763 - recall: 0.8727 - val_loss: 0.3413 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5241 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 100: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8687 - precision: 0.8760 - recall: 0.8707 - val_loss: 0.3413 - val_accuracy: 0.8445 - val_precision: 0.8880 - val_recall: 0.8284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove resting bp\n",
        "- val_accuracy: 0.8151\n",
        "- val_loss: 0.4053\n",
        "- val_precision: 0.7887\n",
        "- val_recall: 0.8889"
      ],
      "metadata": {
        "id": "eIeFwO7ALnrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RTRAIN = np.delete(CTRAIN, 3, axis=1 )\n",
        "RVALID = np.delete(CVALID, 3, axis=1 )\n",
        "less_resting_model = Sequential(name=\"less_resting_model\")\n",
        "less_resting_model.add(Input(shape=(7,)))\n",
        "less_resting_model.add(Dense(16, activation='relu'))\n",
        "less_resting_model.add(Dense(8, activation='relu'))\n",
        "less_resting_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_resting_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_resting_history = less_resting_model.fit(RTRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(RVALID, YVALID), callbacks=[model_checkpoint, early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2d3MdLN7GioY",
        "outputId": "ac50be84-ad43-4215-e81b-4f11a49b0553"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7484 - accuracy: 0.3438 - precision: 0.8605 - recall: 0.7351\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6535 - accuracy: 0.5410 - precision: 0.8270 - recall: 0.3116 - val_loss: 0.6366 - val_accuracy: 0.6176 - val_precision: 0.8772 - val_recall: 0.3731\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6517 - accuracy: 0.7188 - precision: 0.8889 - recall: 0.5000\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.7269 - precision: 0.8507 - recall: 0.5758 - val_loss: 0.5932 - val_accuracy: 0.7227 - val_precision: 0.8696 - val_recall: 0.5970\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6139 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.6154\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5591 - accuracy: 0.7836 - precision: 0.8516 - recall: 0.7071 - val_loss: 0.5555 - val_accuracy: 0.7773 - val_precision: 0.8857 - val_recall: 0.6940\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5816 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.7143\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.8078 - precision: 0.8451 - recall: 0.7717 - val_loss: 0.5173 - val_accuracy: 0.8109 - val_precision: 0.8938 - val_recall: 0.7537\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5035 - accuracy: 0.8125 - precision: 0.8889 - recall: 0.8000\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.8193 - precision: 0.8344 - recall: 0.8141 - val_loss: 0.4805 - val_accuracy: 0.8361 - val_precision: 0.8862 - val_recall: 0.8134\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4543 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.8571\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.8256 - precision: 0.8257 - recall: 0.8424 - val_loss: 0.4516 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4321 - accuracy: 0.8125 - precision: 0.8667 - recall: 0.7647\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8256 - precision: 0.8245 - recall: 0.8444 - val_loss: 0.4302 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4115 - accuracy: 0.7812 - precision: 0.7727 - recall: 0.8947\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8235 - precision: 0.8187 - recall: 0.8485 - val_loss: 0.4151 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3804 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4090 - accuracy: 0.8288 - precision: 0.8230 - recall: 0.8545 - val_loss: 0.4043 - val_accuracy: 0.8319 - val_precision: 0.8561 - val_recall: 0.8433\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2529 - accuracy: 0.9375 - precision: 0.9333 - recall: 0.9333\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8298 - precision: 0.8233 - recall: 0.8566 - val_loss: 0.3969 - val_accuracy: 0.8319 - val_precision: 0.8507 - val_recall: 0.8507\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4790 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.8421\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8330 - precision: 0.8243 - recall: 0.8626 - val_loss: 0.3918 - val_accuracy: 0.8235 - val_precision: 0.8433 - val_recall: 0.8433\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4349 - accuracy: 0.8125 - precision: 0.9375 - recall: 0.7500\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8309 - precision: 0.8249 - recall: 0.8566 - val_loss: 0.3880 - val_accuracy: 0.8277 - val_precision: 0.8496 - val_recall: 0.8433\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.7812 - precision: 0.7273 - recall: 0.9412\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8340 - precision: 0.8285 - recall: 0.8586 - val_loss: 0.3848 - val_accuracy: 0.8361 - val_precision: 0.8571 - val_recall: 0.8507\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2567 - accuracy: 0.9375 - precision: 0.9286 - recall: 0.9286\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8372 - precision: 0.8333 - recall: 0.8586 - val_loss: 0.3822 - val_accuracy: 0.8361 - val_precision: 0.8571 - val_recall: 0.8507\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3387 - accuracy: 0.7812 - precision: 0.8667 - recall: 0.7222\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8361 - precision: 0.8317 - recall: 0.8586 - val_loss: 0.3811 - val_accuracy: 0.8403 - val_precision: 0.8636 - val_recall: 0.8507\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5539 - accuracy: 0.7500 - precision: 0.6429 - recall: 0.7500\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8403 - precision: 0.8356 - recall: 0.8626 - val_loss: 0.3800 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3318 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8414 - precision: 0.8454 - recall: 0.8505 - val_loss: 0.3776 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3098 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8414 - precision: 0.8426 - recall: 0.8545 - val_loss: 0.3770 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3617 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8424 - precision: 0.8443 - recall: 0.8545 - val_loss: 0.3753 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3827 - accuracy: 0.8750 - precision: 0.8125 - recall: 0.9286\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8445 - precision: 0.8449 - recall: 0.8586 - val_loss: 0.3741 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4582 - accuracy: 0.7500 - precision: 0.6842 - recall: 0.8667\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8445 - precision: 0.8491 - recall: 0.8525 - val_loss: 0.3726 - val_accuracy: 0.8445 - val_precision: 0.8702 - val_recall: 0.8507\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4024 - accuracy: 0.8438 - precision: 0.7692 - recall: 0.8333\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8414 - precision: 0.8440 - recall: 0.8525 - val_loss: 0.3726 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2417 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8445 - precision: 0.8491 - recall: 0.8525 - val_loss: 0.3714 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4298 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8435 - precision: 0.8446 - recall: 0.8566 - val_loss: 0.3710 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5364 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8445 - precision: 0.8477 - recall: 0.8545 - val_loss: 0.3702 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3847 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8456 - precision: 0.8508 - recall: 0.8525 - val_loss: 0.3698 - val_accuracy: 0.8487 - val_precision: 0.8769 - val_recall: 0.8507\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5295 - accuracy: 0.7500 - precision: 0.6500 - recall: 0.9286\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8466 - precision: 0.8483 - recall: 0.8586 - val_loss: 0.3702 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3987 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8435 - precision: 0.8460 - recall: 0.8545 - val_loss: 0.3702 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3218 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8487 - precision: 0.8531 - recall: 0.8566 - val_loss: 0.3701 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3169 - accuracy: 0.8750 - precision: 0.7692 - recall: 0.9091\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8487 - precision: 0.8560 - recall: 0.8525 - val_loss: 0.3689 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3654 - accuracy: 0.8125 - precision: 0.9286 - recall: 0.7222\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3620 - accuracy: 0.8466 - precision: 0.8511 - recall: 0.8545 - val_loss: 0.3691 - val_accuracy: 0.8529 - val_precision: 0.8779 - val_recall: 0.8582\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3878 - accuracy: 0.8125 - precision: 1.0000 - recall: 0.7143\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8477 - precision: 0.8557 - recall: 0.8505 - val_loss: 0.3691 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3166 - accuracy: 0.9062 - precision: 0.8947 - recall: 0.9444\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8508 - precision: 0.8566 - recall: 0.8566 - val_loss: 0.3684 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4153 - accuracy: 0.8125 - precision: 0.9048 - recall: 0.8261\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8519 - precision: 0.8583 - recall: 0.8566 - val_loss: 0.3676 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4631 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8508 - precision: 0.8566 - recall: 0.8566 - val_loss: 0.3682 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2882 - accuracy: 0.9062 - precision: 0.9048 - recall: 0.9500\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8540 - precision: 0.8603 - recall: 0.8586 - val_loss: 0.3678 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3106 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8571 - precision: 0.8656 - recall: 0.8586 - val_loss: 0.3666 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3731 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8571\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8561 - precision: 0.8653 - recall: 0.8566 - val_loss: 0.3675 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4908 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8592 - precision: 0.8691 - recall: 0.8586 - val_loss: 0.3666 - val_accuracy: 0.8361 - val_precision: 0.8740 - val_recall: 0.8284\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3207 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.7500\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8582 - precision: 0.8644 - recall: 0.8626 - val_loss: 0.3675 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4299 - accuracy: 0.8438 - precision: 0.8462 - recall: 0.7857\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8592 - precision: 0.8706 - recall: 0.8566 - val_loss: 0.3667 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6483 - accuracy: 0.7188 - precision: 0.7692 - recall: 0.6250\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8603 - precision: 0.8709 - recall: 0.8586 - val_loss: 0.3674 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5162 - accuracy: 0.7812 - precision: 0.9167 - recall: 0.6471\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8634 - precision: 0.8747 - recall: 0.8606 - val_loss: 0.3664 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5162 - accuracy: 0.7812 - precision: 0.8421 - recall: 0.8000\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8645 - precision: 0.8750 - recall: 0.8626 - val_loss: 0.3659 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1347 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9500\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3506 - accuracy: 0.8645 - precision: 0.8735 - recall: 0.8646 - val_loss: 0.3665 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4674 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 46: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8624 - precision: 0.8730 - recall: 0.8606 - val_loss: 0.3657 - val_accuracy: 0.8403 - val_precision: 0.8810 - val_recall: 0.8284\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2881 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 47: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8645 - precision: 0.8720 - recall: 0.8667 - val_loss: 0.3651 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3415 - accuracy: 0.8750 - precision: 0.9333 - recall: 0.8235\n",
            "Epoch 48: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3482 - accuracy: 0.8645 - precision: 0.8720 - recall: 0.8667 - val_loss: 0.3645 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2388 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8750\n",
            "Epoch 49: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.8592 - precision: 0.8632 - recall: 0.8667 - val_loss: 0.3658 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3633 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 50: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8676 - precision: 0.8758 - recall: 0.8687 - val_loss: 0.3657 - val_accuracy: 0.8445 - val_precision: 0.8819 - val_recall: 0.8358\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2763 - accuracy: 0.8750 - precision: 0.9286 - recall: 0.8125\n",
            "Epoch 51: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8697 - precision: 0.8778 - recall: 0.8707 - val_loss: 0.3647 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1883 - accuracy: 0.9375 - precision: 0.8947 - recall: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8708 - precision: 0.8735 - recall: 0.8788 - val_loss: 0.3659 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3199 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 53: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8750 - precision: 0.8790 - recall: 0.8808 - val_loss: 0.3653 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2067 - accuracy: 0.9062 - precision: 0.8000 - recall: 0.8889\n",
            "Epoch 54: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3655 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3905 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 55: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8739 - precision: 0.8788 - recall: 0.8788 - val_loss: 0.3653 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3246 - accuracy: 0.9062 - precision: 0.8462 - recall: 0.9167\n",
            "Epoch 56: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8697 - precision: 0.8747 - recall: 0.8747 - val_loss: 0.3669 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2845 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 57: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3667 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2002 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9333\n",
            "Epoch 58: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8729 - precision: 0.8785 - recall: 0.8768 - val_loss: 0.3644 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3099 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 59: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8729 - precision: 0.8770 - recall: 0.8788 - val_loss: 0.3641 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4520 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.8571\n",
            "Epoch 60: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3655 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4370 - accuracy: 0.8438 - precision: 0.9444 - recall: 0.8095\n",
            "Epoch 61: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3644 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2647 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 62: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8708 - precision: 0.8765 - recall: 0.8747 - val_loss: 0.3640 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3241 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 63: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8729 - precision: 0.8755 - recall: 0.8808 - val_loss: 0.3650 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3778 - accuracy: 0.8438 - precision: 0.8000 - recall: 0.9412\n",
            "Epoch 64: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.8718 - precision: 0.8753 - recall: 0.8788 - val_loss: 0.3646 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3772 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 65: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8697 - precision: 0.8732 - recall: 0.8768 - val_loss: 0.3667 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3001 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 66: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8739 - precision: 0.8788 - recall: 0.8788 - val_loss: 0.3650 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2631 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 67: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8729 - precision: 0.8770 - recall: 0.8788 - val_loss: 0.3663 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3733 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 68: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8729 - precision: 0.8770 - recall: 0.8788 - val_loss: 0.3662 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3002 - accuracy: 0.8438 - precision: 0.9444 - recall: 0.8095\n",
            "Epoch 69: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8718 - precision: 0.8753 - recall: 0.8788 - val_loss: 0.3675 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4839 - accuracy: 0.7812 - precision: 0.8889 - recall: 0.7619\n",
            "Epoch 70: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8729 - precision: 0.8785 - recall: 0.8768 - val_loss: 0.3680 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2969 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 71: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8729 - precision: 0.8785 - recall: 0.8768 - val_loss: 0.3655 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2817 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8095\n",
            "Epoch 72: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8718 - precision: 0.8783 - recall: 0.8747 - val_loss: 0.3644 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4025 - accuracy: 0.9062 - precision: 0.8824 - recall: 0.9375\n",
            "Epoch 73: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8697 - precision: 0.8747 - recall: 0.8747 - val_loss: 0.3645 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5813 - accuracy: 0.7812 - precision: 0.8095 - recall: 0.8500\n",
            "Epoch 74: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8708 - precision: 0.8780 - recall: 0.8727 - val_loss: 0.3636 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3679 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 75: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8729 - precision: 0.8785 - recall: 0.8768 - val_loss: 0.3655 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3861 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 76: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3333 - accuracy: 0.8708 - precision: 0.8750 - recall: 0.8768 - val_loss: 0.3652 - val_accuracy: 0.8445 - val_precision: 0.8760 - val_recall: 0.8433\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3112 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 77: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3643 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2072 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9091\n",
            "Epoch 78: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8697 - precision: 0.8732 - recall: 0.8768 - val_loss: 0.3648 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2184 - accuracy: 0.9062 - precision: 0.9091 - recall: 0.8333\n",
            "Epoch 79: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8708 - precision: 0.8750 - recall: 0.8768 - val_loss: 0.3650 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1979 - accuracy: 0.9062 - precision: 0.9375 - recall: 0.8824\n",
            "Epoch 80: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8718 - precision: 0.8768 - recall: 0.8768 - val_loss: 0.3625 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4361 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.7273\n",
            "Epoch 81: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8729 - precision: 0.8755 - recall: 0.8808 - val_loss: 0.3632 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3290 - accuracy: 0.9375 - precision: 0.9444 - recall: 0.9444\n",
            "Epoch 82: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8750 - precision: 0.8790 - recall: 0.8808 - val_loss: 0.3619 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2774 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8571\n",
            "Epoch 83: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8729 - precision: 0.8755 - recall: 0.8808 - val_loss: 0.3608 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4745 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 84: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8729 - precision: 0.8770 - recall: 0.8788 - val_loss: 0.3607 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2352 - accuracy: 0.9375 - precision: 0.9091 - recall: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8739 - precision: 0.8773 - recall: 0.8808 - val_loss: 0.3607 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1856 - accuracy: 0.9375 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 86: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8708 - precision: 0.8735 - recall: 0.8788 - val_loss: 0.3614 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3733 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 87: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8761 - precision: 0.8778 - recall: 0.8848 - val_loss: 0.3621 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3520 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 88: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8718 - precision: 0.8753 - recall: 0.8788 - val_loss: 0.3621 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3925 - accuracy: 0.8750 - precision: 0.9167 - recall: 0.7857\n",
            "Epoch 89: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - precision: 0.8775 - recall: 0.8828 - val_loss: 0.3607 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2545 - accuracy: 0.8750 - precision: 1.0000 - recall: 0.8095\n",
            "Epoch 90: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - precision: 0.8760 - recall: 0.8848 - val_loss: 0.3621 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3135 - accuracy: 0.8125 - precision: 0.9333 - recall: 0.7368\n",
            "Epoch 91: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - precision: 0.8775 - recall: 0.8828 - val_loss: 0.3609 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3278 - accuracy: 0.8750 - precision: 0.8235 - recall: 0.9333\n",
            "Epoch 92: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8761 - precision: 0.8793 - recall: 0.8828 - val_loss: 0.3595 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5589 - accuracy: 0.7812 - precision: 0.8500 - recall: 0.8095\n",
            "Epoch 93: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8761 - precision: 0.8793 - recall: 0.8828 - val_loss: 0.3605 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3334 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 94: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - precision: 0.8760 - recall: 0.8848 - val_loss: 0.3620 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3347 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.7692\n",
            "Epoch 95: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3604 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3266 - accuracy: 0.8438 - precision: 0.7333 - recall: 0.9167\n",
            "Epoch 96: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8782 - precision: 0.8782 - recall: 0.8889 - val_loss: 0.3587 - val_accuracy: 0.8403 - val_precision: 0.8692 - val_recall: 0.8433\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3945 - accuracy: 0.8750 - precision: 0.9444 - recall: 0.8500\n",
            "Epoch 97: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8771 - precision: 0.8765 - recall: 0.8889 - val_loss: 0.3609 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3452 - accuracy: 0.8438 - precision: 0.8182 - recall: 0.7500\n",
            "Epoch 98: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8771 - precision: 0.8780 - recall: 0.8869 - val_loss: 0.3615 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4329 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 99: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.8771 - precision: 0.8795 - recall: 0.8848 - val_loss: 0.3579 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2538 - accuracy: 0.9062 - precision: 0.9500 - recall: 0.9048\n",
            "Epoch 100: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8761 - precision: 0.8762 - recall: 0.8869 - val_loss: 0.3585 - val_accuracy: 0.8487 - val_precision: 0.8828 - val_recall: 0.8433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing chest pain\n",
        "- val_accuracy: 0.8361\n",
        "- val_loss: 0.4152\n",
        "- val_precision: 0.8129\n",
        "- val_recall: 0.8968"
      ],
      "metadata": {
        "id": "UUvEon0mO2cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CTTRAIN = np.delete(RTRAIN, 2, axis=1 )\n",
        "CTVALID = np.delete(RVALID, 2, axis=1 )\n",
        "less_chest_model = Sequential(name=\"less_chest_model\")\n",
        "less_chest_model.add(Input(shape=(6,)))\n",
        "less_chest_model.add(Dense(16, activation='relu'))\n",
        "less_chest_model.add(Dense(8, activation='relu'))\n",
        "less_chest_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_chest_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_chest_history = less_chest_model.fit(CTTRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(CTVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F_lbn5gbOyuA",
        "outputId": "1afcf282-f82f-47ca-fa69-343b2dc0b5d4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.8407 - accuracy: 0.5000 - precision: 0.8141 - recall: 0.8467\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7266 - accuracy: 0.5515 - precision: 0.5894 - recall: 0.8696 - val_loss: 0.6227 - val_accuracy: 0.6723 - val_precision: 0.6609 - val_recall: 0.8582\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6397 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.9375\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5983 - accuracy: 0.7132 - precision: 0.6768 - recall: 0.8586 - val_loss: 0.5493 - val_accuracy: 0.7605 - val_precision: 0.7692 - val_recall: 0.8209\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5486 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8500\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5263 - accuracy: 0.7773 - precision: 0.7685 - recall: 0.8182 - val_loss: 0.5039 - val_accuracy: 0.7857 - val_precision: 0.8168 - val_recall: 0.7985\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4941 - accuracy: 0.7500 - precision: 0.7895 - recall: 0.7895\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4822 - accuracy: 0.7952 - precision: 0.7988 - recall: 0.8101 - val_loss: 0.4761 - val_accuracy: 0.7815 - val_precision: 0.8254 - val_recall: 0.7761\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5192 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.8099 - precision: 0.8244 - recall: 0.8061 - val_loss: 0.4613 - val_accuracy: 0.7815 - val_precision: 0.8306 - val_recall: 0.7687\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4209 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8088 - precision: 0.8227 - recall: 0.8061 - val_loss: 0.4508 - val_accuracy: 0.7731 - val_precision: 0.8175 - val_recall: 0.7687\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4474 - accuracy: 0.7500 - precision: 0.8421 - recall: 0.7619\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8099 - precision: 0.8204 - recall: 0.8121 - val_loss: 0.4444 - val_accuracy: 0.7689 - val_precision: 0.8015 - val_recall: 0.7836\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3448 - accuracy: 0.8750 - precision: 0.8696 - recall: 0.9524\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4288 - accuracy: 0.8120 - precision: 0.8211 - recall: 0.8162 - val_loss: 0.4426 - val_accuracy: 0.7857 - val_precision: 0.8217 - val_recall: 0.7910\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4033 - accuracy: 0.8125 - precision: 0.8182 - recall: 0.9000\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8193 - precision: 0.8249 - recall: 0.8283 - val_loss: 0.4398 - val_accuracy: 0.7941 - val_precision: 0.8244 - val_recall: 0.8060\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3954 - accuracy: 0.8750 - precision: 0.8500 - recall: 0.9444\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8172 - precision: 0.8216 - recall: 0.8283 - val_loss: 0.4370 - val_accuracy: 0.7899 - val_precision: 0.8182 - val_recall: 0.8060\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3240 - accuracy: 0.8750 - precision: 0.9231 - recall: 0.8000\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.8225 - precision: 0.8221 - recall: 0.8404 - val_loss: 0.4376 - val_accuracy: 0.7857 - val_precision: 0.8168 - val_recall: 0.7985\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2692 - accuracy: 0.8750 - precision: 0.7857 - recall: 0.9167\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8204 - precision: 0.8227 - recall: 0.8343 - val_loss: 0.4337 - val_accuracy: 0.7941 - val_precision: 0.8195 - val_recall: 0.8134\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4230 - accuracy: 0.8125 - precision: 0.7000 - recall: 1.0000\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4126 - accuracy: 0.8256 - precision: 0.8232 - recall: 0.8465 - val_loss: 0.4317 - val_accuracy: 0.7983 - val_precision: 0.8209 - val_recall: 0.8209\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4382 - accuracy: 0.8125 - precision: 0.7222 - recall: 0.9286\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8288 - precision: 0.8242 - recall: 0.8525 - val_loss: 0.4302 - val_accuracy: 0.7983 - val_precision: 0.8209 - val_recall: 0.8209\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3223 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8298 - precision: 0.8246 - recall: 0.8545 - val_loss: 0.4320 - val_accuracy: 0.7983 - val_precision: 0.8209 - val_recall: 0.8209\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3878 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4070 - accuracy: 0.8309 - precision: 0.8236 - recall: 0.8586 - val_loss: 0.4314 - val_accuracy: 0.7983 - val_precision: 0.8209 - val_recall: 0.8209\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5585 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8298 - precision: 0.8208 - recall: 0.8606 - val_loss: 0.4310 - val_accuracy: 0.8067 - val_precision: 0.8235 - val_recall: 0.8358\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2859 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8288 - precision: 0.8192 - recall: 0.8606 - val_loss: 0.4319 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4820 - accuracy: 0.8125 - precision: 0.7143 - recall: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4027 - accuracy: 0.8309 - precision: 0.8224 - recall: 0.8606 - val_loss: 0.4292 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3360 - accuracy: 0.8750 - precision: 0.8571 - recall: 0.9474\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8309 - precision: 0.8212 - recall: 0.8626 - val_loss: 0.4290 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3555 - accuracy: 0.8438 - precision: 0.9000 - recall: 0.6923\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4002 - accuracy: 0.8309 - precision: 0.8212 - recall: 0.8626 - val_loss: 0.4308 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3379 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8351 - precision: 0.8275 - recall: 0.8626 - val_loss: 0.4281 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4767 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8330 - precision: 0.8231 - recall: 0.8646 - val_loss: 0.4295 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2796 - accuracy: 0.9062 - precision: 0.9412 - recall: 0.8889\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8351 - precision: 0.8250 - recall: 0.8667 - val_loss: 0.4282 - val_accuracy: 0.8109 - val_precision: 0.8248 - val_recall: 0.8433\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4720 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8382 - precision: 0.8260 - recall: 0.8727 - val_loss: 0.4297 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4595 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.7692\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8382 - precision: 0.8273 - recall: 0.8707 - val_loss: 0.4289 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2931 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8382 - precision: 0.8298 - recall: 0.8667 - val_loss: 0.4262 - val_accuracy: 0.8151 - val_precision: 0.8309 - val_recall: 0.8433\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5647 - accuracy: 0.7188 - precision: 0.6316 - recall: 0.8571\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8393 - precision: 0.8263 - recall: 0.8747 - val_loss: 0.4281 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3498 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8414 - precision: 0.8308 - recall: 0.8727 - val_loss: 0.4260 - val_accuracy: 0.8151 - val_precision: 0.8309 - val_recall: 0.8433\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5885 - accuracy: 0.6875 - precision: 0.7000 - recall: 0.7778\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8435 - precision: 0.8302 - recall: 0.8788 - val_loss: 0.4289 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4987 - accuracy: 0.7812 - precision: 0.7895 - recall: 0.8333\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8424 - precision: 0.8311 - recall: 0.8747 - val_loss: 0.4279 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4391 - accuracy: 0.8125 - precision: 0.8462 - recall: 0.7333\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8445 - precision: 0.8305 - recall: 0.8808 - val_loss: 0.4309 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4292 - accuracy: 0.8438 - precision: 0.8636 - recall: 0.9048\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8424 - precision: 0.8311 - recall: 0.8747 - val_loss: 0.4259 - val_accuracy: 0.8193 - val_precision: 0.8370 - val_recall: 0.8433\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3560 - accuracy: 0.8750 - precision: 0.8947 - recall: 0.8947\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3874 - accuracy: 0.8424 - precision: 0.8286 - recall: 0.8788 - val_loss: 0.4291 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4181 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8424 - precision: 0.8311 - recall: 0.8747 - val_loss: 0.4260 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4418 - accuracy: 0.8438 - precision: 0.7778 - recall: 0.9333\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.4264 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2938 - accuracy: 0.8438 - precision: 0.7619 - recall: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.4255 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4104 - accuracy: 0.9062 - precision: 0.7857 - recall: 1.0000\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.4266 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4417 - accuracy: 0.8125 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3832 - accuracy: 0.8435 - precision: 0.8314 - recall: 0.8768 - val_loss: 0.4258 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3386 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.8435 - precision: 0.8302 - recall: 0.8788 - val_loss: 0.4286 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3568 - accuracy: 0.9062 - precision: 0.8571 - recall: 0.9231\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8445 - precision: 0.8317 - recall: 0.8788 - val_loss: 0.4246 - val_accuracy: 0.8151 - val_precision: 0.8358 - val_recall: 0.8358\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3717 - accuracy: 0.8438 - precision: 0.8095 - recall: 0.9444\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8414 - precision: 0.8295 - recall: 0.8747 - val_loss: 0.4221 - val_accuracy: 0.8193 - val_precision: 0.8370 - val_recall: 0.8433\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3854 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8445 - precision: 0.8305 - recall: 0.8808 - val_loss: 0.4266 - val_accuracy: 0.8109 - val_precision: 0.8397 - val_recall: 0.8209\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4986 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.7143\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8445 - precision: 0.8343 - recall: 0.8747 - val_loss: 0.4239 - val_accuracy: 0.8151 - val_precision: 0.8409 - val_recall: 0.8284\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3789 - accuracy: 0.7812 - precision: 0.6429 - recall: 0.8182\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8445 - precision: 0.8330 - recall: 0.8768 - val_loss: 0.4252 - val_accuracy: 0.8151 - val_precision: 0.8409 - val_recall: 0.8284\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4621 - accuracy: 0.7812 - precision: 0.6923 - recall: 0.7500\n",
            "Epoch 46: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8477 - precision: 0.8391 - recall: 0.8747 - val_loss: 0.4248 - val_accuracy: 0.8151 - val_precision: 0.8409 - val_recall: 0.8284\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3008 - accuracy: 0.9062 - precision: 0.9000 - recall: 0.9474\n",
            "Epoch 47: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8487 - precision: 0.8395 - recall: 0.8768 - val_loss: 0.4251 - val_accuracy: 0.8151 - val_precision: 0.8409 - val_recall: 0.8284\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3794 - accuracy: 0.8750 - precision: 0.9167 - recall: 0.7857\n",
            "Epoch 48: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8466 - precision: 0.8375 - recall: 0.8747 - val_loss: 0.4230 - val_accuracy: 0.8193 - val_precision: 0.8473 - val_recall: 0.8284\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2698 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 49: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8519 - precision: 0.8430 - recall: 0.8788 - val_loss: 0.4244 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2978 - accuracy: 0.8750 - precision: 0.8824 - recall: 0.8824\n",
            "Epoch 50: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8519 - precision: 0.8444 - recall: 0.8768 - val_loss: 0.4244 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 51/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3239 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.9375\n",
            "Epoch 51: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8508 - precision: 0.8454 - recall: 0.8727 - val_loss: 0.4214 - val_accuracy: 0.8109 - val_precision: 0.8397 - val_recall: 0.8209\n",
            "Epoch 52/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4135 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 52: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.8540 - precision: 0.8477 - recall: 0.8768 - val_loss: 0.4227 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 53/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2309 - accuracy: 0.9375 - precision: 0.8667 - recall: 1.0000\n",
            "Epoch 53: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8519 - precision: 0.8444 - recall: 0.8768 - val_loss: 0.4207 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 54/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3073 - accuracy: 0.9062 - precision: 0.8462 - recall: 0.9167\n",
            "Epoch 54: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8540 - precision: 0.8463 - recall: 0.8788 - val_loss: 0.4223 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 55/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3873 - accuracy: 0.7500 - precision: 0.8333 - recall: 0.7500\n",
            "Epoch 55: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8519 - precision: 0.8471 - recall: 0.8727 - val_loss: 0.4225 - val_accuracy: 0.8151 - val_precision: 0.8462 - val_recall: 0.8209\n",
            "Epoch 56/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5382 - accuracy: 0.8125 - precision: 0.9048 - recall: 0.8261\n",
            "Epoch 56: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8498 - precision: 0.8451 - recall: 0.8707 - val_loss: 0.4226 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 57/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3479 - accuracy: 0.8750 - precision: 0.9048 - recall: 0.9048\n",
            "Epoch 57: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8540 - precision: 0.8504 - recall: 0.8727 - val_loss: 0.4226 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 58/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4346 - accuracy: 0.8125 - precision: 0.8125 - recall: 0.8125\n",
            "Epoch 58: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8519 - precision: 0.8512 - recall: 0.8667 - val_loss: 0.4220 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 59/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3970 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 59: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8529 - precision: 0.8501 - recall: 0.8707 - val_loss: 0.4219 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 60/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3703 - accuracy: 0.8750 - precision: 0.8421 - recall: 0.9412\n",
            "Epoch 60: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8550 - precision: 0.8521 - recall: 0.8727 - val_loss: 0.4211 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 61/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2704 - accuracy: 0.9375 - precision: 0.9500 - recall: 0.9500\n",
            "Epoch 61: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8540 - precision: 0.8504 - recall: 0.8727 - val_loss: 0.4218 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 62/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3867 - accuracy: 0.8438 - precision: 0.8824 - recall: 0.8333\n",
            "Epoch 62: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8550 - precision: 0.8549 - recall: 0.8687 - val_loss: 0.4189 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 63/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3129 - accuracy: 0.8750 - precision: 0.8462 - recall: 0.8462\n",
            "Epoch 63: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.8540 - precision: 0.8546 - recall: 0.8667 - val_loss: 0.4192 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 64/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4337 - accuracy: 0.8125 - precision: 0.8182 - recall: 0.9000\n",
            "Epoch 64: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.8540 - precision: 0.8532 - recall: 0.8687 - val_loss: 0.4185 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 65/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3675 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 65: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8540 - precision: 0.8532 - recall: 0.8687 - val_loss: 0.4191 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 66/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5033 - accuracy: 0.7812 - precision: 0.7059 - recall: 0.8571\n",
            "Epoch 66: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8519 - precision: 0.8512 - recall: 0.8667 - val_loss: 0.4187 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 67/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3973 - accuracy: 0.8125 - precision: 0.9231 - recall: 0.7059\n",
            "Epoch 67: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8571 - precision: 0.8569 - recall: 0.8707 - val_loss: 0.4169 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 68/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3369 - accuracy: 0.9062 - precision: 1.0000 - recall: 0.8636\n",
            "Epoch 68: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.4172 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 69/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3772 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8889\n",
            "Epoch 69: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8550 - precision: 0.8563 - recall: 0.8667 - val_loss: 0.4182 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 70/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2449 - accuracy: 0.9375 - precision: 0.9412 - recall: 0.9412\n",
            "Epoch 70: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8582 - precision: 0.8586 - recall: 0.8707 - val_loss: 0.4167 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 71/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3715 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.8824\n",
            "Epoch 71: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8561 - precision: 0.8566 - recall: 0.8687 - val_loss: 0.4161 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 72/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4956 - accuracy: 0.7500 - precision: 0.8667 - recall: 0.6842\n",
            "Epoch 72: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8561 - precision: 0.8594 - recall: 0.8646 - val_loss: 0.4155 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 73/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3067 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 73: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3617 - accuracy: 0.8582 - precision: 0.8586 - recall: 0.8707 - val_loss: 0.4192 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 74/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4021 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 74: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8592 - precision: 0.8617 - recall: 0.8687 - val_loss: 0.4191 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 75/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4126 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 75: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3607 - accuracy: 0.8603 - precision: 0.8635 - recall: 0.8687 - val_loss: 0.4162 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 76/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2744 - accuracy: 0.9062 - precision: 0.8750 - recall: 0.9333\n",
            "Epoch 76: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8582 - precision: 0.8586 - recall: 0.8707 - val_loss: 0.4182 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 77/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3695 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 77: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.8561 - precision: 0.8552 - recall: 0.8707 - val_loss: 0.4182 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 78/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1695 - accuracy: 0.9688 - precision: 0.9565 - recall: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4159 - val_accuracy: 0.8193 - val_precision: 0.8527 - val_recall: 0.8209\n",
            "Epoch 79/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3150 - accuracy: 0.8750 - precision: 0.8182 - recall: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.4170 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 80/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4312 - accuracy: 0.8125 - precision: 0.8000 - recall: 0.8000\n",
            "Epoch 80: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.4164 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 81/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3906 - accuracy: 0.8438 - precision: 0.8125 - recall: 0.8667\n",
            "Epoch 81: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8571 - precision: 0.8583 - recall: 0.8687 - val_loss: 0.4141 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 82/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3814 - accuracy: 0.8438 - precision: 0.8235 - recall: 0.8750\n",
            "Epoch 82: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8603 - precision: 0.8620 - recall: 0.8707 - val_loss: 0.4170 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 83/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2927 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9091\n",
            "Epoch 83: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4144 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 84/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4414 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 84: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8592 - precision: 0.8617 - recall: 0.8687 - val_loss: 0.4153 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 85/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4379 - accuracy: 0.8438 - precision: 0.7727 - recall: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4159 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 86/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2089 - accuracy: 0.9375 - precision: 0.9524 - recall: 0.9524\n",
            "Epoch 86: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3554 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4143 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 87/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4107 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 87: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3547 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4159 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 88/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3768 - accuracy: 0.8438 - precision: 0.8333 - recall: 0.7692\n",
            "Epoch 88: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4129 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 89/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3550 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 89: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4121 - val_accuracy: 0.8235 - val_precision: 0.8594 - val_recall: 0.8209\n",
            "Epoch 90/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2337 - accuracy: 0.9062 - precision: 0.9286 - recall: 0.8667\n",
            "Epoch 90: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8592 - precision: 0.8617 - recall: 0.8687 - val_loss: 0.4115 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 91/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5560 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 91: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4158 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 92/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2380 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 92: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4121 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 93/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2571 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571\n",
            "Epoch 93: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8582 - precision: 0.8614 - recall: 0.8667 - val_loss: 0.4106 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 94/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4086 - accuracy: 0.8125 - precision: 0.7895 - recall: 0.8824\n",
            "Epoch 94: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4145 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 95/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5027 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 95: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4146 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 96/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4695 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 96: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3506 - accuracy: 0.8592 - precision: 0.8632 - recall: 0.8667 - val_loss: 0.4130 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 97/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2163 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 97: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4105 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n",
            "Epoch 98/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4200 - accuracy: 0.8125 - precision: 1.0000 - recall: 0.6842\n",
            "Epoch 98: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8561 - precision: 0.8594 - recall: 0.8646 - val_loss: 0.4136 - val_accuracy: 0.8193 - val_precision: 0.8583 - val_recall: 0.8134\n",
            "Epoch 99/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.1780 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9375\n",
            "Epoch 99: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8582 - precision: 0.8600 - recall: 0.8687 - val_loss: 0.4098 - val_accuracy: 0.8109 - val_precision: 0.8450 - val_recall: 0.8134\n",
            "Epoch 100/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3509 - accuracy: 0.8438 - precision: 0.7692 - recall: 0.8333\n",
            "Epoch 100: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3489 - accuracy: 0.8571 - precision: 0.8597 - recall: 0.8667 - val_loss: 0.4107 - val_accuracy: 0.8151 - val_precision: 0.8516 - val_recall: 0.8134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove slope\n",
        "- val_accuracy: 0.8025\n",
        "- val_loss: 0.4664\n",
        "- val_precision: 0.8015\n",
        "- val_recall: 0.8333"
      ],
      "metadata": {
        "id": "mhytqoOJP1i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STRAIN = np.delete(CTTRAIN, 5, axis=1 )\n",
        "SVALID = np.delete(CTVALID, 5, axis=1 )\n",
        "less_slope_model = Sequential(name=\"less_slope_model\")\n",
        "less_slope_model.add(Input(shape=(5,)))\n",
        "less_slope_model.add(Dense(16, activation='relu'))\n",
        "less_slope_model.add(Dense(8, activation='relu'))\n",
        "less_slope_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_slope_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_slope_history = less_slope_model.fit(STRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(SVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dLem4LhDPSBh",
        "outputId": "9a25e3b9-5efa-4e30-837d-ab802cbbe0a2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.8000 - accuracy: 0.2812 - precision: 0.8000 - recall: 0.7417\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.7390 - accuracy: 0.3771 - precision: 0.4924 - recall: 0.4102 - val_loss: 0.6907 - val_accuracy: 0.4790 - val_precision: 0.5424 - val_recall: 0.4776\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6908 - accuracy: 0.4688 - precision: 0.5556 - recall: 0.5263\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5326 - precision: 0.5465 - recall: 0.5939 - val_loss: 0.6363 - val_accuracy: 0.6345 - val_precision: 0.6460 - val_recall: 0.7761\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6685 - accuracy: 0.5938 - precision: 0.6190 - recall: 0.7222\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.6261 - precision: 0.6045 - recall: 0.8121 - val_loss: 0.5956 - val_accuracy: 0.6639 - val_precision: 0.6627 - val_recall: 0.8209\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6417 - accuracy: 0.5938 - precision: 0.5500 - recall: 0.7333\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.6670 - precision: 0.6391 - recall: 0.8263 - val_loss: 0.5629 - val_accuracy: 0.7185 - val_precision: 0.7219 - val_recall: 0.8134\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5510 - accuracy: 0.7188 - precision: 0.7143 - recall: 0.8333\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.7195 - precision: 0.6966 - recall: 0.8162 - val_loss: 0.5354 - val_accuracy: 0.7731 - val_precision: 0.7985 - val_recall: 0.7985\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4580 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7637 - precision: 0.7491 - recall: 0.8202 - val_loss: 0.5160 - val_accuracy: 0.7899 - val_precision: 0.8281 - val_recall: 0.7910\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5177 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7826 - precision: 0.7824 - recall: 0.8061 - val_loss: 0.4998 - val_accuracy: 0.8025 - val_precision: 0.8480 - val_recall: 0.7910\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4778 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.7500\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5195 - accuracy: 0.7815 - precision: 0.7887 - recall: 0.7919 - val_loss: 0.4875 - val_accuracy: 0.7983 - val_precision: 0.8525 - val_recall: 0.7761\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4517 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.9375\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7878 - precision: 0.7972 - recall: 0.7939 - val_loss: 0.4779 - val_accuracy: 0.8025 - val_precision: 0.8595 - val_recall: 0.7761\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5762 - accuracy: 0.7188 - precision: 0.7500 - recall: 0.7895\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.7920 - precision: 0.8087 - recall: 0.7859 - val_loss: 0.4703 - val_accuracy: 0.8025 - val_precision: 0.8595 - val_recall: 0.7761\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4858 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.7962 - precision: 0.8155 - recall: 0.7859 - val_loss: 0.4639 - val_accuracy: 0.7983 - val_precision: 0.8583 - val_recall: 0.7687\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4779 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.5833\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7941 - precision: 0.8161 - recall: 0.7798 - val_loss: 0.4600 - val_accuracy: 0.7899 - val_precision: 0.8559 - val_recall: 0.7537\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4200 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7920 - precision: 0.8180 - recall: 0.7717 - val_loss: 0.4559 - val_accuracy: 0.7983 - val_precision: 0.8583 - val_recall: 0.7687\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5122 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7910 - precision: 0.8136 - recall: 0.7758 - val_loss: 0.4535 - val_accuracy: 0.7815 - val_precision: 0.8534 - val_recall: 0.7388\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3418 - accuracy: 0.9062 - precision: 0.9333 - recall: 0.8750\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4741 - accuracy: 0.7920 - precision: 0.8194 - recall: 0.7697 - val_loss: 0.4517 - val_accuracy: 0.7815 - val_precision: 0.8534 - val_recall: 0.7388\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3868 - accuracy: 0.8438 - precision: 0.8750 - recall: 0.8235\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4709 - accuracy: 0.7899 - precision: 0.8132 - recall: 0.7737 - val_loss: 0.4520 - val_accuracy: 0.7815 - val_precision: 0.8596 - val_recall: 0.7313\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6053 - accuracy: 0.7188 - precision: 0.6923 - recall: 0.6429\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4684 - accuracy: 0.7910 - precision: 0.8162 - recall: 0.7717 - val_loss: 0.4506 - val_accuracy: 0.7815 - val_precision: 0.8596 - val_recall: 0.7313\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3468 - accuracy: 0.9062 - precision: 0.8889 - recall: 0.9412\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.7952 - precision: 0.8247 - recall: 0.7697 - val_loss: 0.4497 - val_accuracy: 0.7815 - val_precision: 0.8596 - val_recall: 0.7313\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4591 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.7931 - precision: 0.8268 - recall: 0.7616 - val_loss: 0.4477 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5906 - accuracy: 0.7188 - precision: 0.7059 - recall: 0.7500\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.7878 - precision: 0.8151 - recall: 0.7657 - val_loss: 0.4475 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4998 - accuracy: 0.7500 - precision: 0.8421 - recall: 0.7619\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7899 - precision: 0.8158 - recall: 0.7697 - val_loss: 0.4487 - val_accuracy: 0.7815 - val_precision: 0.8596 - val_recall: 0.7313\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2781 - accuracy: 0.9375 - precision: 0.9375 - recall: 0.9375\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7941 - precision: 0.8243 - recall: 0.7677 - val_loss: 0.4478 - val_accuracy: 0.7857 - val_precision: 0.8609 - val_recall: 0.7388\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6054 - accuracy: 0.7500 - precision: 0.6875 - recall: 0.7857\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4608 - accuracy: 0.7899 - precision: 0.8186 - recall: 0.7657 - val_loss: 0.4486 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3601 - accuracy: 0.8750 - precision: 0.8750 - recall: 0.8750\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7962 - precision: 0.8265 - recall: 0.7697 - val_loss: 0.4485 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4891 - accuracy: 0.8125 - precision: 0.6667 - recall: 0.8000\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.7994 - precision: 0.8348 - recall: 0.7657 - val_loss: 0.4477 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5697 - accuracy: 0.6875 - precision: 0.7778 - recall: 0.7000\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7962 - precision: 0.8251 - recall: 0.7717 - val_loss: 0.4486 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4360 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.8015 - precision: 0.8326 - recall: 0.7737 - val_loss: 0.4488 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5048 - accuracy: 0.8125 - precision: 0.8182 - recall: 0.9000\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7973 - precision: 0.8240 - recall: 0.7758 - val_loss: 0.4505 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4315 - accuracy: 0.7500 - precision: 0.7000 - recall: 0.8750\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7994 - precision: 0.8290 - recall: 0.7737 - val_loss: 0.4512 - val_accuracy: 0.7941 - val_precision: 0.8696 - val_recall: 0.7463\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4380 - accuracy: 0.7812 - precision: 0.9474 - recall: 0.7500\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8004 - precision: 0.8252 - recall: 0.7818 - val_loss: 0.4530 - val_accuracy: 0.7899 - val_precision: 0.8818 - val_recall: 0.7239\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5679 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8046 - precision: 0.8366 - recall: 0.7758 - val_loss: 0.4520 - val_accuracy: 0.7941 - val_precision: 0.8696 - val_recall: 0.7463\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4792 - accuracy: 0.8125 - precision: 0.9333 - recall: 0.7368\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8036 - precision: 0.8348 - recall: 0.7758 - val_loss: 0.4517 - val_accuracy: 0.7857 - val_precision: 0.8547 - val_recall: 0.7463\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4687 - accuracy: 0.7500 - precision: 0.7647 - recall: 0.7647\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.7973 - precision: 0.8240 - recall: 0.7758 - val_loss: 0.4527 - val_accuracy: 0.7899 - val_precision: 0.8621 - val_recall: 0.7463\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3963 - accuracy: 0.8125 - precision: 0.7778 - recall: 0.8750\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8004 - precision: 0.8252 - recall: 0.7818 - val_loss: 0.4551 - val_accuracy: 0.7941 - val_precision: 0.8829 - val_recall: 0.7313\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3565 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8025 - precision: 0.8315 - recall: 0.7778 - val_loss: 0.4562 - val_accuracy: 0.7941 - val_precision: 0.8829 - val_recall: 0.7313\n",
            "Epoch 35: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Old peak\n",
        "- val_accuracy: 0.7311\n",
        "- val_loss: 0.5201\n",
        "- val_precision: 0.7627\n",
        "- val_recall: 0.7143"
      ],
      "metadata": {
        "id": "4dApQSJeQ-vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OTRAIN = np.delete(STRAIN, 4, axis=1 )\n",
        "OVALID = np.delete(SVALID, 4, axis=1 )\n",
        "less_old_model = Sequential(name=\"less_old_model\")\n",
        "less_old_model.add(Input(shape=(4,)))\n",
        "less_old_model.add(Dense(16, activation='relu'))\n",
        "less_old_model.add(Dense(8, activation='relu'))\n",
        "less_old_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_old_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_old_history = less_old_model.fit(OTRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(OVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xdwu07MDQ4pK",
        "outputId": "a37abd46-ccc2-450c-e88f-d2b56c7098c2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.7032 - accuracy: 0.5312 - precision: 0.8772 - recall: 0.6667\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6399 - accuracy: 0.6218 - precision: 0.8416 - recall: 0.4308 - val_loss: 0.6465 - val_accuracy: 0.6092 - val_precision: 0.7470 - val_recall: 0.4627\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6040 - accuracy: 0.7188 - precision: 0.7333 - recall: 0.6875\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.7027 - precision: 0.7718 - recall: 0.6081 - val_loss: 0.6099 - val_accuracy: 0.6975 - val_precision: 0.7818 - val_recall: 0.6418\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5421 - accuracy: 0.7188 - precision: 0.7778 - recall: 0.7368\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7353 - precision: 0.7755 - recall: 0.6909 - val_loss: 0.5810 - val_accuracy: 0.7395 - val_precision: 0.8051 - val_recall: 0.7090\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5709 - accuracy: 0.7188 - precision: 0.8571 - recall: 0.6316\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7416 - precision: 0.7677 - recall: 0.7212 - val_loss: 0.5597 - val_accuracy: 0.7563 - val_precision: 0.8115 - val_recall: 0.7388\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5998 - accuracy: 0.6250 - precision: 0.7692 - recall: 0.5263\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5274 - accuracy: 0.7521 - precision: 0.7738 - recall: 0.7394 - val_loss: 0.5433 - val_accuracy: 0.7563 - val_precision: 0.8065 - val_recall: 0.7463\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5534 - accuracy: 0.7500 - precision: 0.7222 - recall: 0.8125\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5160 - accuracy: 0.7500 - precision: 0.7683 - recall: 0.7434 - val_loss: 0.5311 - val_accuracy: 0.7605 - val_precision: 0.8182 - val_recall: 0.7388\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3979 - accuracy: 0.8125 - precision: 0.8571 - recall: 0.8571\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5086 - accuracy: 0.7542 - precision: 0.7724 - recall: 0.7475 - val_loss: 0.5225 - val_accuracy: 0.7563 - val_precision: 0.8167 - val_recall: 0.7313\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6307 - accuracy: 0.6875 - precision: 0.7143 - recall: 0.6250\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7605 - precision: 0.7787 - recall: 0.7535 - val_loss: 0.5157 - val_accuracy: 0.7479 - val_precision: 0.8136 - val_recall: 0.7164\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4287 - accuracy: 0.8750 - precision: 0.7857 - recall: 0.9167\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4994 - accuracy: 0.7595 - precision: 0.7759 - recall: 0.7556 - val_loss: 0.5105 - val_accuracy: 0.7479 - val_precision: 0.8190 - val_recall: 0.7090\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6746 - accuracy: 0.5938 - precision: 0.6667 - recall: 0.6316\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4966 - accuracy: 0.7668 - precision: 0.7874 - recall: 0.7556 - val_loss: 0.5065 - val_accuracy: 0.7521 - val_precision: 0.8261 - val_recall: 0.7090\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4161 - accuracy: 0.7812 - precision: 0.7333 - recall: 0.7857\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.7689 - precision: 0.7983 - recall: 0.7434 - val_loss: 0.5042 - val_accuracy: 0.7563 - val_precision: 0.8333 - val_recall: 0.7090\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4295 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.7143\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7689 - precision: 0.7983 - recall: 0.7434 - val_loss: 0.5020 - val_accuracy: 0.7605 - val_precision: 0.8182 - val_recall: 0.7388\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3377 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.8667\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7679 - precision: 0.7940 - recall: 0.7475 - val_loss: 0.5011 - val_accuracy: 0.7647 - val_precision: 0.8250 - val_recall: 0.7388\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4236 - accuracy: 0.8438 - precision: 0.7692 - recall: 0.8333\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.7647 - precision: 0.7914 - recall: 0.7434 - val_loss: 0.5001 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5190 - accuracy: 0.7500 - precision: 0.8125 - recall: 0.7222\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.7637 - precision: 0.7860 - recall: 0.7495 - val_loss: 0.4995 - val_accuracy: 0.7521 - val_precision: 0.8319 - val_recall: 0.7015\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5391 - accuracy: 0.7812 - precision: 0.7059 - recall: 0.8571\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4910 - accuracy: 0.7637 - precision: 0.7909 - recall: 0.7414 - val_loss: 0.4989 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5209 - accuracy: 0.7188 - precision: 0.8000 - recall: 0.6667\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.7668 - precision: 0.7974 - recall: 0.7394 - val_loss: 0.4980 - val_accuracy: 0.7731 - val_precision: 0.8390 - val_recall: 0.7388\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4624 - accuracy: 0.7812 - precision: 0.8125 - recall: 0.7647\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.7637 - precision: 0.7897 - recall: 0.7434 - val_loss: 0.4979 - val_accuracy: 0.7731 - val_precision: 0.8390 - val_recall: 0.7388\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5781 - accuracy: 0.6250 - precision: 0.6667 - recall: 0.5882\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7637 - precision: 0.7885 - recall: 0.7455 - val_loss: 0.4982 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.7668 - accuracy: 0.5312 - precision: 0.4762 - recall: 0.7143\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7637 - precision: 0.7897 - recall: 0.7434 - val_loss: 0.4972 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4803 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.8571\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7668 - precision: 0.7961 - recall: 0.7414 - val_loss: 0.4966 - val_accuracy: 0.7731 - val_precision: 0.8390 - val_recall: 0.7388\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4536 - accuracy: 0.6875 - precision: 0.7692 - recall: 0.5882\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.7626 - precision: 0.7880 - recall: 0.7434 - val_loss: 0.4967 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4782 - accuracy: 0.7812 - precision: 0.8000 - recall: 0.7500\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.7679 - precision: 0.7953 - recall: 0.7455 - val_loss: 0.4966 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5095 - accuracy: 0.7812 - precision: 0.8235 - recall: 0.7778\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.7689 - precision: 0.7944 - recall: 0.7495 - val_loss: 0.4972 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4195 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.7658 - precision: 0.7931 - recall: 0.7434 - val_loss: 0.4965 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5840 - accuracy: 0.6875 - precision: 0.7143 - recall: 0.6250\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7658 - precision: 0.7918 - recall: 0.7455 - val_loss: 0.4974 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3637 - accuracy: 0.8750 - precision: 0.8667 - recall: 0.8667\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7637 - precision: 0.7897 - recall: 0.7434 - val_loss: 0.4973 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4809 - accuracy: 0.7812 - precision: 0.7778 - recall: 0.8235\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7647 - precision: 0.7927 - recall: 0.7414 - val_loss: 0.4966 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5902 - accuracy: 0.6875 - precision: 0.6471 - recall: 0.7333\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.7668 - precision: 0.7910 - recall: 0.7495 - val_loss: 0.4965 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5052 - accuracy: 0.7500 - precision: 0.6667 - recall: 0.8571\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4859 - accuracy: 0.7647 - precision: 0.7965 - recall: 0.7354 - val_loss: 0.4953 - val_accuracy: 0.7731 - val_precision: 0.8390 - val_recall: 0.7388\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4804 - accuracy: 0.8438 - precision: 0.9412 - recall: 0.8000\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.7647 - precision: 0.7889 - recall: 0.7475 - val_loss: 0.4965 - val_accuracy: 0.7689 - val_precision: 0.8376 - val_recall: 0.7313\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4619 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.7658 - precision: 0.7906 - recall: 0.7475 - val_loss: 0.4971 - val_accuracy: 0.7605 - val_precision: 0.8348 - val_recall: 0.7164\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5614 - accuracy: 0.6562 - precision: 0.6000 - recall: 0.6429\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4858 - accuracy: 0.7700 - precision: 0.7987 - recall: 0.7455 - val_loss: 0.4973 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5164 - accuracy: 0.7500 - precision: 0.7391 - recall: 0.8947\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4857 - accuracy: 0.7658 - precision: 0.7918 - recall: 0.7455 - val_loss: 0.4977 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5647 - accuracy: 0.6875 - precision: 0.7895 - recall: 0.7143\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.7647 - precision: 0.7927 - recall: 0.7414 - val_loss: 0.4970 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5675 - accuracy: 0.7812 - precision: 0.7368 - recall: 0.8750\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4851 - accuracy: 0.7637 - precision: 0.7897 - recall: 0.7434 - val_loss: 0.4979 - val_accuracy: 0.7605 - val_precision: 0.8348 - val_recall: 0.7164\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.2953 - accuracy: 0.8750 - precision: 0.8333 - recall: 0.9375\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.7679 - precision: 0.7965 - recall: 0.7434 - val_loss: 0.4976 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4254 - accuracy: 0.8438 - precision: 0.8947 - recall: 0.8500\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7626 - precision: 0.7918 - recall: 0.7374 - val_loss: 0.4969 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3655 - accuracy: 0.8125 - precision: 0.8235 - recall: 0.8235\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7679 - precision: 0.7890 - recall: 0.7556 - val_loss: 0.4979 - val_accuracy: 0.7605 - val_precision: 0.8348 - val_recall: 0.7164\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4969 - accuracy: 0.6875 - precision: 0.6471 - recall: 0.7333\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7689 - precision: 0.7957 - recall: 0.7475 - val_loss: 0.4976 - val_accuracy: 0.7647 - val_precision: 0.8421 - val_recall: 0.7164\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4517 - accuracy: 0.7500 - precision: 0.7273 - recall: 0.6154\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7626 - precision: 0.7918 - recall: 0.7374 - val_loss: 0.4973 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4014 - accuracy: 0.8438 - precision: 0.7857 - recall: 0.8462\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7647 - precision: 0.7927 - recall: 0.7414 - val_loss: 0.4970 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4202 - accuracy: 0.8438 - precision: 0.7647 - recall: 0.9286\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.7647 - precision: 0.7939 - recall: 0.7394 - val_loss: 0.4968 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4240 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.7778\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7658 - precision: 0.7918 - recall: 0.7455 - val_loss: 0.4974 - val_accuracy: 0.7647 - val_precision: 0.8362 - val_recall: 0.7239\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4585 - accuracy: 0.8125 - precision: 0.7857 - recall: 0.7857\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.7668 - precision: 0.7935 - recall: 0.7455 - val_loss: 0.4975 - val_accuracy: 0.7605 - val_precision: 0.8348 - val_recall: 0.7164\n",
            "Epoch 45: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Exercise Angina\n",
        "- val_accuracy: 0.7185\n",
        "- val_loss: 0.5657\n",
        "- val_precision: 0.7398\n",
        "- val_recall: 0.7222"
      ],
      "metadata": {
        "id": "zkuYu6NkSELM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ATRAIN = np.delete(OTRAIN, 3, axis=1 )\n",
        "AVALID = np.delete(OVALID, 3, axis=1 )\n",
        "less_ang_model = Sequential(name=\"less_ang_model\")\n",
        "less_ang_model.add(Input(shape=(3,)))\n",
        "less_ang_model.add(Dense(16, activation='relu'))\n",
        "less_ang_model.add(Dense(8, activation='relu'))\n",
        "less_ang_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "less_ang_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', prec, rec])\n",
        "\n",
        "# fit the model\n",
        "less_ang_history = less_ang_model.fit(ATRAIN, YTRAIN, epochs=100, verbose=1, validation_data=(AVALID, YVALID), callbacks=[model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v3LUgVM7RwBK",
        "outputId": "a8d10095-caab-4e0a-bcd4-5a65402e8783"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/30 [>.............................] - ETA: 16s - loss: 0.6697 - accuracy: 0.6875 - precision: 0.8403 - recall: 0.6757\n",
            "Epoch 1: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 1s 10ms/step - loss: 0.6436 - accuracy: 0.6838 - precision: 0.8173 - recall: 0.5548 - val_loss: 0.6414 - val_accuracy: 0.6345 - val_precision: 0.7423 - val_recall: 0.5373\n",
            "Epoch 2/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6744 - accuracy: 0.5312 - precision: 0.6000 - recall: 0.3529\n",
            "Epoch 2: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 0.7048 - precision: 0.7688 - recall: 0.6182 - val_loss: 0.6201 - val_accuracy: 0.6555 - val_precision: 0.7364 - val_recall: 0.6045\n",
            "Epoch 3/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6046 - accuracy: 0.7188 - precision: 0.9000 - recall: 0.5294\n",
            "Epoch 3: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5865 - accuracy: 0.7111 - precision: 0.7455 - recall: 0.6747 - val_loss: 0.6035 - val_accuracy: 0.6597 - val_precision: 0.7304 - val_recall: 0.6269\n",
            "Epoch 4/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5419 - accuracy: 0.8125 - precision: 0.6000 - recall: 1.0000\n",
            "Epoch 4: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.7153 - precision: 0.7393 - recall: 0.6990 - val_loss: 0.5940 - val_accuracy: 0.6723 - val_precision: 0.7333 - val_recall: 0.6567\n",
            "Epoch 5/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4392 - accuracy: 0.8438 - precision: 0.8421 - recall: 0.8889\n",
            "Epoch 5: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7122 - precision: 0.7260 - recall: 0.7172 - val_loss: 0.5905 - val_accuracy: 0.6723 - val_precision: 0.7222 - val_recall: 0.6791\n",
            "Epoch 6/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5951 - accuracy: 0.6562 - precision: 0.7059 - recall: 0.6667\n",
            "Epoch 6: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5507 - accuracy: 0.7101 - precision: 0.7203 - recall: 0.7232 - val_loss: 0.5890 - val_accuracy: 0.6639 - val_precision: 0.7109 - val_recall: 0.6791\n",
            "Epoch 7/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4976 - accuracy: 0.7812 - precision: 0.8750 - recall: 0.7368\n",
            "Epoch 7: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.7143 - precision: 0.7217 - recall: 0.7333 - val_loss: 0.5891 - val_accuracy: 0.6639 - val_precision: 0.7109 - val_recall: 0.6791\n",
            "Epoch 8/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4651 - accuracy: 0.7188 - precision: 0.6500 - recall: 0.8667\n",
            "Epoch 8: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5466 - accuracy: 0.7153 - precision: 0.7240 - recall: 0.7313 - val_loss: 0.5889 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 9/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5514 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.7000\n",
            "Epoch 9: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5458 - accuracy: 0.7174 - precision: 0.7216 - recall: 0.7434 - val_loss: 0.5889 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 10/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4822 - accuracy: 0.7500 - precision: 0.7368 - recall: 0.8235\n",
            "Epoch 10: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5449 - accuracy: 0.7206 - precision: 0.7267 - recall: 0.7414 - val_loss: 0.5880 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 11/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6034 - accuracy: 0.6875 - precision: 0.8462 - recall: 0.5789\n",
            "Epoch 11: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5439 - accuracy: 0.7195 - precision: 0.7244 - recall: 0.7434 - val_loss: 0.5880 - val_accuracy: 0.6681 - val_precision: 0.7132 - val_recall: 0.6866\n",
            "Epoch 12/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5444 - accuracy: 0.7500 - precision: 0.7895 - recall: 0.7895\n",
            "Epoch 12: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7185 - precision: 0.7239 - recall: 0.7414 - val_loss: 0.5877 - val_accuracy: 0.6723 - val_precision: 0.7222 - val_recall: 0.6791\n",
            "Epoch 13/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5589 - accuracy: 0.6562 - precision: 0.7647 - recall: 0.6500\n",
            "Epoch 13: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5429 - accuracy: 0.7132 - precision: 0.7211 - recall: 0.7313 - val_loss: 0.5874 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 14/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5357 - accuracy: 0.7188 - precision: 0.7222 - recall: 0.7647\n",
            "Epoch 14: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.7195 - precision: 0.7262 - recall: 0.7394 - val_loss: 0.5871 - val_accuracy: 0.6723 - val_precision: 0.7222 - val_recall: 0.6791\n",
            "Epoch 15/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5447 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 15: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5421 - accuracy: 0.7195 - precision: 0.7280 - recall: 0.7354 - val_loss: 0.5872 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 16/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4423 - accuracy: 0.8125 - precision: 0.7826 - recall: 0.9474\n",
            "Epoch 16: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5420 - accuracy: 0.7174 - precision: 0.7260 - recall: 0.7333 - val_loss: 0.5875 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 17/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6329 - accuracy: 0.6562 - precision: 0.7143 - recall: 0.5882\n",
            "Epoch 17: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7195 - precision: 0.7317 - recall: 0.7273 - val_loss: 0.5869 - val_accuracy: 0.6765 - val_precision: 0.7280 - val_recall: 0.6791\n",
            "Epoch 18/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6220 - accuracy: 0.6562 - precision: 0.5625 - recall: 0.6923\n",
            "Epoch 18: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5410 - accuracy: 0.7206 - precision: 0.7304 - recall: 0.7333 - val_loss: 0.5861 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 19/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4889 - accuracy: 0.7500 - precision: 0.8462 - recall: 0.6471\n",
            "Epoch 19: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.7206 - precision: 0.7313 - recall: 0.7313 - val_loss: 0.5847 - val_accuracy: 0.6681 - val_precision: 0.7165 - val_recall: 0.6791\n",
            "Epoch 20/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4418 - accuracy: 0.8125 - precision: 0.8333 - recall: 0.8333\n",
            "Epoch 20: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.7185 - precision: 0.7275 - recall: 0.7333 - val_loss: 0.5846 - val_accuracy: 0.6723 - val_precision: 0.7222 - val_recall: 0.6791\n",
            "Epoch 21/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3334 - accuracy: 0.9062 - precision: 0.8421 - recall: 1.0000\n",
            "Epoch 21: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5399 - accuracy: 0.7216 - precision: 0.7309 - recall: 0.7354 - val_loss: 0.5851 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 22/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6135 - accuracy: 0.6562 - precision: 0.6842 - recall: 0.7222\n",
            "Epoch 22: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5394 - accuracy: 0.7227 - precision: 0.7324 - recall: 0.7354 - val_loss: 0.5847 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 23/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5667 - accuracy: 0.6875 - precision: 0.7222 - recall: 0.7222\n",
            "Epoch 23: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.7237 - precision: 0.7348 - recall: 0.7333 - val_loss: 0.5837 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 24/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6126 - accuracy: 0.6562 - precision: 0.5294 - recall: 0.7500\n",
            "Epoch 24: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5388 - accuracy: 0.7227 - precision: 0.7324 - recall: 0.7354 - val_loss: 0.5836 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 25/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6077 - accuracy: 0.5938 - precision: 0.6154 - recall: 0.5000\n",
            "Epoch 25: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5385 - accuracy: 0.7237 - precision: 0.7339 - recall: 0.7354 - val_loss: 0.5834 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 26/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6393 - accuracy: 0.5312 - precision: 0.4737 - recall: 0.6429\n",
            "Epoch 26: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5380 - accuracy: 0.7185 - precision: 0.7312 - recall: 0.7253 - val_loss: 0.5838 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 27/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5414 - accuracy: 0.7188 - precision: 0.5556 - recall: 0.9091\n",
            "Epoch 27: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5383 - accuracy: 0.7216 - precision: 0.7319 - recall: 0.7333 - val_loss: 0.5835 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 28/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5554 - accuracy: 0.7188 - precision: 0.8667 - recall: 0.6500\n",
            "Epoch 28: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7227 - precision: 0.7333 - recall: 0.7333 - val_loss: 0.5835 - val_accuracy: 0.6807 - val_precision: 0.7377 - val_recall: 0.6716\n",
            "Epoch 29/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6351 - accuracy: 0.6250 - precision: 0.5556 - recall: 0.7143\n",
            "Epoch 29: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7279 - precision: 0.7408 - recall: 0.7333 - val_loss: 0.5838 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 30/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5422 - accuracy: 0.7188 - precision: 0.6500 - recall: 0.8667\n",
            "Epoch 30: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5372 - accuracy: 0.7269 - precision: 0.7383 - recall: 0.7354 - val_loss: 0.5840 - val_accuracy: 0.6807 - val_precision: 0.7377 - val_recall: 0.6716\n",
            "Epoch 31/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4397 - accuracy: 0.7812 - precision: 0.7500 - recall: 0.8000\n",
            "Epoch 31: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7269 - precision: 0.7393 - recall: 0.7333 - val_loss: 0.5837 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 32/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5486 - accuracy: 0.7188 - precision: 0.8421 - recall: 0.7273\n",
            "Epoch 32: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.7237 - precision: 0.7358 - recall: 0.7313 - val_loss: 0.5829 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 33/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5638 - accuracy: 0.6250 - precision: 0.6250 - recall: 0.6250\n",
            "Epoch 33: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7258 - precision: 0.7378 - recall: 0.7333 - val_loss: 0.5831 - val_accuracy: 0.6807 - val_precision: 0.7339 - val_recall: 0.6791\n",
            "Epoch 34/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5490 - accuracy: 0.7188 - precision: 0.6667 - recall: 0.8000\n",
            "Epoch 34: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5362 - accuracy: 0.7227 - precision: 0.7333 - recall: 0.7333 - val_loss: 0.5827 - val_accuracy: 0.6807 - val_precision: 0.7339 - val_recall: 0.6791\n",
            "Epoch 35/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5650 - accuracy: 0.7188 - precision: 0.6842 - recall: 0.8125\n",
            "Epoch 35: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5358 - accuracy: 0.7258 - precision: 0.7378 - recall: 0.7333 - val_loss: 0.5821 - val_accuracy: 0.6765 - val_precision: 0.7280 - val_recall: 0.6791\n",
            "Epoch 36/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4523 - accuracy: 0.7500 - precision: 0.8421 - recall: 0.7619\n",
            "Epoch 36: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7195 - precision: 0.7271 - recall: 0.7374 - val_loss: 0.5827 - val_accuracy: 0.6723 - val_precision: 0.7258 - val_recall: 0.6716\n",
            "Epoch 37/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6403 - accuracy: 0.6250 - precision: 0.5385 - recall: 0.5385\n",
            "Epoch 37: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7206 - precision: 0.7313 - recall: 0.7313 - val_loss: 0.5829 - val_accuracy: 0.6765 - val_precision: 0.7280 - val_recall: 0.6791\n",
            "Epoch 38/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5213 - accuracy: 0.6875 - precision: 0.7500 - recall: 0.5625\n",
            "Epoch 38: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7258 - precision: 0.7349 - recall: 0.7394 - val_loss: 0.5831 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 39/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4910 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 39: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7237 - precision: 0.7329 - recall: 0.7374 - val_loss: 0.5831 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 40/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6452 - accuracy: 0.6562 - precision: 0.6500 - recall: 0.7647\n",
            "Epoch 40: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7227 - precision: 0.7343 - recall: 0.7313 - val_loss: 0.5829 - val_accuracy: 0.6765 - val_precision: 0.7280 - val_recall: 0.6791\n",
            "Epoch 41/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5784 - accuracy: 0.6250 - precision: 0.5000 - recall: 0.6667\n",
            "Epoch 41: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7248 - precision: 0.7373 - recall: 0.7313 - val_loss: 0.5827 - val_accuracy: 0.6765 - val_precision: 0.7244 - val_recall: 0.6866\n",
            "Epoch 42/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.6197 - accuracy: 0.6875 - precision: 0.5833 - recall: 0.5833\n",
            "Epoch 42: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7237 - precision: 0.7320 - recall: 0.7394 - val_loss: 0.5833 - val_accuracy: 0.6807 - val_precision: 0.7339 - val_recall: 0.6791\n",
            "Epoch 43/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5134 - accuracy: 0.7188 - precision: 0.8462 - recall: 0.6111\n",
            "Epoch 43: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7248 - precision: 0.7354 - recall: 0.7354 - val_loss: 0.5822 - val_accuracy: 0.6807 - val_precision: 0.7339 - val_recall: 0.6791\n",
            "Epoch 44/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.4834 - accuracy: 0.8125 - precision: 0.8824 - recall: 0.7895\n",
            "Epoch 44: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7248 - precision: 0.7335 - recall: 0.7394 - val_loss: 0.5836 - val_accuracy: 0.6765 - val_precision: 0.7317 - val_recall: 0.6716\n",
            "Epoch 45/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3960 - accuracy: 0.8438 - precision: 0.9375 - recall: 0.7895\n",
            "Epoch 45: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7227 - precision: 0.7324 - recall: 0.7354 - val_loss: 0.5828 - val_accuracy: 0.6807 - val_precision: 0.7302 - val_recall: 0.6866\n",
            "Epoch 46/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3831 - accuracy: 0.9062 - precision: 0.9524 - recall: 0.9091\n",
            "Epoch 46: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7227 - precision: 0.7362 - recall: 0.7273 - val_loss: 0.5827 - val_accuracy: 0.6807 - val_precision: 0.7266 - val_recall: 0.6940\n",
            "Epoch 47/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5022 - accuracy: 0.7812 - precision: 0.8571 - recall: 0.7059\n",
            "Epoch 47: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7237 - precision: 0.7348 - recall: 0.7333 - val_loss: 0.5835 - val_accuracy: 0.6849 - val_precision: 0.7360 - val_recall: 0.6866\n",
            "Epoch 48/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.3792 - accuracy: 0.8750 - precision: 0.8889 - recall: 0.8889\n",
            "Epoch 48: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7290 - precision: 0.7384 - recall: 0.7414 - val_loss: 0.5826 - val_accuracy: 0.6765 - val_precision: 0.7244 - val_recall: 0.6866\n",
            "Epoch 49/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5431 - accuracy: 0.7188 - precision: 0.6154 - recall: 0.6667\n",
            "Epoch 49: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7237 - precision: 0.7348 - recall: 0.7333 - val_loss: 0.5823 - val_accuracy: 0.6723 - val_precision: 0.7188 - val_recall: 0.6866\n",
            "Epoch 50/100\n",
            " 1/30 [>.............................] - ETA: 0s - loss: 0.5542 - accuracy: 0.7500 - precision: 0.7143 - recall: 0.7143\n",
            "Epoch 50: val_loss did not improve from 0.29998\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7237 - precision: 0.7348 - recall: 0.7333 - val_loss: 0.5823 - val_accuracy: 0.6765 - val_precision: 0.7209 - val_recall: 0.6940\n",
            "Epoch 50: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "less_rest = less_resting_history.history['val_accuracy'][-1]\n",
        "less_ch = less_ch_history.history['val_accuracy'][-1]\n",
        "less_fast = less_fasting_history.history['val_accuracy'][-1]\n",
        "less_ecg = less_ecg_history.history['val_accuracy'][-1]\n",
        "less_slop = less_slope_history.history['val_accuracy'][-1]\n",
        "less_ang = less_ang_history.history['val_accuracy'][-1]\n",
        "less_chest = less_chest_history.history['val_accuracy'][-1]\n",
        "less_old = less_old_history.history['val_accuracy'][-1]\n",
        "all = three_layer_history.history['val_accuracy'][-1]\n",
        "\n",
        "three_layer_history.history\n",
        "def plot_feature_reduction():\n",
        "  cols = ['All features', 'removing resting bp', 'removing cholesterol', 'removing fasting bs', 'removing ecg', 'removing chest pain', 'removing slope', 'removing old peak', 'remove ang']\n",
        "  vals = [all, less_rest, less_ch, less_fast, less_ecg, less_chest, less_slop, less_old, less_ang]\n",
        "  plt.bar(cols, vals)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Features')\n",
        "  plt.ylim(.4, 1)\n",
        "  plt.title('Performance when removing features')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()\n",
        "\n",
        "plot_feature_reduction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "ei1U_o3eMKYO",
        "outputId": "cdd9dd47-6c08-4fd9-9006-5801b20f969e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}